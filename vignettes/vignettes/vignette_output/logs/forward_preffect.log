2025-08-01 11:39:54,587 - forward - INFO - 2025-08-01 11:39:54.587419
2025-08-01 11:39:55,177 - forward - INFO - 
batch X for epoch: 0 batch: 0
2025-08-01 11:39:55,238 - forward - INFO - Size: torch.Size([100, 100]) dtype: torch.float32 Min: tensor(0., device='cuda:6')Max: tensor(565., device='cuda:6')
2025-08-01 11:39:55,247 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:55,254 - forward - INFO - tensor([[ 81.,   5.,  47.,  ...,  71.,  41.,  40.],
        [ 42.,   2.,   2.,  ..., 101.,  62., 170.],
        [  0., 395.,  46.,  ...,   0.,  28.,   0.],
        ...,
        [ 21.,  36.,  39.,  ...,  50.,  10.,   0.],
        [ 29.,   5., 104.,  ...,  93.,   7.,  13.],
        [ 57.,  17., 103.,  ...,  61.,  74.,  19.]], device='cuda:6')
2025-08-01 11:39:55,255 - forward - INFO - Size: torch.Size([100, 100]) dtype: torch.float32 Min: tensor(0., device='cuda:6')Max: tensor(602., device='cuda:6')
2025-08-01 11:39:55,255 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:55,258 - forward - INFO - tensor([[ 14.,   0.,  25.,  ...,  45.,  63.,  33.],
        [110., 127.,  21.,  ..., 113.,   4.,  11.],
        [  3.,  48.,   4.,  ...,   4.,  15.,   9.],
        ...,
        [  1.,  44.,  25.,  ...,  14.,  42.,  33.],
        [  2.,   9.,   7.,  ...,  12.,   5.,  50.],
        [131.,  70.,  32.,  ..., 112.,   4.,   0.]], device='cuda:6')
2025-08-01 11:39:55,258 - forward - INFO - 
batch indices for epoch: 0 batch: 0
2025-08-01 11:39:55,258 - forward - INFO - Size: torch.Size([100]) dtype: torch.int64 Min: tensor(0, device='cuda:6')Max: tensor(99, device='cuda:6')
2025-08-01 11:39:55,261 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:55,262 - forward - INFO - tensor([10, 71,  5, 83, 73, 33, 25, 37, 81, 51, 11, 15, 82, 48,  2, 14,  7, 27,
        87,  3, 75, 22, 46, 97, 80, 86, 55, 18, 42, 45, 84, 41,  0, 63, 90, 24,
        89, 66, 13, 54, 40, 70, 95, 69, 60,  8, 79, 61, 96, 67, 23, 50, 32, 57,
        77, 88, 56, 36, 85,  1, 21, 17, 94, 62, 19, 74, 99, 65, 38, 98, 31, 52,
        68, 34, 16, 59, 12, 39, 26, 64, 91, 43, 30, 53, 93, 35,  9, 28, 78,  4,
        92, 58, 76, 49,  6, 72, 20, 44, 29, 47], device='cuda:6')
2025-08-01 11:39:55,263 - forward - INFO - Size: torch.Size([100]) dtype: torch.int64 Min: tensor(0, device='cuda:6')Max: tensor(99, device='cuda:6')
2025-08-01 11:39:55,263 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:55,265 - forward - INFO - tensor([10, 71,  5, 83, 73, 33, 25, 37, 81, 51, 11, 15, 82, 48,  2, 14,  7, 27,
        87,  3, 75, 22, 46, 97, 80, 86, 55, 18, 42, 45, 84, 41,  0, 63, 90, 24,
        89, 66, 13, 54, 40, 70, 95, 69, 60,  8, 79, 61, 96, 67, 23, 50, 32, 57,
        77, 88, 56, 36, 85,  1, 21, 17, 94, 62, 19, 74, 99, 65, 38, 98, 31, 52,
        68, 34, 16, 59, 12, 39, 26, 64, 91, 43, 30, 53, 93, 35,  9, 28, 78,  4,
        92, 58, 76, 49,  6, 72, 20, 44, 29, 47], device='cuda:6')
2025-08-01 11:39:55,488 - forward - INFO - --------------++------ LOSS INFORMATION ------------------++--------
2025-08-01 11:39:55,489 - forward - INFO - X_hat mu:
 tensor([[8.8306e-11, 2.0979e-08, 5.9752e-02],
        [8.8894e-25, 9.3905e-14, 3.6697e-12],
        [5.5846e-27, 1.4303e-11, 6.3064e-14]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:55,490 - forward - INFO - lib size factors:
 tensor([[5287.],
        [5314.],
        [2985.]], device='cuda:6')
2025-08-01 11:39:55,490 - forward - INFO - omega:
 tensor([[1.6703e-14, 3.9680e-12, 1.1302e-05],
        [1.6728e-28, 1.7671e-17, 6.9057e-16],
        [1.8709e-30, 4.7917e-15, 2.1127e-17]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:55,496 - forward - INFO - omega * lib_size:
 tensor([[8.8306e-11, 2.0979e-08, 5.9752e-02, 1.0618e-01, 5.3935e-02, 9.2058e-10,
         1.3550e-12, 2.8761e-03, 7.3687e-11, 5.3505e-04, 1.1792e-15, 1.4773e-12,
         1.8047e-03, 5.4799e-08, 2.5137e-11, 6.7569e-06, 2.7830e-08, 9.3299e-11,
         1.7311e+01, 3.9592e-06, 9.3247e-03, 3.9996e-02, 5.5951e-11, 1.0018e+01,
         6.8393e-08, 8.5077e-12, 1.7949e-13, 8.2848e-10, 3.6518e-06, 8.6773e-04,
         1.5138e-09, 4.1147e-12, 3.6115e-09, 1.2707e-07, 4.0861e-05, 1.0041e-04,
         3.5574e-08, 2.0583e-02, 1.5301e-12, 3.6896e-03, 7.1173e-09, 7.6099e-06,
         1.7672e+00, 9.4745e-12, 1.9974e-14, 4.1834e+02, 4.0306e-05, 2.8064e-07,
         3.6497e+02, 1.0477e-10, 4.1468e-05, 2.1856e-04, 4.7626e-01, 1.9676e-10,
         1.9744e+01, 1.5444e-09, 2.6419e-09, 2.0074e-05, 1.9859e-05, 2.2105e-06,
         9.7197e-04, 5.5461e-04, 1.7784e+01, 3.3405e-11, 4.8409e+01, 1.3182e+03,
         1.0412e+03, 5.8592e-04, 3.6150e-06, 2.2171e-11, 7.7912e-06, 2.2516e-03,
         4.0887e-06, 2.0227e+03, 1.3136e-09, 1.6972e-03, 4.1450e-04, 1.1264e-10,
         5.5146e+00, 5.5995e-06, 5.4646e-09, 2.7334e-12, 1.9377e-04, 2.3877e-06,
         5.3494e-14, 1.9959e-05, 1.7406e-10, 7.6071e-07, 6.4293e-02, 2.2740e-03,
         2.8687e-08, 7.2412e-13, 2.1383e-09, 3.0454e-11, 4.0046e-05, 8.8169e-02,
         4.3407e-07, 2.1095e-03, 1.1432e-05, 1.7935e-09],
        [8.8894e-25, 9.3905e-14, 3.6697e-12, 7.0769e-05, 4.1698e-15, 8.3758e-23,
         1.1567e-16, 5.7682e-11, 1.8801e-17, 2.5638e-05, 4.8143e-20, 2.2733e-11,
         3.5628e-17, 2.7660e-14, 2.0182e-20, 6.8992e-19, 1.6045e-16, 3.3991e-23,
         1.7673e-11, 2.1158e-18, 2.5028e-33, 7.0511e-17, 1.2494e-13, 6.6623e-17,
         5.8453e-23, 2.3590e-08, 7.8685e-15, 9.4768e-18, 4.4805e-22, 2.4896e-10,
         6.8338e-25, 1.4151e-21, 5.7983e-07, 2.7042e-16, 6.2244e-20, 7.6792e-10,
         2.4610e-09, 1.7408e-09, 1.4403e-13, 1.5104e-21, 2.3633e-21, 1.7869e-10,
         1.5353e-11, 8.4190e-24, 1.8857e-04, 3.4370e-14, 5.3138e+03, 1.3570e-23,
         8.9287e-18, 4.6373e-07, 4.8282e-15, 2.6064e-15, 2.1842e-11, 1.2279e-07,
         4.2275e-13, 2.0996e-18, 9.3018e-26, 4.7487e-12, 9.2129e-13, 1.3625e-12,
         5.1002e-10, 2.3318e-15, 3.5126e-19, 1.1044e-24, 2.4607e-20, 2.1291e-10,
         9.1690e-10, 1.5866e-06, 1.3108e-01, 8.5651e-10, 3.1085e-02, 1.5512e-16,
         5.0619e-14, 5.9961e-10, 5.5562e-15, 1.7030e-03, 2.2736e-15, 3.7931e-08,
         1.3883e-06, 7.2990e-25, 9.4236e-24, 1.4838e-18, 7.4637e-12, 2.3048e-05,
         4.8813e-21, 4.2851e-12, 2.9923e-27, 5.9934e-21, 1.0688e-16, 1.9149e-11,
         6.0518e-17, 5.5559e-25, 4.5370e-11, 2.9912e-23, 1.4324e-21, 2.0387e-22,
         4.6768e-18, 7.0766e-09, 5.7774e-12, 4.3231e-23],
        [5.5846e-27, 1.4303e-11, 6.3064e-14, 1.0860e-18, 3.8959e-07, 2.4666e-23,
         1.0427e-17, 2.4697e-09, 1.9892e-10, 1.3020e-07, 1.4479e-08, 2.1033e-13,
         2.8835e-11, 4.5225e-14, 3.7938e-14, 1.3851e-12, 4.3731e-13, 1.1577e-15,
         3.2720e+00, 6.7513e-11, 1.7692e-22, 1.8580e-22, 7.4707e-19, 2.0352e-13,
         5.2465e-15, 9.1962e-13, 2.1113e-18, 8.2761e-09, 6.6092e-05, 6.8258e-02,
         4.0496e-18, 1.3452e-15, 1.9273e-09, 8.8111e-20, 2.3609e-18, 3.8415e-15,
         6.9259e-07, 1.0701e-08, 1.4442e-11, 3.6378e-14, 1.6738e-21, 2.8952e-03,
         8.6526e-05, 3.1703e-26, 1.6144e-08, 1.7941e-16, 1.0783e-07, 5.0512e-14,
         1.5036e-17, 5.1217e-12, 5.1321e-16, 8.2986e-20, 2.0740e-16, 4.6407e-11,
         2.8304e-17, 5.0993e-13, 1.4374e-05, 4.5966e-17, 3.4408e-16, 2.9816e+03,
         5.5521e-11, 1.5753e-03, 4.6947e-21, 3.9593e-26, 2.3865e-16, 1.0095e-14,
         8.3085e-06, 2.4839e-02, 4.9302e-11, 3.4841e-07, 1.2314e-09, 2.3216e-10,
         8.3261e-11, 5.2612e-06, 1.6802e-24, 7.8400e-04, 2.2493e-20, 3.6071e-09,
         4.9353e-03, 2.2606e-24, 6.4994e-17, 5.7197e-16, 3.1650e-18, 1.7153e-17,
         1.7179e-20, 3.8587e-11, 3.7012e-20, 5.9239e-21, 8.2282e-22, 7.6035e-09,
         2.3010e-15, 1.8257e-17, 2.1934e-12, 4.3300e-16, 1.4918e-07, 8.1119e-24,
         2.4877e-19, 5.3407e-19, 7.1458e-05, 4.1945e-22]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:55,496 - forward - INFO - Original Rs:
 tensor([[ 81.,   5.,  47.],
        [ 42.,   2.,   2.],
        [  0., 395.,  46.]], device='cuda:6')
2025-08-01 11:39:55,498 - forward - INFO - Generative mu:
 tensor([[56.7508, 60.1260, 53.7265],
        [56.7508, 60.1260, 53.7265],
        [10.3388, 43.6731, 38.2124]], device='cuda:6', dtype=torch.float64)
2025-08-01 11:39:55,499 - forward - INFO - X_hat theta:
 tensor([[63647036.0000,   462553.4375, 19414576.0000],
        [63647036.0000,   462553.4375, 19414576.0000],
        [55846348.0000,  6986933.0000, 19524058.0000]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:55,499 - forward - INFO - JS-divergence omega orig 0.14330881665825462 learn 0.640245219579613 flat8.195639361607795e-08
2025-08-01 11:39:55,502 - forward - INFO - average loss when compared to Xs: tensor([ 84854.5938, 110012.3828,  63357.9961,  61201.5430, 104237.1484,
         34649.3906,  33933.3359, 111196.0312,  46940.3516,  47900.6719,
         35240.6758,  46855.9805, 104712.8906,  38026.3359, 106684.8359,
        119539.4297,  44008.0781,  46244.3672, 121244.7812,  49045.5859,
         12171.2363, 115364.9688,  52953.9648, 112704.0312, 121672.6094,
         55605.0039, 121304.6250,  40505.7422,  57978.3789,  86308.9922,
        107983.1250, 111308.4531,  42561.0898,  31456.1289,  49688.4688,
         95698.2344,  29366.8730, 110093.7109,  95693.9688, 120805.0234,
         81035.4297,  93672.0781,  44787.6797,  97087.5234,  28976.3945,
         28115.3086,  34144.1562,  49031.5625,  34308.6680,  81374.4844,
         37597.5391, 118287.1719,  60577.1484,  38312.2734, 107344.9219,
         38379.4648,  42410.1094, 103286.7266, 133704.4375, 117378.8438,
         99366.1719, 121274.7734,  52715.3906, 101216.7500,  29900.8379,
        117499.9375,  53554.2578,  43550.9062,  59619.1250,  46664.2578,
        102919.5703,  85569.4844,  85615.7812, 124288.1406, 113289.8906,
         54289.0195,  30929.3828,  37419.8906,  26677.1797,  92210.3594,
         52496.0586,  58471.3594, 129742.6797,  80083.4688, 119346.2500,
         94253.8750, 111575.0312,  37492.2656,  93169.0781, 117901.5938,
        102967.9688,  41677.8516,  46966.6211, 131546.9062,  77715.1094,
         51569.1016,  41427.4492,  44528.9766,  97084.5625,  94711.7031],
       device='cuda:6', grad_fn=<MulBackward0>)
2025-08-01 11:39:55,502 - forward - INFO - -------------------------------- END --------------------------------
2025-08-01 11:39:55,509 - forward - INFO - 
2025-08-01 11:39:55,509 - forward - INFO - Loss Function         | Loss Value(s)        
2025-08-01 11:39:55,509 - forward - INFO - ---------------------------------------------
2025-08-01 11:39:55,509 - forward - INFO - Adj KL Lib:           |                   0.0
2025-08-01 11:39:55,509 - forward - INFO - Adj KL Gene:          |         7265.28515625
2025-08-01 11:39:55,510 - forward - INFO - Adj KL X:             |      5620.55517578125
2025-08-01 11:39:55,510 - forward - INFO - Adj Recon Lib:        | 0.00e+00, 0.00e+00
2025-08-01 11:39:55,510 - forward - INFO - Adj Recon DA:         | 1.78e+07, 2.09e+07
2025-08-01 11:39:55,510 - forward - INFO - Adj Batch Correction: | 0.00e+00
2025-08-01 11:39:55,510 - forward - INFO - Adj Recon Express:    | 7.454e+06            
2025-08-01 11:39:55,635 - forward - INFO - 
Training Epoch: 001/1000 | Batch 0001/0001
2025-08-01 11:39:55,635 - forward - INFO - 
Average training loss per epoch 92109.7109375
2025-08-01 11:39:55,667 - forward - INFO - --------------++------ LOSS INFORMATION ------------------++--------
2025-08-01 11:39:55,668 - forward - INFO - X_hat mu:
 tensor([[2.2468e+01, 2.4425e-11, 9.8984e-18],
        [2.0050e-03, 5.9890e-03, 7.9219e-02],
        [8.7945e-13, 1.8510e-07, 5.1261e-12]], device='cuda:6')
2025-08-01 11:39:55,668 - forward - INFO - lib size factors:
 tensor([[5781.],
        [2099.],
        [4750.]], device='cuda:6')
2025-08-01 11:39:55,669 - forward - INFO - omega:
 tensor([[3.8866e-03, 4.2250e-15, 1.7122e-21],
        [9.5520e-07, 2.8532e-06, 3.7741e-05],
        [1.8515e-16, 3.8969e-11, 1.0792e-15]], device='cuda:6')
2025-08-01 11:39:55,675 - forward - INFO - omega * lib_size:
 tensor([[2.2468e+01, 2.4425e-11, 9.8984e-18, 3.2955e-08, 1.0952e-09, 2.1663e-22,
         6.0362e-29, 7.4792e-08, 8.3081e-18, 1.4257e+00, 6.0650e-14, 1.4398e-18,
         1.2330e-17, 1.8670e-15, 3.5971e-26, 6.4035e-32, 5.5816e-11, 5.1610e-25,
         2.4756e+00, 5.4105e-16, 6.8225e-36, 4.1297e-25, 8.7027e-20, 4.9800e-06,
         1.6038e-19, 1.8279e-10, 8.4345e-11, 1.9718e-30, 1.5001e-07, 1.3499e-07,
         3.2961e-10, 1.0466e-05, 6.3358e-05, 3.7484e-17, 7.5817e-23, 3.8050e-08,
         7.4893e-14, 1.4090e-08, 2.4534e-19, 4.4295e-28, 2.6082e-16, 1.1874e-10,
         1.6835e-15, 4.5317e-24, 6.1834e-10, 4.9320e-18, 7.0289e-07, 7.6259e-17,
         5.6234e-03, 1.1392e-12, 5.8752e-10, 7.1298e-24, 1.6006e-21, 3.0045e-07,
         9.1679e-14, 1.7089e-24, 4.0442e-14, 1.9135e-22, 1.5108e-11, 8.6094e-01,
         1.8983e+02, 2.9367e+01, 4.4598e-23, 1.3524e-37, 1.7356e-17, 7.6989e-11,
         3.1833e-07, 9.0767e+00, 8.0633e-07, 1.7402e-08, 6.6276e-15, 4.8160e-19,
         6.2171e-17, 9.5107e-19, 8.1113e-30, 5.4996e+03, 1.6823e-18, 1.0894e-02,
         2.5708e+01, 1.3454e-21, 4.2908e-36, 7.5218e-14, 3.5196e-19, 1.7168e-05,
         1.6170e-29, 2.0552e-26, 2.6762e-10, 6.4013e-31, 1.4198e-16, 1.9610e-12,
         5.6354e-14, 7.4824e-26, 2.6739e-20, 1.1650e-24, 6.2039e-29, 4.4105e-13,
         5.3629e-11, 5.0890e-13, 1.6205e-01, 8.7709e-27],
        [2.0050e-03, 5.9890e-03, 7.9219e-02, 7.1365e+00, 8.4447e-01, 7.6818e-05,
         1.1871e-02, 1.8275e+02, 1.0314e-06, 3.5811e-01, 2.6663e-03, 3.3354e-04,
         3.4985e+01, 1.1134e+00, 2.4643e-03, 6.5534e-03, 9.7039e-04, 7.9636e-08,
         1.5615e+00, 5.6841e-03, 2.5987e-01, 1.9476e-03, 4.6005e-07, 3.7334e-01,
         7.9427e-03, 2.1770e-03, 8.8895e-08, 2.0675e-03, 9.0327e-01, 2.1639e+00,
         5.8233e-03, 1.7456e-01, 1.4642e-05, 3.5563e-03, 8.1387e-04, 3.9388e-01,
         9.5521e-01, 3.7071e-01, 4.0303e-05, 1.9013e-01, 2.8205e-03, 5.1400e-03,
         1.2668e+02, 1.0951e-06, 1.6385e+00, 2.9911e+02, 3.0449e-01, 5.4301e-02,
         5.8846e-03, 3.3142e-01, 1.2999e-03, 2.7675e-03, 9.3080e-01, 1.5042e+00,
         1.9592e-01, 2.9782e-02, 5.4862e-03, 2.3877e-01, 4.9106e-01, 2.8379e-03,
         1.0799e+00, 7.1077e+01, 5.1252e-02, 6.4518e-06, 5.3621e-02, 1.3797e+02,
         1.3674e+01, 7.1610e+00, 1.0017e-03, 4.1288e-02, 4.0479e-02, 1.8782e-02,
         5.0777e-01, 3.8609e+00, 4.2069e-05, 1.1794e-01, 6.5183e-02, 1.9831e-03,
         1.1511e+00, 3.6004e-02, 1.9570e-05, 6.7688e-05, 6.2625e+00, 5.6446e-01,
         8.2790e-04, 1.8340e+01, 5.1481e-04, 3.9038e-03, 1.0517e+01, 9.8487e+02,
         5.1907e-03, 1.8230e-09, 6.0133e-03, 1.0369e-05, 6.7294e-03, 1.3955e+02,
         5.8609e-04, 9.5258e+00, 2.6230e+01, 1.0488e-05],
        [8.7945e-13, 1.8510e-07, 5.1261e-12, 6.1562e-10, 5.3693e-16, 1.5439e-11,
         1.6774e-15, 7.1806e-09, 1.1493e-12, 1.1611e+01, 2.8997e-16, 1.9770e-18,
         9.4927e-10, 3.9439e-15, 6.3522e-14, 5.9330e-17, 1.8706e-10, 5.6263e-13,
         1.8369e-07, 4.4540e-11, 1.5707e-16, 1.5782e-08, 7.7689e-09, 6.5389e-18,
         2.4794e-08, 7.6729e-15, 5.4809e-15, 4.6614e-17, 5.4833e-07, 2.6439e-10,
         1.2041e-08, 2.0680e-03, 1.9577e-13, 2.2134e-14, 4.2652e-19, 7.4223e-05,
         1.8238e-03, 8.4385e-10, 9.8651e-09, 7.7509e-15, 1.5705e-12, 6.7670e-10,
         5.8746e-09, 9.1510e-25, 7.2724e-07, 1.3488e-01, 1.7897e-01, 1.6691e-15,
         1.8135e-13, 2.4653e-04, 7.1095e+00, 8.2334e-11, 1.5197e-08, 3.0817e-13,
         7.4291e-07, 2.7946e-12, 8.4181e-14, 7.6357e-12, 2.7085e-01, 7.0603e-09,
         3.6454e+02, 8.1816e-16, 5.4269e-20, 3.9863e-21, 3.2040e-12, 2.5003e-13,
         3.7967e-11, 7.3347e-07, 1.2126e-17, 5.7856e-13, 3.3667e-06, 3.7594e-07,
         2.6372e-13, 2.3832e-06, 8.7329e-09, 1.6786e-01, 5.7087e-10, 1.0126e-16,
         4.3625e+03, 4.2797e-11, 3.0228e-18, 3.0620e-24, 3.6085e-09, 9.3512e-06,
         8.0816e-22, 5.7458e-01, 6.2483e-19, 2.4968e-16, 2.0629e-16, 2.3510e+00,
         1.4243e-01, 4.1921e-08, 4.9094e-09, 6.2781e-16, 6.6981e-17, 2.6720e-09,
         1.2257e-06, 3.4525e-20, 4.5578e-01, 7.1612e-14]], device='cuda:6')
2025-08-01 11:39:55,676 - forward - INFO - Original Rs:
 tensor([[451.,   2.,  25.],
        [  0.,  27.,   4.],
        [155.,   3.,  18.]], device='cuda:6')
2025-08-01 11:39:55,676 - forward - INFO - Generative mu:
 tensor([[56.7508, 60.1260, 53.7265],
        [10.3388, 43.6731, 38.2124],
        [56.7508, 60.1260, 53.7265]], device='cuda:6', dtype=torch.float64)
2025-08-01 11:39:55,677 - forward - INFO - X_hat theta:
 tensor([[1.9442e+07, 9.4741e+02, 1.4944e+07],
        [1.0250e+07, 3.9422e+03, 9.7035e+06],
        [1.9442e+07, 9.4741e+02, 1.4944e+07]], device='cuda:6')
2025-08-01 11:39:55,678 - forward - INFO - JS-divergence omega orig 0.14689707773992297 learn 0.5967382908390236 flat8.195639361607795e-08
2025-08-01 11:39:55,680 - forward - INFO - average loss when compared to Xs: tensor([106630.6094,  14027.4746,  97938.2656, 114456.7031,  70211.7500,
         10646.2900,  52632.4805, 111353.2188,  25991.7031,  28596.7656,
         88311.2266,  13595.3457,  14411.8906,  90065.6094,  18934.3574,
        101429.2656,   2873.7275,  85791.9766,  22530.3516,  41060.8125,
         84133.9922,  53622.3359,  78128.5859,  87860.7812,  35838.9297,
         83927.4844,  49160.3789,  24078.7695,   9538.7148,   3859.8325,
         46342.1094,  41296.3516,  98092.6562,   5128.5815,  70767.4375,
         19917.1504,  43708.5234, 112759.0625,  43507.5039,  89799.2734,
         86316.2969,  32973.1797,  23961.5000,  16709.0293, 120772.9844,
         20352.8906, 118604.4375,  37535.3438,  76307.4531, 113686.3438,
         97788.3672,  24747.4883,  61420.6016, 118356.3203,  87333.6719,
        117481.9062,  34113.7070,  87093.1797,  49064.0273,  39976.9922,
         36183.7422,  28551.7012,  15929.8584,  52861.5625,  61653.1094,
         85181.3203,  93228.9141,  41079.2500,  23144.4512,  97216.5000,
         14054.5010,  31054.9961,  81426.9688, 124649.7031,   6543.6699,
         92601.9844,  79173.5938,  93176.7578,  54454.0938,  44724.8828,
         34843.8594,  74948.9609,  91503.7031,   3617.9229,  53580.3438,
         82671.3906,  88824.4609,  13495.5645, 123912.3750,  56774.5938,
         25782.4160,  32560.7969,  92463.0391, 102378.7734,  15344.0469,
         57589.2734,  57889.1055, 106354.7656,  41912.6016,  63310.2422],
       device='cuda:6')
2025-08-01 11:39:55,680 - forward - INFO - -------------------------------- END --------------------------------
2025-08-01 11:39:55,681 - forward - INFO - 
2025-08-01 11:39:55,681 - forward - INFO - Loss Function         | Loss Value(s)        
2025-08-01 11:39:55,681 - forward - INFO - ---------------------------------------------
2025-08-01 11:39:55,681 - forward - INFO - Adj KL Lib:           |                   0.0
2025-08-01 11:39:55,681 - forward - INFO - Adj KL Gene:          |     3671.682861328125
2025-08-01 11:39:55,682 - forward - INFO - Adj KL X:             |     3177.258544921875
2025-08-01 11:39:55,682 - forward - INFO - Adj Recon Lib:        | 0.00e+00, 0.00e+00
2025-08-01 11:39:55,682 - forward - INFO - Adj Recon DA:         | 6.78e+06, 1.20e+07
2025-08-01 11:39:55,682 - forward - INFO - Adj Batch Correction: | 0.00e+00
2025-08-01 11:39:55,682 - forward - INFO - Adj Recon Express:    | 5.934e+06            
2025-08-01 11:39:55,683 - forward - INFO - Validation Epoch: 001/1000 | Batch 0001/0002
2025-08-01 11:39:55,684 - forward - INFO - 
Average validation loss per epoch 24677.8828125
2025-08-01 11:39:55,686 - forward - INFO - 
Time elapsed: 0.01 min
2025-08-01 11:39:55,688 - forward - INFO - 
batch X for epoch: 1 batch: 0
2025-08-01 11:39:55,689 - forward - INFO - Size: torch.Size([100, 100]) dtype: torch.float32 Min: tensor(0., device='cuda:6')Max: tensor(565., device='cuda:6')
2025-08-01 11:39:55,689 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:55,691 - forward - INFO - tensor([[ 81.,   5.,  47.,  ...,  71.,  41.,  40.],
        [ 42.,   2.,   2.,  ..., 101.,  62., 170.],
        [  0., 395.,  46.,  ...,   0.,  28.,   0.],
        ...,
        [ 21.,  36.,  39.,  ...,  50.,  10.,   0.],
        [ 29.,   5., 104.,  ...,  93.,   7.,  13.],
        [ 57.,  17., 103.,  ...,  61.,  74.,  19.]], device='cuda:6')
2025-08-01 11:39:55,692 - forward - INFO - Size: torch.Size([100, 100]) dtype: torch.float32 Min: tensor(0., device='cuda:6')Max: tensor(602., device='cuda:6')
2025-08-01 11:39:55,692 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:55,695 - forward - INFO - tensor([[ 14.,   0.,  25.,  ...,  45.,  63.,  33.],
        [110., 127.,  21.,  ..., 113.,   4.,  11.],
        [  3.,  48.,   4.,  ...,   4.,  15.,   9.],
        ...,
        [  1.,  44.,  25.,  ...,  14.,  42.,  33.],
        [  2.,   9.,   7.,  ...,  12.,   5.,  50.],
        [131.,  70.,  32.,  ..., 112.,   4.,   0.]], device='cuda:6')
2025-08-01 11:39:55,695 - forward - INFO - 
batch indices for epoch: 1 batch: 0
2025-08-01 11:39:55,696 - forward - INFO - Size: torch.Size([100]) dtype: torch.int64 Min: tensor(0, device='cuda:6')Max: tensor(99, device='cuda:6')
2025-08-01 11:39:55,697 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:55,699 - forward - INFO - tensor([10, 71,  5, 83, 73, 33, 25, 37, 81, 51, 11, 15, 82, 48,  2, 14,  7, 27,
        87,  3, 75, 22, 46, 97, 80, 86, 55, 18, 42, 45, 84, 41,  0, 63, 90, 24,
        89, 66, 13, 54, 40, 70, 95, 69, 60,  8, 79, 61, 96, 67, 23, 50, 32, 57,
        77, 88, 56, 36, 85,  1, 21, 17, 94, 62, 19, 74, 99, 65, 38, 98, 31, 52,
        68, 34, 16, 59, 12, 39, 26, 64, 91, 43, 30, 53, 93, 35,  9, 28, 78,  4,
        92, 58, 76, 49,  6, 72, 20, 44, 29, 47], device='cuda:6')
2025-08-01 11:39:55,699 - forward - INFO - Size: torch.Size([100]) dtype: torch.int64 Min: tensor(0, device='cuda:6')Max: tensor(99, device='cuda:6')
2025-08-01 11:39:55,700 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:55,702 - forward - INFO - tensor([10, 71,  5, 83, 73, 33, 25, 37, 81, 51, 11, 15, 82, 48,  2, 14,  7, 27,
        87,  3, 75, 22, 46, 97, 80, 86, 55, 18, 42, 45, 84, 41,  0, 63, 90, 24,
        89, 66, 13, 54, 40, 70, 95, 69, 60,  8, 79, 61, 96, 67, 23, 50, 32, 57,
        77, 88, 56, 36, 85,  1, 21, 17, 94, 62, 19, 74, 99, 65, 38, 98, 31, 52,
        68, 34, 16, 59, 12, 39, 26, 64, 91, 43, 30, 53, 93, 35,  9, 28, 78,  4,
        92, 58, 76, 49,  6, 72, 20, 44, 29, 47], device='cuda:6')
2025-08-01 11:39:55,733 - forward - INFO - --------------++------ LOSS INFORMATION ------------------++--------
2025-08-01 11:39:55,734 - forward - INFO - X_hat mu:
 tensor([[4.8928e-17, 4.5324e-06, 3.3519e+03],
        [6.2709e-15, 2.1852e-07, 1.2698e+03],
        [2.7810e-15, 2.4616e-10, 8.4290e-09]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:55,735 - forward - INFO - lib size factors:
 tensor([[5287.],
        [5314.],
        [2985.]], device='cuda:6')
2025-08-01 11:39:55,736 - forward - INFO - omega:
 tensor([[9.2543e-21, 8.5727e-10, 6.3399e-01],
        [1.1801e-18, 4.1121e-11, 2.3894e-01],
        [9.3164e-19, 8.2466e-14, 2.8238e-12]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:55,741 - forward - INFO - omega * lib_size:
 tensor([[4.8928e-17, 4.5324e-06, 3.3519e+03, 3.9630e-09, 1.9867e-07, 4.8357e-11,
         1.1935e-13, 5.6838e-08, 1.3784e-11, 1.9146e-04, 3.9317e-05, 1.1177e-12,
         6.8245e-09, 1.2344e-12, 1.7814e-10, 1.6423e-04, 2.5443e-08, 9.4385e-14,
         6.5124e-04, 1.3475e-04, 8.3004e-08, 4.7553e-10, 1.9140e-06, 1.1362e-04,
         4.5846e-07, 2.8752e-07, 6.4408e-13, 1.0452e-11, 1.8027e-08, 6.4552e-08,
         2.0532e-08, 1.5277e-09, 4.8777e-08, 2.4083e-03, 1.4524e-01, 3.9618e-15,
         3.1342e-07, 4.1664e-04, 1.7965e-09, 1.1017e+00, 2.5169e-09, 2.7939e-12,
         1.4031e-02, 1.9884e-14, 9.5750e-08, 7.0749e-07, 1.0206e-09, 4.4699e-09,
         2.6462e-10, 7.2944e-04, 4.5932e-07, 8.7868e-07, 1.3380e-05, 4.3272e-07,
         2.8172e-11, 2.8439e-09, 9.3043e-11, 9.8928e-09, 6.4092e-09, 1.0248e-06,
         8.6763e-10, 1.9337e+03, 4.7357e-13, 2.5286e-08, 4.7760e-04, 1.9646e-08,
         3.9821e-06, 1.0510e-05, 1.8043e-03, 2.1186e-10, 1.6060e-03, 1.1976e-10,
         5.8208e-07, 1.9133e-05, 5.0125e-07, 5.0732e-07, 2.5006e-11, 9.9896e-06,
         6.7606e-07, 2.4691e-10, 9.5537e-07, 9.6265e-14, 3.1399e-04, 2.6595e-02,
         2.0629e-10, 1.0788e-06, 3.1399e-07, 3.0528e-08, 1.2458e-04, 6.6826e-06,
         8.8825e-11, 4.4060e-14, 8.9425e-03, 2.2916e-11, 1.3650e-13, 1.9163e-06,
         1.8840e-08, 9.4834e-09, 4.2488e-02, 5.4528e-10],
        [6.2709e-15, 2.1852e-07, 1.2698e+03, 8.7711e-11, 3.3095e+03, 4.1236e-17,
         4.6983e-22, 9.1799e-08, 5.9650e-06, 5.0230e-07, 4.0690e-05, 1.0942e-20,
         8.7301e-06, 1.9999e-15, 1.7406e-16, 4.3856e-03, 6.8864e-11, 1.3561e-19,
         3.4712e-10, 4.1654e-12, 2.8857e-08, 4.8709e-11, 2.5685e-34, 2.3968e-02,
         5.0306e-09, 1.2768e-12, 9.7254e-15, 2.4433e-10, 8.3290e-02, 1.4015e-02,
         1.5488e-11, 2.5345e-07, 1.1922e-13, 3.1663e-18, 3.4494e-06, 5.2123e-10,
         6.8577e-13, 5.9303e-03, 9.2747e-17, 3.9195e-12, 5.1928e-18, 2.3743e-01,
         6.1327e-08, 7.1827e-31, 3.3290e-09, 2.1932e+02, 5.0579e-21, 3.0186e-14,
         4.6389e+01, 2.1877e-16, 1.2666e-07, 3.7370e-01, 1.5898e-09, 5.5254e-03,
         1.8522e-03, 2.7228e-03, 1.1645e-01, 2.0244e-13, 2.4909e-08, 7.8886e-03,
         2.4693e-08, 4.8762e+01, 1.0612e-11, 6.5077e-20, 4.7142e-06, 3.8431e-06,
         1.0124e-02, 8.7831e+01, 2.9829e-05, 1.3936e-09, 1.0400e-14, 1.4698e-06,
         2.8398e-04, 4.5611e+01, 6.5002e-26, 1.0686e-01, 2.8897e-22, 3.1548e-11,
         2.1611e+01, 4.3635e+01, 5.9894e-11, 8.3299e-19, 3.1592e-13, 1.6938e+02,
         1.1126e-16, 9.8193e-13, 1.3919e-13, 1.2583e-19, 5.5687e-14, 8.5530e-07,
         8.6760e-19, 2.9297e-12, 1.3859e-07, 1.2208e-14, 3.6959e-04, 3.4524e-07,
         5.3865e-15, 1.2232e-14, 5.1181e+01, 4.9574e-12],
        [2.7810e-15, 2.4616e-10, 8.4290e-09, 4.3789e-11, 5.8208e-16, 4.7432e-16,
         3.6162e-09, 1.0193e-11, 7.9449e-15, 4.3753e-07, 9.5087e-12, 1.2857e-16,
         1.6999e-12, 1.6599e-14, 4.3450e-21, 3.5900e-16, 2.7689e-16, 2.4825e-09,
         3.3497e-03, 1.6931e-11, 1.2248e-20, 1.1794e-07, 3.0194e+01, 8.5158e-16,
         6.8254e-15, 6.4592e-11, 1.5799e-21, 6.1094e-10, 2.7379e-08, 2.0025e-14,
         2.1555e-21, 2.6314e-05, 1.1722e-13, 7.1543e-09, 2.7718e-14, 2.1136e-06,
         3.0939e-01, 3.7224e-15, 2.4049e-02, 2.5961e-15, 1.4439e-25, 5.4173e-20,
         7.1634e-08, 4.3236e-18, 1.3098e-07, 8.7961e-04, 2.9513e+03, 3.5403e-10,
         2.2580e-09, 1.6537e-02, 2.1074e+00, 4.4174e-18, 3.9200e-09, 2.7867e-04,
         3.7490e-21, 2.5562e-09, 1.1000e-01, 7.4947e-22, 6.5733e-11, 6.9563e-11,
         1.1365e-04, 1.2208e-01, 2.6718e-16, 1.5663e-19, 1.8860e-12, 4.7316e-10,
         4.9715e-07, 2.1432e-15, 1.0861e-08, 2.4422e-11, 1.1479e-14, 5.5056e-20,
         6.0142e-15, 3.0580e-09, 4.1274e-11, 4.1696e-17, 4.0993e-22, 5.6561e-14,
         4.1113e-01, 9.7992e-17, 1.2681e-19, 1.2888e-26, 1.1734e-15, 1.4623e-07,
         6.7437e-18, 9.5832e-07, 2.7370e-24, 3.9679e-16, 9.3497e-10, 8.2954e-07,
         8.6505e-02, 1.2756e-21, 3.0169e-11, 4.8892e-15, 1.2302e-12, 1.1619e-05,
         5.2102e-18, 2.8139e-17, 3.1076e-01, 1.5934e-24]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:55,742 - forward - INFO - Original Rs:
 tensor([[ 81.,   5.,  47.],
        [ 42.,   2.,   2.],
        [  0., 395.,  46.]], device='cuda:6')
2025-08-01 11:39:55,742 - forward - INFO - Generative mu:
 tensor([[56.7508, 60.1260, 53.7265],
        [56.7508, 60.1260, 53.7265],
        [10.3388, 43.6731, 38.2124]], device='cuda:6', dtype=torch.float64)
2025-08-01 11:39:55,743 - forward - INFO - X_hat theta:
 tensor([[2.8050e+07, 4.7494e+05, 1.2792e+07],
        [2.8050e+07, 4.7494e+05, 1.2792e+07],
        [1.3871e+07, 1.5897e+01, 7.9836e+04]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:55,744 - forward - INFO - JS-divergence omega orig 0.14330881665825462 learn 0.6173508260025036 flat8.195639361607795e-08
2025-08-01 11:39:55,746 - forward - INFO - average loss when compared to Xs: tensor([ 93880.6016,  94195.5000,  52605.0781,  48448.9531, 116009.5156,
          1985.6029,  11453.6758,  93526.9375,  47951.2852,  47191.7539,
         39797.9492,  47739.8945, 107847.3047,  55916.1992, 105191.2969,
        137055.5469,  19908.0332,  27958.3223, 119113.7031,  42575.1680,
         40617.7422, 110542.8438,  29946.6602, 130355.1953,  91083.8594,
         13119.9951, 107629.3594,  43392.0078,  46237.1172,  84752.4141,
        102617.9688, 122516.9062,  41727.5898,  13821.6016,  25532.0898,
         90272.3125,  40809.6523, 103155.7031,  73735.5000,  98681.5312,
         89297.6719, 119322.7734,  27683.2305,  64524.4297,  20262.9141,
         15911.5996,  34000.5781,  18799.3535,  31304.5840,  93582.6016,
         46582.9727,  84905.1797,  57669.6562,  51992.0391,  98327.1250,
         38721.1094,  21067.9531,  57052.3516, 119108.6719,  97046.0312,
         93402.0938, 105260.4219,  15956.4775, 119222.8750,  33300.7500,
         88705.8906,  53960.5000,  47594.8125,  31316.4121,  28981.2441,
         82996.8125,  72211.8906,  84161.3906, 130907.2109, 105061.8047,
         49456.4805,  29587.6270,  40448.0234,  22826.9629,  84508.7422,
         35846.9961,  42544.8867, 130216.5156,  83657.9844, 115497.5469,
        107951.7891, 108354.8125,  12300.6514,  75604.7812,  71021.9531,
         98333.2812,  11229.0869,  34362.8438,  76706.2969,  42038.5703,
         51630.9258,  39115.2969,  51250.6250, 101195.7734,  89308.4297],
       device='cuda:6', grad_fn=<MulBackward0>)
2025-08-01 11:39:55,746 - forward - INFO - -------------------------------- END --------------------------------
2025-08-01 11:39:55,747 - forward - INFO - 
2025-08-01 11:39:55,747 - forward - INFO - Loss Function         | Loss Value(s)        
2025-08-01 11:39:55,748 - forward - INFO - ---------------------------------------------
2025-08-01 11:39:55,749 - forward - INFO - Adj KL Lib:           |                   0.0
2025-08-01 11:39:55,750 - forward - INFO - Adj KL Gene:          |      4936.90576171875
2025-08-01 11:39:55,751 - forward - INFO - Adj KL X:             |      4193.16259765625
2025-08-01 11:39:55,751 - forward - INFO - Adj Recon Lib:        | 0.00e+00, 0.00e+00
2025-08-01 11:39:55,752 - forward - INFO - Adj Recon DA:         | 1.11e+07, 1.55e+07
2025-08-01 11:39:55,752 - forward - INFO - Adj Batch Correction: | 0.00e+00
2025-08-01 11:39:55,753 - forward - INFO - Adj Recon Express:    | 6.606e+06            
2025-08-01 11:39:55,769 - forward - INFO - 
Training Epoch: 002/1000 | Batch 0001/0001
2025-08-01 11:39:55,769 - forward - INFO - 
Average training loss per epoch 66170.28125
2025-08-01 11:39:55,801 - forward - INFO - --------------++------ LOSS INFORMATION ------------------++--------
2025-08-01 11:39:55,802 - forward - INFO - X_hat mu:
 tensor([[2.6816e-14, 7.1802e-10, 5.7747e-02],
        [8.8977e+00, 3.5755e+00, 4.5042e+01],
        [1.8282e-01, 8.8165e+00, 1.1656e-01]], device='cuda:6')
2025-08-01 11:39:55,803 - forward - INFO - lib size factors:
 tensor([[5781.],
        [2099.],
        [4750.]], device='cuda:6')
2025-08-01 11:39:55,804 - forward - INFO - omega:
 tensor([[4.6386e-18, 1.2420e-13, 9.9892e-06],
        [4.2390e-03, 1.7034e-03, 2.1459e-02],
        [3.8489e-05, 1.8561e-03, 2.4539e-05]], device='cuda:6')
2025-08-01 11:39:55,809 - forward - INFO - omega * lib_size:
 tensor([[2.6816e-14, 7.1802e-10, 5.7747e-02, 1.5918e-09, 4.8654e-09, 4.6761e-08,
         1.3917e-15, 6.0587e-10, 1.5737e-09, 6.3981e-01, 7.8766e-15, 9.0787e-17,
         5.1471e-08, 3.2249e-12, 1.9265e-13, 8.0908e-09, 4.5097e-08, 2.6279e-07,
         5.1757e-03, 6.9706e-12, 2.7336e-06, 7.8751e-04, 1.1060e-15, 1.6167e-01,
         8.7214e-13, 2.0043e-14, 3.0104e-16, 9.1750e-11, 8.7225e-06, 8.7074e+00,
         8.6582e-02, 1.5280e-08, 2.6209e-05, 7.1542e-11, 2.6762e-06, 5.2304e-05,
         1.4161e-02, 5.4367e+00, 2.4558e-12, 5.3530e-06, 1.4358e-07, 5.6798e-04,
         2.4139e-08, 3.0023e-20, 2.3074e-10, 1.5908e-03, 3.7446e-06, 8.6667e-09,
         3.5909e-03, 1.7758e-05, 3.0864e-01, 2.8725e-07, 1.0637e-07, 5.7568e+03,
         2.4081e-08, 5.2796e-16, 3.6160e-08, 2.7296e-09, 9.5572e-05, 3.5819e-09,
         1.4336e-04, 3.5118e-09, 1.7344e-04, 2.0063e-07, 3.1891e-05, 3.7169e-09,
         7.2723e+00, 2.7449e-01, 2.5044e-11, 1.1148e-12, 5.5010e-21, 4.4968e-08,
         3.0672e-10, 1.0782e-07, 3.5962e-10, 1.1351e+00, 1.2749e-10, 1.2144e-10,
         6.5491e-02, 2.2463e-04, 3.5350e-14, 2.7011e-14, 1.3987e-02, 4.8778e-02,
         1.1571e-16, 2.1330e-10, 9.3124e-05, 2.7914e-13, 1.3243e-12, 1.2142e-11,
         1.3306e-02, 3.6896e-14, 1.5442e-07, 2.7613e-07, 2.3270e-07, 2.1249e-07,
         4.3446e-04, 1.5000e-17, 1.1202e-04, 1.4128e-12],
        [8.8977e+00, 3.5755e+00, 4.5042e+01, 2.0172e+02, 1.1369e+01, 5.8070e+00,
         2.4529e+00, 1.1787e+01, 2.5779e-01, 1.6782e+01, 1.8199e+00, 4.8748e+00,
         1.5136e+00, 1.9372e+01, 3.3187e+00, 2.6268e-01, 1.7843e+00, 9.5318e-01,
         1.4552e+01, 2.1554e+01, 6.8454e+00, 3.0634e+00, 4.4102e+00, 4.5335e+00,
         1.7587e+00, 1.8254e+01, 5.2593e-01, 3.6336e+00, 6.0495e+01, 3.6344e+01,
         4.8432e+00, 3.8460e+00, 9.1863e+00, 2.2819e+01, 8.8379e+00, 5.4753e+00,
         1.0592e+01, 2.1583e+00, 9.6066e-01, 3.3819e+00, 1.2712e+00, 8.4884e-01,
         8.0971e+01, 7.2041e-01, 4.0219e+00, 2.1105e+01, 4.4638e+00, 1.2454e+01,
         1.0198e+01, 6.3696e+01, 7.4617e+00, 1.4061e+00, 1.4486e+01, 2.6953e+00,
         9.0699e+00, 5.9697e+00, 3.0555e+00, 9.3429e+00, 7.1489e+00, 5.6635e+00,
         3.0218e+00, 7.6467e+01, 2.2766e+00, 1.7537e+00, 1.8394e+01, 6.5451e+01,
         4.8553e+00, 3.4255e+01, 7.5441e+00, 1.0105e+01, 1.5652e+01, 1.3453e+00,
         7.4103e+00, 8.2554e+00, 3.9397e+00, 7.5820e+00, 4.9359e-01, 1.2794e+02,
         4.7979e+01, 5.7993e+00, 1.2844e+00, 4.5243e+00, 3.8151e+01, 3.4418e+01,
         4.9765e+00, 9.6439e+01, 1.0290e+01, 7.7500e+00, 4.0498e+01, 8.4132e+01,
         8.8275e+00, 6.5793e-01, 2.9739e+01, 1.2367e-01, 1.1883e+00, 9.7955e+01,
         7.0696e+00, 5.0941e+01, 2.7056e+02, 3.2350e+00],
        [1.8282e-01, 8.8165e+00, 1.1656e-01, 7.4324e+01, 4.4862e-04, 1.4547e-02,
         8.7063e-04, 3.3075e-06, 2.7922e-05, 8.0556e+01, 1.2137e-03, 3.6839e-05,
         4.0181e-02, 1.8698e-04, 1.5857e-05, 2.2900e-04, 3.0840e-02, 7.4178e-03,
         7.2169e-02, 2.6122e-03, 8.2535e-06, 4.7116e-01, 2.8093e+00, 6.6934e-04,
         2.1216e-03, 9.3110e-04, 5.2801e-05, 2.6743e-02, 7.2615e-01, 9.6905e-02,
         1.3564e-03, 3.8748e+01, 6.0721e-02, 4.3545e-01, 1.0088e-05, 4.2551e-02,
         1.7460e-02, 3.1042e+00, 5.4283e-04, 1.8379e-04, 9.3073e-04, 3.8343e-04,
         5.1882e-01, 1.3279e-04, 3.2275e-03, 4.0315e-01, 7.1902e+00, 3.5128e-06,
         2.6575e-03, 4.1024e-04, 2.9369e-01, 7.1525e-02, 8.7207e-04, 1.1882e+01,
         1.0098e-01, 9.1265e-04, 2.3689e-05, 1.1367e-04, 7.8038e-01, 1.5412e-03,
         3.6936e+01, 2.4011e-01, 1.3931e-05, 4.7770e-06, 3.6832e-01, 3.9291e-02,
         7.2761e-01, 2.2897e+00, 4.3294e+01, 3.6671e-02, 5.9436e+01, 4.0199e-03,
         1.8755e-04, 4.5945e+01, 4.6155e-04, 8.3851e-02, 6.8348e-02, 1.4774e-02,
         3.6873e+02, 2.4008e-05, 4.4989e-05, 2.6558e-04, 1.3461e+02, 1.3236e-02,
         5.6614e-05, 6.2788e+01, 1.1857e-04, 4.5493e-04, 8.4409e-01, 3.6853e+03,
         8.6060e-04, 6.0883e-01, 2.8421e-02, 2.0006e-04, 3.6842e-05, 1.4379e+01,
         3.4451e-03, 9.5376e-01, 6.0322e+01, 9.2883e-05]], device='cuda:6')
2025-08-01 11:39:55,810 - forward - INFO - Original Rs:
 tensor([[451.,   2.,  25.],
        [  0.,  27.,   4.],
        [155.,   3.,  18.]], device='cuda:6')
2025-08-01 11:39:55,811 - forward - INFO - Generative mu:
 tensor([[56.7508, 60.1260, 53.7265],
        [10.3388, 43.6731, 38.2124],
        [56.7508, 60.1260, 53.7265]], device='cuda:6', dtype=torch.float64)
2025-08-01 11:39:55,812 - forward - INFO - X_hat theta:
 tensor([[2.0561e+07, 1.0558e+04, 5.3704e+05],
        [2.1302e+03, 3.5223e+01, 5.1768e+01],
        [2.0561e+07, 1.0558e+04, 5.3704e+05]], device='cuda:6')
2025-08-01 11:39:55,813 - forward - INFO - JS-divergence omega orig 0.14689707773992297 learn 0.5398604038311976 flat8.195639361607795e-08
2025-08-01 11:39:55,815 - forward - INFO - average loss when compared to Xs: tensor([104058.6719,   2279.6492,  39792.0000, 100183.8906,  39157.0859,
         19645.3359,  37356.7969, 103141.0000,  20507.0391,   2795.1707,
        117229.7812,   4323.9185,  15183.7520,  53666.8672,  44976.0938,
        108934.8750,   3009.9583, 116460.2422,  20232.6191,  22791.6094,
         86583.3750,   8852.6660,  32889.9766, 104300.7656,   3894.6216,
         87495.7266,  28169.6875,   6597.9497,   9480.2109,   2244.3821,
         24782.4590,  14667.0264,  69738.0469,   5134.0723,  39200.1953,
         16111.8379,  31123.2695,  75130.1016,  21063.1367,  38258.2969,
        105452.8359,   9818.7012,  25904.4023,  10773.7285,  96107.6328,
         16061.2871,  82578.1094,  17565.0156,  84544.0469, 108962.2969,
         77145.3359,   2782.6362, 110472.0156,  43819.4531,  96449.8828,
         81517.2734,  38872.4297,  94544.2344,  18069.6035,   7319.1621,
         11328.4336,   4452.7041,  16600.8086,  41969.4297,  70481.9453,
        112567.8906, 106147.0781,  10574.3242,  12992.8486,  80210.1719,
          5930.7412,   8641.2793,  79496.6406,  56562.0312,   6908.8545,
        108499.1562,  50240.5156,  57204.3047,  53816.3672,  34971.7539,
         11593.0537,  58974.3516, 120333.5703,   1698.9937,  26390.1211,
         73234.2188,  78946.4688,   5120.4697, 106338.7031,  52083.3789,
          6231.9346,  10024.4277,  98387.9688,  56074.6641,  11354.3105,
         46018.3750,  44587.1406,  83298.4688,  39038.0352, 111765.7734],
       device='cuda:6')
2025-08-01 11:39:55,815 - forward - INFO - -------------------------------- END --------------------------------
2025-08-01 11:39:55,815 - forward - INFO - 
2025-08-01 11:39:55,816 - forward - INFO - Loss Function         | Loss Value(s)        
2025-08-01 11:39:55,816 - forward - INFO - ---------------------------------------------
2025-08-01 11:39:55,816 - forward - INFO - Adj KL Lib:           |                   0.0
2025-08-01 11:39:55,816 - forward - INFO - Adj KL Gene:          |     2123.172119140625
2025-08-01 11:39:55,817 - forward - INFO - Adj KL X:             |      2315.30029296875
2025-08-01 11:39:55,817 - forward - INFO - Adj Recon Lib:        | 0.00e+00, 0.00e+00
2025-08-01 11:39:55,817 - forward - INFO - Adj Recon DA:         | 3.34e+06, 7.83e+06
2025-08-01 11:39:55,817 - forward - INFO - Adj Batch Correction: | 0.00e+00
2025-08-01 11:39:55,818 - forward - INFO - Adj Recon Express:    | 4.781e+06            
2025-08-01 11:39:55,818 - forward - INFO - Validation Epoch: 002/1000 | Batch 0001/0002
2025-08-01 11:39:55,819 - forward - INFO - 
Average validation loss per epoch 15902.8955078125
2025-08-01 11:39:55,822 - forward - INFO - 
Time elapsed: 0.01 min
2025-08-01 11:39:55,824 - forward - INFO - 
batch X for epoch: 2 batch: 0
2025-08-01 11:39:55,825 - forward - INFO - Size: torch.Size([100, 100]) dtype: torch.float32 Min: tensor(0., device='cuda:6')Max: tensor(565., device='cuda:6')
2025-08-01 11:39:55,825 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:55,828 - forward - INFO - tensor([[ 81.,   5.,  47.,  ...,  71.,  41.,  40.],
        [ 42.,   2.,   2.,  ..., 101.,  62., 170.],
        [  0., 395.,  46.,  ...,   0.,  28.,   0.],
        ...,
        [ 21.,  36.,  39.,  ...,  50.,  10.,   0.],
        [ 29.,   5., 104.,  ...,  93.,   7.,  13.],
        [ 57.,  17., 103.,  ...,  61.,  74.,  19.]], device='cuda:6')
2025-08-01 11:39:55,829 - forward - INFO - Size: torch.Size([100, 100]) dtype: torch.float32 Min: tensor(0., device='cuda:6')Max: tensor(602., device='cuda:6')
2025-08-01 11:39:55,829 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:55,832 - forward - INFO - tensor([[ 14.,   0.,  25.,  ...,  45.,  63.,  33.],
        [110., 127.,  21.,  ..., 113.,   4.,  11.],
        [  3.,  48.,   4.,  ...,   4.,  15.,   9.],
        ...,
        [  1.,  44.,  25.,  ...,  14.,  42.,  33.],
        [  2.,   9.,   7.,  ...,  12.,   5.,  50.],
        [131.,  70.,  32.,  ..., 112.,   4.,   0.]], device='cuda:6')
2025-08-01 11:39:55,832 - forward - INFO - 
batch indices for epoch: 2 batch: 0
2025-08-01 11:39:55,832 - forward - INFO - Size: torch.Size([100]) dtype: torch.int64 Min: tensor(0, device='cuda:6')Max: tensor(99, device='cuda:6')
2025-08-01 11:39:55,833 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:55,836 - forward - INFO - tensor([10, 71,  5, 83, 73, 33, 25, 37, 81, 51, 11, 15, 82, 48,  2, 14,  7, 27,
        87,  3, 75, 22, 46, 97, 80, 86, 55, 18, 42, 45, 84, 41,  0, 63, 90, 24,
        89, 66, 13, 54, 40, 70, 95, 69, 60,  8, 79, 61, 96, 67, 23, 50, 32, 57,
        77, 88, 56, 36, 85,  1, 21, 17, 94, 62, 19, 74, 99, 65, 38, 98, 31, 52,
        68, 34, 16, 59, 12, 39, 26, 64, 91, 43, 30, 53, 93, 35,  9, 28, 78,  4,
        92, 58, 76, 49,  6, 72, 20, 44, 29, 47], device='cuda:6')
2025-08-01 11:39:55,836 - forward - INFO - Size: torch.Size([100]) dtype: torch.int64 Min: tensor(0, device='cuda:6')Max: tensor(99, device='cuda:6')
2025-08-01 11:39:55,837 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:55,839 - forward - INFO - tensor([10, 71,  5, 83, 73, 33, 25, 37, 81, 51, 11, 15, 82, 48,  2, 14,  7, 27,
        87,  3, 75, 22, 46, 97, 80, 86, 55, 18, 42, 45, 84, 41,  0, 63, 90, 24,
        89, 66, 13, 54, 40, 70, 95, 69, 60,  8, 79, 61, 96, 67, 23, 50, 32, 57,
        77, 88, 56, 36, 85,  1, 21, 17, 94, 62, 19, 74, 99, 65, 38, 98, 31, 52,
        68, 34, 16, 59, 12, 39, 26, 64, 91, 43, 30, 53, 93, 35,  9, 28, 78,  4,
        92, 58, 76, 49,  6, 72, 20, 44, 29, 47], device='cuda:6')
2025-08-01 11:39:55,871 - forward - INFO - --------------++------ LOSS INFORMATION ------------------++--------
2025-08-01 11:39:55,872 - forward - INFO - X_hat mu:
 tensor([[3.8388e-01, 4.4095e-01, 2.0317e-02],
        [1.1909e-13, 2.1094e+01, 2.9333e-11],
        [2.9842e-10, 1.6025e-04, 1.9089e-03]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:55,873 - forward - INFO - lib size factors:
 tensor([[5287.],
        [5314.],
        [2985.]], device='cuda:6')
2025-08-01 11:39:55,874 - forward - INFO - omega:
 tensor([[7.2609e-05, 8.3402e-05, 3.8427e-06],
        [2.2411e-17, 3.9695e-03, 5.5200e-15],
        [9.9974e-14, 5.3686e-08, 6.3949e-07]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:55,879 - forward - INFO - omega * lib_size:
 tensor([[3.8388e-01, 4.4095e-01, 2.0317e-02, 3.9011e-01, 9.8797e-12, 4.5791e-07,
         1.5331e-14, 2.9545e-04, 1.0373e-08, 5.7338e+00, 1.2376e-05, 1.9352e-10,
         3.3257e-10, 2.5855e-08, 3.3651e-08, 6.3303e-04, 2.0836e-05, 1.3282e-05,
         9.3185e-02, 1.0233e-09, 3.0135e-07, 3.1185e-03, 6.6875e-08, 3.0319e-05,
         3.1261e-10, 1.7473e-03, 3.6201e-09, 6.2932e-11, 8.3411e-11, 8.0254e-02,
         1.1175e-08, 7.6206e-07, 1.4728e-03, 6.8596e-08, 1.4041e-01, 2.3220e-05,
         1.2230e-06, 7.0681e-05, 4.4918e-05, 5.3654e-06, 4.4563e-03, 9.3559e-08,
         3.9856e-09, 7.6582e-07, 2.0320e-04, 5.0915e-04, 6.0477e+02, 1.0986e-10,
         3.6553e-05, 4.4444e+00, 7.9640e-07, 6.5409e-05, 6.8944e-08, 9.2965e-05,
         8.1192e-06, 1.0635e-13, 9.0722e-08, 3.5942e-01, 4.3919e-03, 1.8639e-07,
         2.4016e-06, 2.4471e-06, 6.8498e-10, 2.9244e-05, 1.6042e-06, 1.3164e-06,
         1.5856e-03, 4.5673e+03, 5.0715e-05, 1.8662e-03, 2.5485e-07, 5.4216e-09,
         2.1477e-05, 1.1564e-06, 7.5791e+01, 1.1254e-03, 8.6440e-13, 3.8709e-04,
         1.8258e+00, 4.5029e-07, 2.6138e-07, 4.2357e-04, 6.7722e-04, 7.5437e+00,
         7.9314e-08, 1.7571e+01, 5.6241e-07, 5.0937e-11, 1.3461e-03, 6.3294e-02,
         2.1793e-03, 8.3630e-06, 2.7684e-06, 1.1657e-07, 5.8024e-07, 3.6748e-04,
         9.5892e-10, 1.9400e-03, 2.2463e-05, 4.5544e-06],
        [1.1909e-13, 2.1094e+01, 2.9333e-11, 8.0738e-12, 2.8157e-17, 1.8165e-13,
         5.7217e-16, 1.0949e-12, 1.7732e-23, 9.2842e-01, 5.4533e-17, 8.2671e-18,
         3.4436e+01, 1.3577e-13, 6.9737e-20, 5.1446e-15, 5.4855e-13, 6.0193e-12,
         4.0002e-11, 8.7600e-17, 2.6660e-08, 2.8050e-16, 5.5380e-25, 9.7077e-05,
         1.2913e-13, 3.3013e-13, 1.3814e-14, 9.4264e-13, 1.7006e+02, 1.6166e-06,
         1.9794e-14, 2.8728e-10, 5.3069e-15, 1.1751e-04, 7.7957e-06, 1.0881e-12,
         1.3793e-04, 9.2564e-06, 2.2989e-14, 3.8671e+00, 7.0447e-19, 5.3106e-11,
         3.0384e-13, 3.4551e-28, 4.0062e-16, 1.3172e-01, 2.9555e-16, 6.4394e-13,
         4.9804e+03, 8.8969e-10, 2.5672e-08, 2.1485e-06, 1.7155e-18, 4.6455e-10,
         2.3567e-01, 1.9711e-10, 2.7552e-17, 4.2687e-11, 3.7891e-12, 9.2544e+01,
         5.7966e-16, 5.8821e-04, 3.5795e+00, 2.8626e-19, 6.4858e+00, 1.9262e-05,
         2.9874e-09, 4.2929e-09, 1.2218e-18, 3.2222e-18, 2.7157e-08, 2.2272e-17,
         4.2705e-15, 2.6770e-07, 8.8054e-12, 1.4834e-11, 9.2685e-14, 1.1853e-10,
         2.0769e-08, 1.9668e-01, 1.0365e-20, 4.9286e-17, 3.4391e-16, 2.0839e-05,
         3.5734e-25, 1.2239e-03, 4.4302e-13, 1.3643e-17, 8.5288e-10, 2.9658e-07,
         6.3329e-07, 7.1428e-09, 5.7961e-20, 5.8691e-19, 1.2595e-13, 3.5792e-05,
         1.7723e-12, 9.2683e-12, 1.6414e-04, 4.1129e-13],
        [2.9842e-10, 1.6025e-04, 1.9089e-03, 8.4502e-05, 3.7876e+00, 3.2111e-02,
         1.0491e-06, 3.1235e+00, 1.8971e-05, 1.6429e-05, 1.0192e-06, 5.3191e-09,
         2.8227e-01, 2.3120e-03, 2.1980e-06, 2.7224e-03, 4.8043e-11, 9.1196e-12,
         1.9793e-01, 4.8384e-10, 7.3986e-08, 2.5646e-07, 6.0292e-04, 1.7166e-05,
         7.7345e-08, 3.4695e-10, 1.5845e-07, 1.8961e+03, 9.7944e-03, 3.5025e-07,
         3.8782e-07, 1.1972e-10, 6.6095e-05, 3.1684e-08, 1.1406e-12, 1.4559e-05,
         7.0044e-02, 9.2023e-02, 7.6433e-01, 7.9895e-06, 1.1018e-05, 7.2395e-06,
         5.7652e-05, 1.1000e-11, 1.4745e-03, 5.1623e+02, 5.5016e+02, 6.2235e-05,
         1.8067e-07, 4.5280e-09, 1.3458e-05, 3.1515e-08, 2.3496e-03, 1.2625e+01,
         2.4315e-07, 7.0457e-01, 1.4573e-02, 1.1436e-04, 3.9970e-06, 3.4418e-07,
         2.3644e-04, 2.8310e-05, 3.4391e-05, 3.2624e-11, 4.2854e-10, 3.4765e-04,
         2.6434e-02, 1.8348e-03, 3.1729e-05, 1.9872e-01, 2.1898e-10, 2.7326e-02,
         1.6172e-07, 1.7397e-05, 3.0815e-06, 4.3656e-05, 1.9655e-05, 2.3292e-04,
         5.8322e-03, 8.1896e-03, 8.3269e-09, 8.7261e-06, 5.1374e-01, 2.6633e-03,
         9.8896e-09, 1.0110e-03, 1.0611e-07, 1.1934e-09, 9.5698e-06, 1.6648e-04,
         2.2324e-07, 2.7515e-04, 1.0085e-06, 1.3247e-09, 3.3714e-03, 2.8126e-10,
         1.4273e-10, 1.3193e-08, 2.9583e-03, 5.4232e-05]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:55,880 - forward - INFO - Original Rs:
 tensor([[ 81.,   5.,  47.],
        [ 42.,   2.,   2.],
        [  0., 395.,  46.]], device='cuda:6')
2025-08-01 11:39:55,881 - forward - INFO - Generative mu:
 tensor([[56.7508, 60.1260, 53.7265],
        [56.7508, 60.1260, 53.7265],
        [10.3388, 43.6731, 38.2124]], device='cuda:6', dtype=torch.float64)
2025-08-01 11:39:55,882 - forward - INFO - X_hat theta:
 tensor([[4.2161e+07, 1.3595e+04, 2.6689e+07],
        [4.2161e+07, 1.3595e+04, 2.6689e+07],
        [1.9759e+07, 1.8411e+04, 7.3412e+05]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:55,883 - forward - INFO - JS-divergence omega orig 0.14330881665825462 learn 0.6042715302701324 flat8.195639361607795e-08
2025-08-01 11:39:55,885 - forward - INFO - average loss when compared to Xs: tensor([ 83640.7344, 106259.8672,  39318.8320,  54300.2578, 111407.1094,
          7261.6294,  10710.2656, 113928.9375,  42629.2188,  43342.1016,
         42295.5508,  49274.8398, 112565.0156,  46384.0156,  93218.0469,
        121041.3438,  26966.0098,  40365.9102, 106131.6562,  31650.8730,
         24544.5801,  93566.6562,  50757.9727, 123841.8750,  35823.6953,
         11976.2812,  98229.9141,  27425.9746,  37828.6289, 120952.1875,
         96495.7188, 100876.7734,  18006.2695,  37028.5664,  51785.0625,
         77719.8672,  35267.1953,  71530.8984,  61110.1562,  67401.8203,
         68716.2969,  92915.1719,  18853.3730,  87033.1250,   5077.3525,
         11643.8789,  43006.1406,  35668.2969,  36343.6953,  86871.8984,
          4430.2100, 103268.5000,  48919.5547,  48119.5078,  53183.6250,
         18471.9336,  11706.5098,  76174.0781,  95069.0234, 107815.8906,
        128403.0625, 110839.7656,  38847.3047,  98964.2969,  17514.6055,
         49643.5938,  44575.7656,  10786.1895,  20423.5801,  35457.7461,
         95659.6094,  26044.0684,  79000.5000, 120108.5781,  58652.0234,
         29874.3398,  14911.3730,  34638.4180,  29355.0469,  37805.4258,
         35640.9922,  47718.9922,  98067.3516,  56087.1250, 113673.5000,
        101812.0781,  82308.8594,  12522.1602,  92359.6094, 106837.8125,
        109746.1562,  22179.2578,  22163.0449, 106892.4688,  90432.5078,
         57004.2695,  32188.9746,  49731.0859, 108273.0312,  68379.9531],
       device='cuda:6', grad_fn=<MulBackward0>)
2025-08-01 11:39:55,885 - forward - INFO - -------------------------------- END --------------------------------
2025-08-01 11:39:55,886 - forward - INFO - 
2025-08-01 11:39:55,886 - forward - INFO - Loss Function         | Loss Value(s)        
2025-08-01 11:39:55,887 - forward - INFO - ---------------------------------------------
2025-08-01 11:39:55,887 - forward - INFO - Adj KL Lib:           |                   0.0
2025-08-01 11:39:55,887 - forward - INFO - Adj KL Gene:          |      3337.03564453125
2025-08-01 11:39:55,887 - forward - INFO - Adj KL X:             |      3272.79443359375
2025-08-01 11:39:55,888 - forward - INFO - Adj Recon Lib:        | 0.00e+00, 0.00e+00
2025-08-01 11:39:55,888 - forward - INFO - Adj Recon DA:         | 7.06e+06, 1.14e+07
2025-08-01 11:39:55,888 - forward - INFO - Adj Batch Correction: | 0.00e+00
2025-08-01 11:39:55,889 - forward - INFO - Adj Recon Express:    | 6.074e+06            
2025-08-01 11:39:55,903 - forward - INFO - 
Training Epoch: 003/1000 | Batch 0001/0001
2025-08-01 11:39:55,903 - forward - INFO - 
Average training loss per epoch 48956.5234375
2025-08-01 11:39:55,935 - forward - INFO - --------------++------ LOSS INFORMATION ------------------++--------
2025-08-01 11:39:55,935 - forward - INFO - X_hat mu:
 tensor([[2.3274e-16, 2.1852e-18, 5.7671e+03],
        [7.6856e-01, 2.1782e+01, 1.7233e+02],
        [2.5410e-12, 7.0045e-01, 5.4908e+02]], device='cuda:6')
2025-08-01 11:39:55,936 - forward - INFO - lib size factors:
 tensor([[5781.],
        [2099.],
        [4750.]], device='cuda:6')
2025-08-01 11:39:55,938 - forward - INFO - omega:
 tensor([[4.0260e-20, 3.7799e-22, 9.9759e-01],
        [3.6615e-04, 1.0377e-02, 8.2102e-02],
        [5.3494e-16, 1.4746e-04, 1.1560e-01]], device='cuda:6')
2025-08-01 11:39:55,943 - forward - INFO - omega * lib_size:
 tensor([[2.3274e-16, 2.1852e-18, 5.7671e+03, 2.9764e-12, 1.6868e-14, 8.3730e-10,
         2.4922e-25, 7.0421e-09, 4.4584e-14, 2.5542e-03, 2.2054e-14, 1.3440e-18,
         8.4835e-11, 1.5025e-10, 2.4787e-19, 1.1242e-06, 6.8309e-18, 1.2258e-13,
         4.6649e-06, 4.7023e-18, 5.6503e-08, 9.4169e-04, 1.1331e-16, 7.2052e-08,
         1.3222e-09, 1.3247e-16, 3.0796e-17, 1.7090e-11, 1.8974e-18, 1.8179e-12,
         3.3065e-13, 1.6376e-24, 8.8261e-16, 1.8845e-10, 3.9435e-06, 4.7352e-09,
         1.7237e-08, 1.1057e-11, 3.4000e-19, 1.2448e-02, 1.5977e-13, 1.4671e-06,
         4.3758e-02, 1.0973e-24, 1.6207e-17, 1.3753e+01, 8.7422e-08, 5.1229e-15,
         4.6209e-12, 2.1646e-09, 1.0473e-11, 1.0441e-05, 1.7304e-04, 5.3889e-22,
         3.7282e-07, 9.6917e-17, 3.9929e-16, 3.9775e-11, 1.6219e-06, 1.0371e-13,
         1.0092e-19, 6.5366e-18, 8.2397e-04, 6.6542e-16, 6.2922e-06, 2.6205e-05,
         9.7705e-10, 1.2444e-08, 3.5200e-08, 1.8005e-15, 8.4570e-12, 5.9436e-08,
         1.1055e-09, 8.5514e-07, 5.6498e-07, 2.0063e-08, 1.1854e-14, 2.8899e-10,
         8.2416e-10, 2.2020e-14, 6.3525e-18, 6.5117e-18, 1.4883e-08, 9.4479e-11,
         3.3782e-16, 3.1522e-05, 1.6083e-16, 6.2555e-14, 1.4580e-12, 8.8662e-02,
         1.8144e-03, 6.9458e-04, 2.9177e-15, 2.3372e-18, 1.6089e-11, 3.2368e-11,
         1.5259e-07, 8.5882e-17, 3.3683e-10, 4.5802e-09],
        [7.6856e-01, 2.1782e+01, 1.7233e+02, 1.7169e+01, 8.8523e+01, 1.9549e+01,
         2.8976e+00, 3.6197e+01, 8.7533e+00, 1.1316e+01, 1.9955e+01, 1.0155e+01,
         1.5125e+01, 1.5954e+01, 9.8124e+00, 1.8489e+01, 5.3547e-01, 3.3646e-01,
         4.5313e+01, 3.7631e+00, 5.6645e+00, 4.8753e+00, 7.9047e+00, 8.7635e+00,
         3.4358e+00, 6.5454e+00, 2.4393e+00, 2.0616e+02, 1.6016e+01, 5.6542e+00,
         5.9914e+00, 1.3821e+00, 1.8214e+01, 2.3373e+00, 1.6312e+00, 7.6240e+00,
         1.8475e+01, 4.1196e+01, 3.0026e+01, 1.1430e+01, 5.8355e+00, 1.9810e+00,
         1.1511e+02, 4.2287e-01, 1.5611e+01, 2.7966e+01, 3.5023e+01, 1.5326e+01,
         1.2151e+00, 3.2443e+00, 1.0528e+01, 2.4224e+00, 5.7056e+01, 3.2712e+01,
         1.0010e+00, 3.0373e+01, 1.6220e+01, 1.4874e+01, 7.8257e+00, 1.2563e+01,
         6.1756e+00, 3.9823e+01, 1.4916e+01, 8.5711e-01, 1.4964e+00, 7.6986e+00,
         1.9698e+01, 1.4450e+01, 3.3006e+01, 4.8960e+01, 3.9132e+00, 4.1787e+01,
         1.2197e+01, 8.3287e+00, 4.1838e+00, 2.0510e+01, 5.1588e+00, 5.2685e+01,
         8.9405e+00, 1.0327e+01, 2.0509e+00, 6.6352e+00, 5.9144e+01, 1.1954e+02,
         1.5869e+00, 9.4222e+00, 1.3886e+01, 4.4838e+00, 5.6175e+00, 1.0369e+01,
         1.6632e+00, 3.9961e+00, 1.8126e+01, 9.9183e-01, 1.5222e+01, 6.2639e+00,
         3.0820e+00, 3.1469e+00, 9.9890e+01, 2.6966e+01],
        [2.5410e-12, 7.0045e-01, 5.4908e+02, 2.6313e-02, 7.9035e+00, 3.1753e-04,
         4.3730e-03, 1.8627e+01, 1.1531e-04, 1.6255e-03, 8.2495e-05, 3.6884e-05,
         5.8078e+00, 5.2836e-03, 1.0343e-06, 1.7028e-01, 6.2986e-08, 1.8994e-05,
         1.4348e+03, 1.1211e-02, 6.4642e-07, 1.7283e+00, 7.3503e+01, 8.0261e-01,
         3.6408e-06, 2.8135e-06, 2.6392e-06, 7.0266e+01, 1.1805e+00, 2.5210e-02,
         4.1358e-04, 4.2423e-05, 2.0115e-03, 5.8379e-02, 1.8406e-07, 8.6938e-05,
         9.1278e-01, 4.0157e+01, 4.1762e-03, 2.8123e-03, 6.1426e-02, 1.1429e-04,
         3.1537e+01, 2.1335e-07, 3.1546e-03, 1.2522e+01, 2.4126e-01, 1.0613e-01,
         6.8687e-06, 4.6212e-09, 6.4652e-02, 1.1289e-02, 4.0190e-03, 2.4380e+03,
         3.7614e-06, 9.6565e-01, 1.5826e-02, 2.3753e-04, 3.6214e-05, 2.2826e-02,
         1.0491e-02, 2.6915e-02, 3.4300e-03, 2.5613e-08, 8.9545e-03, 5.5149e-03,
         3.7533e+01, 5.8452e-02, 6.1873e-02, 1.0117e-04, 3.0844e-03, 8.1083e-02,
         1.5507e-02, 2.0212e-02, 7.3267e-09, 3.4276e-05, 3.7459e-03, 1.0032e-02,
         4.9786e-01, 4.7949e-04, 8.9390e-04, 3.3809e-05, 1.5375e+01, 2.6251e-01,
         3.3257e-03, 8.7927e-02, 7.4775e-06, 1.8805e-04, 2.6696e-01, 2.1095e-01,
         3.5717e-07, 3.0253e-04, 6.2704e-04, 1.4225e-05, 5.9452e+00, 8.0552e-03,
         5.8068e-06, 1.0426e-08, 1.7472e-01, 1.9025e-02]], device='cuda:6')
2025-08-01 11:39:55,944 - forward - INFO - Original Rs:
 tensor([[451.,   2.,  25.],
        [  0.,  27.,   4.],
        [155.,   3.,  18.]], device='cuda:6')
2025-08-01 11:39:55,945 - forward - INFO - Generative mu:
 tensor([[56.7508, 60.1260, 53.7265],
        [10.3388, 43.6731, 38.2124],
        [56.7508, 60.1260, 53.7265]], device='cuda:6', dtype=torch.float64)
2025-08-01 11:39:55,946 - forward - INFO - X_hat theta:
 tensor([[1.3244e+07, 1.6800e+00, 3.8975e+07],
        [9.8235e+06, 7.1297e+01, 1.7686e+03],
        [1.3244e+07, 1.6800e+00, 3.8975e+07]], device='cuda:6')
2025-08-01 11:39:55,947 - forward - INFO - JS-divergence omega orig 0.14689707773992297 learn 0.5090194305350857 flat8.195639361607795e-08
2025-08-01 11:39:55,949 - forward - INFO - average loss when compared to Xs: tensor([117357.8984,   1952.4011,  48907.4805,  90037.3594,  79753.2344,
          2342.5540,  40536.5312,  87255.0234,   8110.1860,   2440.6846,
         72871.2031,  11291.9316,   2270.9922,  84438.5469,  21389.3047,
         95496.8750,   3817.2969, 106067.5781,  11664.3203,   4833.4258,
         90660.5781,  14876.4775, 101038.7812,  94113.0156,   3548.1240,
         63227.4062,   6222.9507,  11367.2070,  11151.8301,   2081.0950,
          2677.2263,  18279.2754,  36835.5469,   1557.0703,  64302.8516,
         30118.4434,  17624.6035, 106779.1641,  29616.7383,  94463.6406,
         68058.3125,   9085.8086,  12846.2305,  19656.9414,  76089.4219,
         16263.2188,  73097.2500,  36737.6094,  91656.2109, 108246.2812,
         49608.0391,   3833.1152,  42023.4453,  85174.3359,  68790.3594,
         90154.4609,  18695.2637, 111394.7656,   6932.7168,   6457.9897,
          5481.5312,   5457.1758,   5162.6768,  16322.6016, 116957.7031,
         65417.9297,  43194.7695,   2770.5264,   4641.1738,  32835.6562,
         11070.5078,   3766.5754,  64079.1016,  71115.5000,  13226.1396,
         39060.6406,  34507.6016,  68702.6641,  16830.5625,  24506.4648,
          6414.0137,  70627.9375, 118556.0156,   1822.9539,  20347.4258,
        102630.3828,  89471.9609,   1950.8889, 142583.0000,  41647.8125,
         20974.4023,  11802.2539, 101776.9531,  77063.6016,  16591.0547,
         39104.2109,  62329.0000,  95899.8750,  14449.5674,  83739.7656],
       device='cuda:6')
2025-08-01 11:39:55,949 - forward - INFO - -------------------------------- END --------------------------------
2025-08-01 11:39:55,950 - forward - INFO - 
2025-08-01 11:39:55,950 - forward - INFO - Loss Function         | Loss Value(s)        
2025-08-01 11:39:55,950 - forward - INFO - ---------------------------------------------
2025-08-01 11:39:55,950 - forward - INFO - Adj KL Lib:           |                   0.0
2025-08-01 11:39:55,950 - forward - INFO - Adj KL Gene:          |    1081.1993408203125
2025-08-01 11:39:55,951 - forward - INFO - Adj KL X:             |       1806.5498046875
2025-08-01 11:39:55,951 - forward - INFO - Adj Recon Lib:        | 0.00e+00, 0.00e+00
2025-08-01 11:39:55,951 - forward - INFO - Adj Recon DA:         | 1.39e+06, 4.51e+06
2025-08-01 11:39:55,952 - forward - INFO - Adj Batch Correction: | 0.00e+00
2025-08-01 11:39:55,952 - forward - INFO - Adj Recon Express:    | 4.453e+06            
2025-08-01 11:39:55,954 - forward - INFO - Validation Epoch: 003/1000 | Batch 0001/0002
2025-08-01 11:39:55,954 - forward - INFO - 
Average validation loss per epoch 10324.9423828125
2025-08-01 11:39:55,957 - forward - INFO - 
Time elapsed: 0.01 min
2025-08-01 11:39:55,959 - forward - INFO - 
batch X for epoch: 3 batch: 0
2025-08-01 11:39:55,959 - forward - INFO - Size: torch.Size([100, 100]) dtype: torch.float32 Min: tensor(0., device='cuda:6')Max: tensor(565., device='cuda:6')
2025-08-01 11:39:55,960 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:55,963 - forward - INFO - tensor([[ 81.,   5.,  47.,  ...,  71.,  41.,  40.],
        [ 42.,   2.,   2.,  ..., 101.,  62., 170.],
        [  0., 395.,  46.,  ...,   0.,  28.,   0.],
        ...,
        [ 21.,  36.,  39.,  ...,  50.,  10.,   0.],
        [ 29.,   5., 104.,  ...,  93.,   7.,  13.],
        [ 57.,  17., 103.,  ...,  61.,  74.,  19.]], device='cuda:6')
2025-08-01 11:39:55,964 - forward - INFO - Size: torch.Size([100, 100]) dtype: torch.float32 Min: tensor(0., device='cuda:6')Max: tensor(602., device='cuda:6')
2025-08-01 11:39:55,964 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:55,967 - forward - INFO - tensor([[ 14.,   0.,  25.,  ...,  45.,  63.,  33.],
        [110., 127.,  21.,  ..., 113.,   4.,  11.],
        [  3.,  48.,   4.,  ...,   4.,  15.,   9.],
        ...,
        [  1.,  44.,  25.,  ...,  14.,  42.,  33.],
        [  2.,   9.,   7.,  ...,  12.,   5.,  50.],
        [131.,  70.,  32.,  ..., 112.,   4.,   0.]], device='cuda:6')
2025-08-01 11:39:55,967 - forward - INFO - 
batch indices for epoch: 3 batch: 0
2025-08-01 11:39:55,969 - forward - INFO - Size: torch.Size([100]) dtype: torch.int64 Min: tensor(0, device='cuda:6')Max: tensor(99, device='cuda:6')
2025-08-01 11:39:55,970 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:55,972 - forward - INFO - tensor([10, 71,  5, 83, 73, 33, 25, 37, 81, 51, 11, 15, 82, 48,  2, 14,  7, 27,
        87,  3, 75, 22, 46, 97, 80, 86, 55, 18, 42, 45, 84, 41,  0, 63, 90, 24,
        89, 66, 13, 54, 40, 70, 95, 69, 60,  8, 79, 61, 96, 67, 23, 50, 32, 57,
        77, 88, 56, 36, 85,  1, 21, 17, 94, 62, 19, 74, 99, 65, 38, 98, 31, 52,
        68, 34, 16, 59, 12, 39, 26, 64, 91, 43, 30, 53, 93, 35,  9, 28, 78,  4,
        92, 58, 76, 49,  6, 72, 20, 44, 29, 47], device='cuda:6')
2025-08-01 11:39:55,972 - forward - INFO - Size: torch.Size([100]) dtype: torch.int64 Min: tensor(0, device='cuda:6')Max: tensor(99, device='cuda:6')
2025-08-01 11:39:55,974 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:55,977 - forward - INFO - tensor([10, 71,  5, 83, 73, 33, 25, 37, 81, 51, 11, 15, 82, 48,  2, 14,  7, 27,
        87,  3, 75, 22, 46, 97, 80, 86, 55, 18, 42, 45, 84, 41,  0, 63, 90, 24,
        89, 66, 13, 54, 40, 70, 95, 69, 60,  8, 79, 61, 96, 67, 23, 50, 32, 57,
        77, 88, 56, 36, 85,  1, 21, 17, 94, 62, 19, 74, 99, 65, 38, 98, 31, 52,
        68, 34, 16, 59, 12, 39, 26, 64, 91, 43, 30, 53, 93, 35,  9, 28, 78,  4,
        92, 58, 76, 49,  6, 72, 20, 44, 29, 47], device='cuda:6')
2025-08-01 11:39:56,008 - forward - INFO - --------------++------ LOSS INFORMATION ------------------++--------
2025-08-01 11:39:56,009 - forward - INFO - X_hat mu:
 tensor([[2.8970e-03, 3.5596e-17, 2.0163e-10],
        [8.4852e-21, 9.9838e-03, 2.0365e-05],
        [1.8378e-08, 9.2436e-10, 2.5779e-10]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,010 - forward - INFO - lib size factors:
 tensor([[5287.],
        [5314.],
        [2985.]], device='cuda:6')
2025-08-01 11:39:56,011 - forward - INFO - omega:
 tensor([[5.4795e-07, 6.7328e-21, 3.8136e-14],
        [1.5968e-24, 1.8788e-06, 3.8322e-09],
        [6.1568e-12, 3.0967e-13, 8.6361e-14]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,017 - forward - INFO - omega * lib_size:
 tensor([[2.8970e-03, 3.5596e-17, 2.0163e-10, 1.8358e-03, 1.0985e-12, 1.3394e-09,
         2.2275e-20, 2.3230e-06, 4.1297e-14, 4.9983e-07, 3.9111e-12, 3.5563e-17,
         4.2388e-16, 4.3625e-11, 6.9168e-16, 2.3219e-13, 2.2430e-06, 2.6029e-14,
         3.9096e-07, 9.1675e-07, 4.5250e-13, 2.4123e-02, 9.6765e-18, 1.4645e-01,
         3.9329e-19, 1.7729e-04, 7.7614e-18, 1.7507e-11, 2.2716e-16, 5.2765e-11,
         3.6028e-18, 3.8395e-12, 4.8558e-14, 5.4656e-11, 1.1971e-02, 3.1334e-08,
         3.1154e-18, 2.4491e-11, 3.0909e-09, 3.3770e-09, 6.6291e-08, 7.1443e-10,
         5.2747e-06, 1.2045e-09, 1.0447e-21, 3.0701e-07, 4.9870e-03, 9.9409e-19,
         4.5141e+02, 9.0326e-12, 1.2878e-09, 3.6512e-12, 1.2396e-12, 5.2912e-17,
         8.8770e-09, 1.5622e-18, 2.3633e-11, 3.0663e-08, 1.2921e-16, 2.1889e-14,
         1.3898e-06, 1.0745e-03, 6.9752e-09, 5.9863e-09, 1.8795e-04, 7.5377e-03,
         1.9316e-03, 8.6294e-04, 1.1220e+02, 4.0548e-08, 9.8329e-22, 5.1001e-15,
         1.6398e-11, 1.9480e-05, 4.4461e-08, 8.3495e-04, 2.0943e-19, 1.6243e-12,
         2.1750e-03, 3.1897e-04, 3.1039e-18, 3.4215e-03, 7.4695e-12, 1.1519e-09,
         7.8696e-12, 2.6837e-13, 3.4430e-15, 3.0412e-16, 2.6713e-06, 1.5106e-13,
         1.1444e-09, 9.2454e-09, 2.3996e-18, 2.1817e-16, 6.3415e-17, 2.3427e-01,
         9.6077e-05, 4.7229e+03, 5.3229e-05, 1.2059e-18],
        [8.4852e-21, 9.9838e-03, 2.0365e-05, 5.2592e-11, 3.7811e-07, 3.4230e-07,
         9.0322e-09, 3.3785e-10, 2.5109e-12, 1.2103e-06, 6.3534e-19, 1.5860e-14,
         3.7705e-05, 1.0771e-11, 3.4713e-14, 2.4770e-12, 3.7684e-13, 1.1282e-12,
         9.1790e-07, 1.2331e-07, 1.4796e-05, 1.6080e-14, 7.1090e-11, 3.5081e-03,
         9.5975e-05, 7.5171e-11, 1.5102e-14, 8.7719e-11, 1.6322e-02, 6.9350e-04,
         2.5277e-06, 1.3709e-04, 1.7488e-09, 6.2283e-04, 8.1446e-08, 1.9169e-05,
         1.8357e+00, 4.3190e-03, 5.7464e-08, 9.8223e-05, 4.0906e-11, 8.1780e-10,
         1.6774e-05, 1.4124e-25, 2.9767e-08, 3.6248e-03, 1.3297e-08, 1.9574e-10,
         4.8159e-10, 1.2623e-11, 4.6997e+03, 6.1826e-05, 5.1608e-05, 2.0602e-02,
         6.9636e-09, 4.9472e-07, 6.3807e-05, 1.2622e-02, 9.6594e-05, 4.4889e-05,
         4.0892e-06, 1.9082e-04, 2.1457e-08, 1.8838e-02, 5.9184e+02, 7.1970e-07,
         7.7844e-07, 3.4273e-10, 7.4978e-13, 9.6122e-17, 2.9764e-10, 1.5286e-04,
         1.3883e-12, 1.5659e-06, 3.9983e-08, 4.5273e-07, 3.7355e-10, 5.9563e-18,
         7.3267e-07, 2.0263e+01, 4.1750e-10, 4.4987e-20, 6.4887e-02, 1.6484e-02,
         7.7244e-14, 7.1367e-07, 4.6576e-12, 8.5055e-06, 7.6869e-07, 1.8149e-08,
         3.4936e-03, 2.9418e-11, 5.1849e-11, 1.9907e-09, 2.4533e-09, 4.8075e-07,
         9.3258e-14, 9.8511e-15, 1.5386e-01, 4.8847e-10],
        [1.8378e-08, 9.2436e-10, 2.5779e-10, 2.9033e-09, 1.8009e-07, 1.1059e-10,
         1.0138e-18, 1.7390e-01, 4.5487e-07, 3.8248e-07, 1.1777e-12, 1.4416e-17,
         5.6440e-08, 8.3415e-09, 7.0874e-12, 1.9721e-05, 7.2551e-02, 2.7413e-15,
         5.0377e-07, 1.5840e-12, 8.5343e-09, 4.4564e-07, 3.8301e-17, 9.8071e-07,
         1.5868e-04, 1.6818e-05, 9.5667e-08, 4.1317e-13, 4.4646e-14, 1.1505e-06,
         6.0049e-14, 4.6331e-14, 4.2468e-06, 6.9749e-14, 9.8032e-06, 1.1150e+01,
         7.0901e-06, 4.2591e-07, 1.1791e-09, 1.6487e-10, 3.2438e-16, 8.1002e-05,
         6.3367e-05, 1.9708e-17, 8.7006e-08, 1.1903e-02, 1.8963e+00, 4.5979e-15,
         1.0154e+01, 8.6483e-08, 1.4900e-05, 6.7162e-07, 3.2996e-09, 4.8893e-15,
         7.4679e-04, 7.5974e-11, 3.2086e-09, 5.4581e-02, 1.4148e-04, 7.0233e-07,
         5.5452e-04, 7.5338e-10, 1.7222e-12, 1.9301e-12, 7.4773e-09, 1.8495e-06,
         1.7471e-10, 1.2378e-02, 4.8178e-04, 6.1645e-04, 1.1733e-13, 6.1269e-03,
         3.2891e-06, 1.6801e-06, 3.8594e-09, 2.9614e+03, 7.4914e-11, 2.0771e-13,
         1.7281e-05, 1.1739e-09, 1.8759e-16, 7.5271e-14, 2.9247e-12, 2.8292e-04,
         5.1481e-18, 4.7505e-11, 3.0482e-11, 6.1383e-15, 1.2033e-10, 8.3917e-12,
         3.4310e-02, 9.1102e-09, 4.2108e-09, 7.7237e-12, 2.9835e-11, 1.2945e-08,
         9.2640e-03, 3.8417e-08, 4.4735e-05, 9.0783e-10]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,018 - forward - INFO - Original Rs:
 tensor([[ 81.,   5.,  47.],
        [ 42.,   2.,   2.],
        [  0., 395.,  46.]], device='cuda:6')
2025-08-01 11:39:56,019 - forward - INFO - Generative mu:
 tensor([[56.7508, 60.1260, 53.7265],
        [56.7508, 60.1260, 53.7265],
        [10.3388, 43.6731, 38.2124]], device='cuda:6', dtype=torch.float64)
2025-08-01 11:39:56,021 - forward - INFO - X_hat theta:
 tensor([[3.1005e+07, 2.1098e+03, 1.4902e+07],
        [3.1005e+07, 2.1098e+03, 1.4902e+07],
        [9.2187e+06, 8.1856e+00, 1.2196e+02]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,021 - forward - INFO - JS-divergence omega orig 0.14330881665825462 learn 0.5650660794227336 flat8.195639361607795e-08
2025-08-01 11:39:56,023 - forward - INFO - average loss when compared to Xs: tensor([ 98290.2422,  94583.6250,  50806.8906,  34510.9766, 112576.3906,
          9350.2871,   7359.0684, 112180.4688,  26311.4180,  37579.4375,
         25348.6777,  28968.2305,  95759.1094,  30423.2734, 101210.6719,
         99446.1875,  36292.7344,  16119.9326, 120051.9922,  21705.5625,
         11526.9404,  88016.9297,  24248.1992,  37204.8516,  82371.0781,
          5818.4648,  95242.5391,   8825.5859,  39176.0312,  49751.2500,
        113871.5312,  82251.0625,   8754.5918,   4968.7593,  51232.3984,
         89452.6719,   7084.1343,  70053.7969,  77334.9922,  51644.4922,
         65774.0938, 106101.8438,  30492.6621,  43821.0859,   2303.5762,
         18821.4121,  14687.5859,  16626.9980,  38163.0742,  76837.9688,
          3128.1426,  94788.2031,  42204.3125,  39819.2344,  48091.8711,
         19151.4141,   7655.3604,  50940.7031, 116829.9531, 119738.9062,
         95914.5781, 100354.7422,  22166.7734,  80710.7578,   3606.1025,
        113175.7500,  61585.6797,  23638.3281,  33358.4961,  39259.3438,
        100248.6250,  74778.7188,  62580.6055, 107135.0781,  45085.7969,
         35821.4922,  24065.6992,  27636.1992,   5716.9434,  38668.9258,
         28872.4746,  11591.4717, 110784.8672,  35710.1719, 110726.9766,
        105452.7734, 108673.1875,   6114.3491,  36013.4570,  66682.0547,
         90647.1953,  19436.3203,   9002.8154,  35557.9844,  80203.3750,
         33077.3789,  16742.7656,  43871.1680,  28413.4922,  62663.4883],
       device='cuda:6', grad_fn=<MulBackward0>)
2025-08-01 11:39:56,023 - forward - INFO - -------------------------------- END --------------------------------
2025-08-01 11:39:56,024 - forward - INFO - 
2025-08-01 11:39:56,024 - forward - INFO - Loss Function         | Loss Value(s)        
2025-08-01 11:39:56,024 - forward - INFO - ---------------------------------------------
2025-08-01 11:39:56,024 - forward - INFO - Adj KL Lib:           |                   0.0
2025-08-01 11:39:56,024 - forward - INFO - Adj KL Gene:          |    2028.7012939453125
2025-08-01 11:39:56,026 - forward - INFO - Adj KL X:             |     2436.629150390625
2025-08-01 11:39:56,026 - forward - INFO - Adj Recon Lib:        | 0.00e+00, 0.00e+00
2025-08-01 11:39:56,026 - forward - INFO - Adj Recon DA:         | 3.73e+06, 7.71e+06
2025-08-01 11:39:56,026 - forward - INFO - Adj Batch Correction: | 0.00e+00
2025-08-01 11:39:56,027 - forward - INFO - Adj Recon Express:    | 5.249e+06            
2025-08-01 11:39:56,042 - forward - INFO - 
Training Epoch: 004/1000 | Batch 0001/0001
2025-08-01 11:39:56,042 - forward - INFO - 
Average training loss per epoch 33277.73828125
2025-08-01 11:39:56,073 - forward - INFO - --------------++------ LOSS INFORMATION ------------------++--------
2025-08-01 11:39:56,074 - forward - INFO - X_hat mu:
 tensor([[1.6308e-16, 5.3906e-13, 5.2336e-04],
        [5.5202e+00, 1.5326e+01, 8.3885e+01],
        [3.9437e-02, 3.9292e-06, 1.2112e-04]], device='cuda:6')
2025-08-01 11:39:56,075 - forward - INFO - lib size factors:
 tensor([[5781.],
        [2099.],
        [4750.]], device='cuda:6')
2025-08-01 11:39:56,076 - forward - INFO - omega:
 tensor([[2.8209e-20, 9.3248e-17, 9.0532e-08],
        [2.6299e-03, 7.3016e-03, 3.9964e-02],
        [8.3025e-06, 8.2720e-10, 2.5500e-08]], device='cuda:6')
2025-08-01 11:39:56,082 - forward - INFO - omega * lib_size:
 tensor([[1.6308e-16, 5.3906e-13, 5.2336e-04, 2.9332e-03, 2.9241e+02, 4.4233e-09,
         7.2874e-09, 4.1330e+02, 9.7518e-10, 3.4575e-04, 1.8120e-16, 7.7159e-15,
         2.3768e-04, 5.6989e-05, 2.5174e-09, 2.2373e-13, 8.4118e-06, 4.5935e-19,
         2.5148e+02, 1.3678e-05, 1.2371e-09, 1.2082e-11, 1.0901e-10, 1.0046e-01,
         2.6842e-04, 1.2699e-07, 7.4052e-13, 3.6635e-13, 5.8987e-02, 1.0882e-02,
         5.6629e-02, 4.6461e-07, 9.7704e-08, 1.0025e-07, 7.2537e-12, 4.2065e+02,
         1.3344e-02, 8.5875e-02, 1.5363e-08, 5.1327e-03, 1.6274e-05, 2.3706e-06,
         3.0060e+03, 9.1066e-18, 1.4665e-09, 8.6306e-04, 2.1358e-10, 9.1730e-07,
         2.3923e-01, 7.9331e-12, 5.2676e-04, 8.2515e-08, 1.3494e+01, 6.0351e-12,
         2.9756e-04, 4.6968e-10, 4.6452e-03, 2.0562e-01, 2.8398e+00, 4.6972e-01,
         2.8141e+00, 1.3241e-04, 1.0894e-03, 5.7371e-10, 2.2738e-01, 1.3404e+03,
         1.9840e-02, 5.6640e-02, 8.2891e-10, 8.9720e-15, 1.3559e-05, 7.3216e-04,
         4.8580e-04, 3.5953e+01, 2.0135e-08, 1.6830e-04, 1.4585e-03, 1.2925e-08,
         1.7637e-04, 5.0771e-02, 6.1307e-13, 5.5626e-15, 1.0842e-04, 5.7735e-02,
         2.1486e-15, 6.2557e-10, 2.3832e-11, 1.0486e-05, 8.9312e-06, 8.8596e-07,
         1.3826e-07, 1.2948e-14, 6.6763e-10, 1.7408e-11, 6.9854e-11, 1.0085e-02,
         2.7861e-09, 3.1252e-10, 7.6002e-03, 1.2022e-06],
        [5.5202e+00, 1.5326e+01, 8.3885e+01, 1.1235e+01, 1.4522e+01, 1.8613e+01,
         9.1451e-01, 2.0858e+01, 2.9889e+01, 1.5644e+01, 9.8352e+01, 8.6122e+00,
         2.6895e+00, 7.6501e+00, 5.3784e+00, 2.0584e+01, 8.8860e+00, 2.4687e+00,
         7.5696e+01, 2.0848e+01, 5.6747e+00, 5.6919e+00, 2.9161e+01, 8.2610e+00,
         5.4501e+00, 1.3098e+01, 7.5548e+00, 1.6839e+01, 6.7486e+00, 1.5165e+01,
         4.1434e+00, 2.5360e+00, 7.9505e+00, 1.4601e+00, 2.0289e+01, 7.2521e+00,
         4.1123e+00, 5.7438e+01, 1.1141e+01, 1.7377e+01, 1.6082e+01, 2.8144e+00,
         6.7777e+01, 3.9512e+00, 4.8455e+01, 1.9589e+01, 5.9394e+01, 1.4878e+01,
         9.5573e+00, 2.0875e+01, 1.0592e+01, 2.0385e+00, 1.7562e+01, 1.4005e+01,
         2.3779e+00, 2.1602e+00, 3.6805e+01, 8.8069e+00, 1.4217e+01, 1.3261e+01,
         1.1004e+01, 4.9756e+01, 9.1201e+00, 2.2403e+00, 2.7314e+00, 1.2062e+01,
         3.1935e+01, 1.3146e+01, 5.1287e+01, 8.4468e+01, 2.5308e+01, 5.4539e+01,
         3.4820e+01, 1.4120e+01, 1.0109e+01, 4.4302e+01, 4.2061e+00, 4.7662e+01,
         1.7302e+01, 1.2912e+01, 5.5287e+00, 9.2904e+00, 2.6633e+01, 4.7693e+01,
         3.6694e+00, 3.6814e+01, 1.9708e+01, 2.1433e+00, 6.8339e+00, 1.1127e+02,
         3.4868e+00, 8.6610e+00, 2.2416e+01, 2.7517e+00, 8.9978e+00, 3.6698e+01,
         4.0337e+00, 3.0058e+01, 7.1898e+01, 7.2761e+00],
        [3.9437e-02, 3.9292e-06, 1.2112e-04, 4.4507e-07, 1.3315e-07, 2.2828e-05,
         9.1989e-14, 2.3921e-01, 1.5542e-02, 4.6402e-04, 1.0017e-02, 1.2026e-09,
         2.4904e-10, 1.4029e-06, 9.3307e-10, 6.9287e-05, 3.7842e-01, 3.8365e-08,
         6.0542e-02, 2.8005e-04, 1.2868e-09, 2.1292e-02, 1.0173e-06, 3.7128e-03,
         3.3238e-06, 7.0069e-03, 1.7382e-06, 3.0142e-10, 3.4193e-13, 9.5978e-03,
         2.7735e-11, 1.5051e-08, 4.3704e-06, 2.0778e-09, 1.8159e+00, 2.0946e-03,
         1.5793e-07, 5.6626e-06, 6.5451e-04, 4.8196e-03, 5.2345e-04, 2.1796e-04,
         4.2189e-03, 4.3771e-08, 3.4461e-02, 4.1778e-03, 1.4410e+03, 3.7773e-08,
         1.3658e-03, 3.0181e-01, 8.1838e-07, 4.5031e-05, 4.2388e-05, 9.2101e-07,
         2.3384e-07, 4.7124e-11, 4.0528e-04, 1.0014e-03, 1.7615e-04, 2.8462e-05,
         3.7215e-03, 4.7895e-03, 5.5641e-07, 2.1423e-07, 5.3647e-05, 8.0521e-03,
         4.4437e-04, 4.0757e+00, 2.8135e+03, 1.6217e-01, 3.4885e-06, 2.5280e-03,
         7.7983e-01, 7.0389e-02, 7.8605e-03, 4.6283e+02, 6.0297e-06, 6.2658e-08,
         3.7685e-02, 4.0132e-03, 4.5654e-07, 5.1658e-07, 1.5659e-04, 3.2958e-03,
         4.5304e-09, 1.0422e-04, 7.4508e-07, 5.2914e-11, 1.4972e-06, 7.3136e-02,
         3.1395e-04, 6.3935e-07, 8.1625e-06, 1.2570e-07, 8.3815e-10, 1.5993e+00,
         1.8330e-04, 2.2897e+01, 6.5523e-03, 5.4324e-09]], device='cuda:6')
2025-08-01 11:39:56,083 - forward - INFO - Original Rs:
 tensor([[451.,   2.,  25.],
        [  0.,  27.,   4.],
        [155.,   3.,  18.]], device='cuda:6')
2025-08-01 11:39:56,084 - forward - INFO - Generative mu:
 tensor([[56.7508, 60.1260, 53.7265],
        [10.3388, 43.6731, 38.2124],
        [56.7508, 60.1260, 53.7265]], device='cuda:6', dtype=torch.float64)
2025-08-01 11:39:56,085 - forward - INFO - X_hat theta:
 tensor([[1.1459e+07, 7.0327e+00, 9.7035e+06],
        [2.0355e+07, 7.5145e-01, 6.5957e-01],
        [1.1459e+07, 7.0327e+00, 9.7035e+06]], device='cuda:6')
2025-08-01 11:39:56,086 - forward - INFO - JS-divergence omega orig 0.14689707773992297 learn 0.43431447437730464 flat8.195639361607795e-08
2025-08-01 11:39:56,088 - forward - INFO - average loss when compared to Xs: tensor([ 89916.6328,   1203.3521,  69260.0156,  51003.8945,  44311.3320,
          4239.8379,  13073.4414,  97921.0703,   2071.8281,   1426.5240,
         76440.3750,  15877.3457,   2529.8564,  11836.7910,   5113.9199,
         23670.2227,   1226.2577,  57411.4141,   6251.5137,   3518.8052,
         98344.6094,   6602.5552,  90357.0703,  62259.9141,   1790.1394,
         15360.3691,   6109.7539,   6378.2310,  18587.8242,   1645.2965,
          2390.6270,   2020.8485,  22633.7031,   3430.4170,  60841.3516,
          8319.4883,   4736.3276,  28912.5430,  18098.3477,  41337.8281,
         67328.1094,   4226.0830,  11823.7188,   4392.0830,   8122.8652,
          1966.1952,  52594.6719,  10707.4424,  81668.1094,  22712.9922,
         10510.4453,   1482.9614,  29003.0430,  11451.2168,  31984.6914,
         66374.9219,   7028.7012,  94266.5625,   3627.7070,   2714.4636,
         23713.2363,   1613.3481,   2591.7441,  29283.3809,  36084.7109,
         31750.5469,  73024.1719,   2178.6453,  18677.1250,  67588.2812,
          1128.3704,   8738.7207,  24489.2148,  97870.1875,  12135.3066,
         40483.5664,  20909.4551,  47289.9414,  17451.1250,   2967.1572,
          1159.2155,  14510.9590, 110550.2031,   1004.2561,  20236.4453,
         17646.3125,  37293.7930,   2398.6873, 131573.2812,  16819.3457,
          2910.0171,   7215.1782,  67295.0312,  41107.6172,  22225.6680,
         17890.1289,  53029.8125,  56397.1875,   8990.7158,  98513.0156],
       device='cuda:6')
2025-08-01 11:39:56,088 - forward - INFO - -------------------------------- END --------------------------------
2025-08-01 11:39:56,089 - forward - INFO - 
2025-08-01 11:39:56,089 - forward - INFO - Loss Function         | Loss Value(s)        
2025-08-01 11:39:56,089 - forward - INFO - ---------------------------------------------
2025-08-01 11:39:56,089 - forward - INFO - Adj KL Lib:           |                   0.0
2025-08-01 11:39:56,089 - forward - INFO - Adj KL Gene:          |     524.0953979492188
2025-08-01 11:39:56,091 - forward - INFO - Adj KL X:             |    1344.6402587890625
2025-08-01 11:39:56,091 - forward - INFO - Adj Recon Lib:        | 0.00e+00, 0.00e+00
2025-08-01 11:39:56,091 - forward - INFO - Adj Recon DA:         | 8.78e+05, 2.24e+06
2025-08-01 11:39:56,091 - forward - INFO - Adj Batch Correction: | 0.00e+00
2025-08-01 11:39:56,092 - forward - INFO - Adj Recon Express:    | 2.891e+06            
2025-08-01 11:39:56,092 - forward - INFO - Validation Epoch: 004/1000 | Batch 0001/0002
2025-08-01 11:39:56,093 - forward - INFO - 
Average validation loss per epoch 5989.44921875
2025-08-01 11:39:56,096 - forward - INFO - 
Time elapsed: 0.02 min
2025-08-01 11:39:56,097 - forward - INFO - 
batch X for epoch: 4 batch: 0
2025-08-01 11:39:56,098 - forward - INFO - Size: torch.Size([100, 100]) dtype: torch.float32 Min: tensor(0., device='cuda:6')Max: tensor(565., device='cuda:6')
2025-08-01 11:39:56,099 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,102 - forward - INFO - tensor([[ 81.,   5.,  47.,  ...,  71.,  41.,  40.],
        [ 42.,   2.,   2.,  ..., 101.,  62., 170.],
        [  0., 395.,  46.,  ...,   0.,  28.,   0.],
        ...,
        [ 21.,  36.,  39.,  ...,  50.,  10.,   0.],
        [ 29.,   5., 104.,  ...,  93.,   7.,  13.],
        [ 57.,  17., 103.,  ...,  61.,  74.,  19.]], device='cuda:6')
2025-08-01 11:39:56,103 - forward - INFO - Size: torch.Size([100, 100]) dtype: torch.float32 Min: tensor(0., device='cuda:6')Max: tensor(602., device='cuda:6')
2025-08-01 11:39:56,103 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,107 - forward - INFO - tensor([[ 14.,   0.,  25.,  ...,  45.,  63.,  33.],
        [110., 127.,  21.,  ..., 113.,   4.,  11.],
        [  3.,  48.,   4.,  ...,   4.,  15.,   9.],
        ...,
        [  1.,  44.,  25.,  ...,  14.,  42.,  33.],
        [  2.,   9.,   7.,  ...,  12.,   5.,  50.],
        [131.,  70.,  32.,  ..., 112.,   4.,   0.]], device='cuda:6')
2025-08-01 11:39:56,107 - forward - INFO - 
batch indices for epoch: 4 batch: 0
2025-08-01 11:39:56,107 - forward - INFO - Size: torch.Size([100]) dtype: torch.int64 Min: tensor(0, device='cuda:6')Max: tensor(99, device='cuda:6')
2025-08-01 11:39:56,108 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,111 - forward - INFO - tensor([10, 71,  5, 83, 73, 33, 25, 37, 81, 51, 11, 15, 82, 48,  2, 14,  7, 27,
        87,  3, 75, 22, 46, 97, 80, 86, 55, 18, 42, 45, 84, 41,  0, 63, 90, 24,
        89, 66, 13, 54, 40, 70, 95, 69, 60,  8, 79, 61, 96, 67, 23, 50, 32, 57,
        77, 88, 56, 36, 85,  1, 21, 17, 94, 62, 19, 74, 99, 65, 38, 98, 31, 52,
        68, 34, 16, 59, 12, 39, 26, 64, 91, 43, 30, 53, 93, 35,  9, 28, 78,  4,
        92, 58, 76, 49,  6, 72, 20, 44, 29, 47], device='cuda:6')
2025-08-01 11:39:56,111 - forward - INFO - Size: torch.Size([100]) dtype: torch.int64 Min: tensor(0, device='cuda:6')Max: tensor(99, device='cuda:6')
2025-08-01 11:39:56,112 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,114 - forward - INFO - tensor([10, 71,  5, 83, 73, 33, 25, 37, 81, 51, 11, 15, 82, 48,  2, 14,  7, 27,
        87,  3, 75, 22, 46, 97, 80, 86, 55, 18, 42, 45, 84, 41,  0, 63, 90, 24,
        89, 66, 13, 54, 40, 70, 95, 69, 60,  8, 79, 61, 96, 67, 23, 50, 32, 57,
        77, 88, 56, 36, 85,  1, 21, 17, 94, 62, 19, 74, 99, 65, 38, 98, 31, 52,
        68, 34, 16, 59, 12, 39, 26, 64, 91, 43, 30, 53, 93, 35,  9, 28, 78,  4,
        92, 58, 76, 49,  6, 72, 20, 44, 29, 47], device='cuda:6')
2025-08-01 11:39:56,145 - forward - INFO - --------------++------ LOSS INFORMATION ------------------++--------
2025-08-01 11:39:56,146 - forward - INFO - X_hat mu:
 tensor([[6.7067e-01, 3.8021e-02, 1.8739e+00],
        [7.4308e-15, 1.9636e+00, 3.5880e-03],
        [3.5062e-10, 6.5376e-01, 1.7409e-02]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,147 - forward - INFO - lib size factors:
 tensor([[5287.],
        [5314.],
        [2985.]], device='cuda:6')
2025-08-01 11:39:56,149 - forward - INFO - omega:
 tensor([[1.2685e-04, 7.1914e-06, 3.5443e-04],
        [1.3983e-18, 3.6951e-04, 6.7520e-07],
        [1.1746e-13, 2.1902e-04, 5.8321e-06]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,155 - forward - INFO - omega * lib_size:
 tensor([[6.7067e-01, 3.8021e-02, 1.8739e+00, 1.0931e-02, 5.0947e-08, 2.3036e-02,
         5.4693e-08, 5.0156e-04, 1.8960e-03, 2.9158e-01, 1.1021e+00, 3.3308e-07,
         3.9606e-09, 3.4296e-06, 4.2806e-07, 3.3356e-02, 4.4679e-01, 6.6493e-02,
         5.4090e+02, 3.5777e-02, 8.6235e-08, 1.4609e+00, 2.0044e+01, 2.1812e-03,
         3.4541e-08, 9.4165e-03, 1.9743e-05, 1.7536e-05, 9.3158e-10, 4.2377e-01,
         4.7671e-04, 9.5902e-05, 7.4750e-05, 7.0443e-04, 4.1084e+02, 2.2464e-07,
         2.3884e-05, 7.1676e-03, 8.4611e-01, 3.4393e-01, 5.5140e+01, 1.4674e-06,
         2.6187e-04, 2.5998e-06, 1.5852e-01, 4.1358e-06, 2.3229e+01, 8.3350e-07,
         4.1989e-05, 4.2780e+00, 1.6544e-05, 9.6550e-03, 5.0709e-04, 2.6744e-02,
         2.4504e-05, 1.0248e-10, 2.4567e-06, 1.7302e-03, 5.9014e-06, 4.0211e-04,
         3.2108e-03, 1.7985e-03, 3.4662e-04, 7.0087e-03, 4.3101e-03, 2.7913e-03,
         7.4153e-01, 2.4602e+00, 1.4856e+00, 1.2437e-03, 7.2038e-02, 1.4755e-04,
         2.0592e-02, 3.3240e-01, 7.0818e+02, 4.9317e-02, 8.5528e-07, 4.5989e-02,
         1.8402e+01, 5.1255e-03, 1.5704e-02, 1.0153e-01, 4.2604e-01, 4.1567e+00,
         6.8713e-02, 1.1888e+00, 6.0377e-03, 1.0936e-07, 3.9794e-03, 3.4867e+03,
         1.0589e-04, 2.6461e-03, 2.2212e-04, 1.3117e-06, 5.4655e-06, 4.2930e-02,
         8.9015e-05, 1.4848e-01, 1.0882e-03, 5.8370e-04],
        [7.4308e-15, 1.9636e+00, 3.5880e-03, 1.0051e-02, 2.8771e-05, 5.4594e-12,
         1.5433e-12, 3.3357e-09, 3.6938e-11, 1.1695e-03, 1.6095e-11, 2.7815e-13,
         1.7366e-06, 8.5088e-03, 6.2196e-11, 2.3246e-09, 7.9252e-10, 4.6118e-07,
         6.0499e-06, 2.0462e-09, 3.0028e-08, 2.2262e-02, 7.9831e+00, 2.5746e-05,
         1.0623e-06, 9.0244e-11, 1.4741e-12, 2.9806e-10, 6.9506e-07, 9.2940e-10,
         4.1725e-12, 8.9562e-15, 1.7060e-08, 7.8664e-07, 5.3403e-06, 4.5540e-06,
         8.2706e-09, 6.6830e-04, 1.5612e-08, 9.0407e-08, 2.5831e-09, 3.8265e-17,
         6.1087e-01, 1.3357e-10, 1.8613e-06, 8.6819e-06, 3.6930e+03, 1.7776e-06,
         3.0870e-04, 5.5656e-05, 1.0434e-02, 6.6638e-07, 2.7652e-04, 1.1738e-14,
         5.4044e-07, 6.6728e-11, 1.3771e-12, 5.3708e-07, 2.9629e-09, 6.1262e-11,
         2.2114e-06, 2.5607e-09, 2.2308e-09, 2.2880e-11, 4.6267e-05, 7.8042e-06,
         2.4956e+01, 5.5049e-09, 5.3880e-03, 5.9395e-11, 1.4518e+03, 2.7996e-07,
         2.3119e-06, 2.5764e-05, 5.0327e-07, 1.0629e-02, 2.3106e-10, 1.4155e-05,
         5.4642e+00, 2.1066e-10, 4.1587e-07, 6.6039e-14, 1.7585e-02, 7.7664e-03,
         1.2262e-13, 1.2706e+01, 1.1155e-12, 3.7595e-09, 1.3684e-02, 1.1540e+02,
         7.8084e-09, 1.2863e-12, 5.7348e-03, 1.0929e-14, 1.2975e-12, 3.1224e-05,
         4.5648e-15, 2.2303e-07, 2.2104e-03, 1.1877e-03],
        [3.5062e-10, 6.5376e-01, 1.7409e-02, 3.7625e-05, 1.0706e-02, 7.5936e-07,
         5.1826e-07, 3.2207e-03, 4.7949e-10, 2.9375e-02, 5.8286e-07, 1.2992e-05,
         3.5069e-02, 1.3944e-02, 7.4818e-07, 1.9525e-03, 2.2894e-02, 1.6003e-08,
         1.1263e-02, 5.5567e-06, 2.4600e-04, 9.9989e-08, 3.1857e-07, 3.1072e-01,
         5.6722e-01, 6.4881e-03, 2.1520e-08, 2.1487e-07, 4.1780e+00, 1.0478e+00,
         2.4526e-05, 1.8412e-02, 1.8964e-04, 8.8401e+02, 1.5482e-02, 1.8016e-03,
         4.9573e-01, 1.2980e-04, 8.7148e-03, 2.5217e+02, 3.1712e-06, 2.4112e-06,
         1.3663e+01, 1.1718e-10, 2.7975e-07, 3.2429e-01, 6.9775e-05, 7.2571e-05,
         3.0491e-03, 4.1714e-06, 1.2628e-01, 2.7150e-01, 1.1264e-01, 1.2168e-10,
         3.1431e-02, 4.4285e-02, 1.4119e-01, 1.4960e+00, 8.2626e-03, 1.5597e+03,
         2.4891e-03, 1.0671e+00, 5.2740e-06, 1.1626e-03, 7.3826e+00, 1.6230e-01,
         4.2239e-05, 1.1726e-01, 6.2110e-06, 1.0958e-09, 7.8485e-02, 2.7984e-05,
         8.2585e-05, 7.7954e-02, 6.9832e-02, 2.6342e-04, 1.1672e-04, 1.7576e-05,
         7.7276e-07, 1.4926e-03, 5.9732e-06, 2.5647e-09, 1.0109e-02, 2.2687e-01,
         3.2559e-09, 5.7573e-03, 4.5443e-08, 7.7405e-01, 1.4663e-02, 1.7272e+00,
         1.9024e+02, 1.8944e-06, 4.0797e-06, 9.5595e-06, 9.6410e-05, 1.0455e-01,
         4.3445e-02, 8.8813e-06, 6.3295e+01, 8.1961e-02]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,156 - forward - INFO - Original Rs:
 tensor([[ 81.,   5.,  47.],
        [ 42.,   2.,   2.],
        [  0., 395.,  46.]], device='cuda:6')
2025-08-01 11:39:56,157 - forward - INFO - Generative mu:
 tensor([[56.7508, 60.1260, 53.7265],
        [56.7508, 60.1260, 53.7265],
        [10.3388, 43.6731, 38.2124]], device='cuda:6', dtype=torch.float64)
2025-08-01 11:39:56,158 - forward - INFO - X_hat theta:
 tensor([[2.7501e+07, 5.1442e+02, 9.7574e+06],
        [2.7501e+07, 5.1442e+02, 9.7574e+06],
        [2.7183e+07, 2.2556e+00, 3.9647e+02]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,159 - forward - INFO - JS-divergence omega orig 0.14330881665825462 learn 0.5220454978208915 flat8.195639361607795e-08
2025-08-01 11:39:56,161 - forward - INFO - average loss when compared to Xs: tensor([ 54626.0156,  85497.2109,  31805.3652,  16923.3008,  61612.2656,
          2036.4724,  28499.8398,  17101.9453,  20802.2617,  31045.9648,
         21493.4980,  44260.6133,  81430.7656,  16900.4219,  98258.5078,
         76413.8125,  25617.8008,  15957.4902, 116306.0156,  29125.8945,
         12659.2852,  36234.1953,  62741.9297,  74022.6719,  67823.4531,
         12812.1074,  40227.5312,   9559.9082,  39298.8906,  42098.7578,
         76394.9688,  28547.7617,  14914.9902,   6813.8838,  27786.5586,
         86073.8984,   2721.6172, 122177.6094,  86105.6016, 106321.0156,
         19224.0801,  65924.3203,   9900.4795,  64663.7578,   6100.5659,
          6405.4316,  12480.0430,   4572.5889,  23178.4102,  77203.0312,
          3595.8867,  58932.4375,  58693.1641,  47906.1953,  73735.8828,
          9893.1777,   2445.1724,  60668.0547, 100278.0547,  72364.4453,
         75785.2969,  85689.6406,  28845.3633,  25757.9590,  19447.8008,
         27267.4688,  37741.2578,   6998.6191,  36394.7891,  25078.3789,
         31743.5391,  18719.0898,  38811.8281, 128470.0469,  12839.1055,
         27529.5859,   6384.9443,   3914.0232,   2875.7878,  66378.0547,
         35413.2891,   4504.3408, 113322.8594,  70252.0312, 102680.8438,
         61356.4531, 105527.1328,   9878.0215,  57285.4141,  35466.6836,
         38835.3164,   7858.8193,   1936.5984,  38018.2656,  37753.1484,
         34602.9609,  13792.8887,   6884.5591,  80369.6250,  67813.1719],
       device='cuda:6', grad_fn=<MulBackward0>)
2025-08-01 11:39:56,161 - forward - INFO - -------------------------------- END --------------------------------
2025-08-01 11:39:56,162 - forward - INFO - 
2025-08-01 11:39:56,162 - forward - INFO - Loss Function         | Loss Value(s)        
2025-08-01 11:39:56,162 - forward - INFO - ---------------------------------------------
2025-08-01 11:39:56,162 - forward - INFO - Adj KL Lib:           |                   0.0
2025-08-01 11:39:56,163 - forward - INFO - Adj KL Gene:          |    1309.5555419921875
2025-08-01 11:39:56,163 - forward - INFO - Adj KL X:             |       1922.4267578125
2025-08-01 11:39:56,164 - forward - INFO - Adj Recon Lib:        | 0.00e+00, 0.00e+00
2025-08-01 11:39:56,164 - forward - INFO - Adj Recon DA:         | 2.05e+06, 5.22e+06
2025-08-01 11:39:56,165 - forward - INFO - Adj Batch Correction: | 0.00e+00
2025-08-01 11:39:56,165 - forward - INFO - Adj Recon Express:    | 4.239e+06            
2025-08-01 11:39:56,183 - forward - INFO - 
Training Epoch: 005/1000 | Batch 0001/0001
2025-08-01 11:39:56,183 - forward - INFO - 
Average training loss per epoch 22961.927734375
2025-08-01 11:39:56,214 - forward - INFO - --------------++------ LOSS INFORMATION ------------------++--------
2025-08-01 11:39:56,215 - forward - INFO - X_hat mu:
 tensor([[2.0334e-10, 7.3386e-07, 2.9403e-02],
        [2.1799e-01, 4.3372e+00, 1.3306e+01],
        [4.8417e-01, 1.0800e+00, 7.0782e+00]], device='cuda:6')
2025-08-01 11:39:56,216 - forward - INFO - lib size factors:
 tensor([[5781.],
        [2099.],
        [4750.]], device='cuda:6')
2025-08-01 11:39:56,217 - forward - INFO - omega:
 tensor([[3.5173e-14, 1.2694e-10, 5.0861e-06],
        [1.0386e-04, 2.0663e-03, 6.3394e-03],
        [1.0193e-04, 2.2737e-04, 1.4901e-03]], device='cuda:6')
2025-08-01 11:39:56,223 - forward - INFO - omega * lib_size:
 tensor([[2.0334e-10, 7.3386e-07, 2.9403e-02, 3.0148e-09, 1.0152e-05, 8.0976e-03,
         1.7665e-09, 1.0711e+00, 3.9600e-08, 2.9739e-05, 1.0986e-07, 2.0294e-11,
         3.0587e-04, 6.4711e-08, 7.0297e-09, 1.8392e+03, 1.4245e-11, 7.5323e-14,
         2.9129e-06, 3.0157e-10, 4.5819e-04, 2.4014e-05, 1.6362e-05, 2.0380e-10,
         7.9034e-04, 8.3234e-10, 3.3902e-08, 7.8466e+02, 1.5219e-10, 3.5285e-11,
         5.9365e-07, 8.8829e-13, 1.3010e-05, 2.9984e-08, 1.5018e-08, 4.8994e-06,
         2.6966e-02, 6.3682e-05, 2.5417e+00, 2.7082e-07, 9.5278e-09, 1.4813e-08,
         2.6498e-04, 5.5616e-15, 2.2041e-06, 1.6430e+01, 2.9462e+03, 1.7446e-06,
         1.2746e-09, 1.6636e-06, 2.0028e-04, 1.3101e-05, 9.8446e-01, 1.4076e-07,
         3.5107e-10, 1.1998e-02, 8.7249e-06, 2.3839e-03, 3.5012e-07, 1.6371e-07,
         6.2960e-07, 1.0895e-06, 1.7241e-06, 6.3415e-14, 1.8273e-11, 1.9723e-05,
         9.1739e-05, 1.6445e-04, 1.9763e-08, 3.6206e-03, 8.2224e-10, 1.6505e+01,
         7.6336e-06, 6.8923e-04, 1.6235e+02, 9.1674e-05, 2.0439e-08, 6.5326e-06,
         4.3437e-01, 5.0568e-05, 4.0682e-06, 5.4282e-07, 1.0346e-01, 9.7779e+00,
         2.5933e-12, 5.4567e-01, 5.2028e-08, 1.6972e-07, 3.4488e-07, 3.3125e-03,
         6.4139e-06, 7.8617e-02, 2.9772e-07, 4.6424e-11, 8.2150e-05, 1.5049e-09,
         3.9503e-13, 6.4272e-08, 1.5409e-03, 2.1052e-02],
        [2.1799e-01, 4.3372e+00, 1.3306e+01, 1.5215e-01, 1.0922e+01, 3.1413e+00,
         7.1011e-03, 6.0150e+01, 1.1985e+01, 5.3603e-01, 4.1547e+01, 4.9022e-01,
         1.4783e+00, 1.3873e+00, 5.2494e-01, 3.0547e+01, 1.1703e-01, 4.0028e-02,
         1.7123e+02, 3.8131e+00, 1.5052e-01, 3.5392e-01, 5.7111e+00, 3.8151e-01,
         1.8333e-01, 2.0523e-01, 3.7094e-01, 9.0874e+01, 2.6884e-01, 9.3151e-01,
         6.1910e-02, 1.3424e-01, 2.0946e-02, 3.1191e-02, 2.2140e-01, 1.7842e-01,
         3.9977e-01, 3.5695e+01, 9.4706e+01, 2.7420e+01, 3.4777e+00, 2.9124e-01,
         1.3996e+01, 8.6996e-02, 3.8962e+01, 2.5930e+01, 6.9373e+02, 1.6971e+00,
         8.6888e-02, 1.4068e+00, 5.0704e-01, 1.7329e-01, 3.9526e+00, 5.1583e+00,
         1.7971e-02, 9.2848e-01, 8.1910e+01, 2.0969e+00, 6.2123e-01, 1.0926e+00,
         5.0968e+00, 1.9843e+01, 9.2495e-01, 1.1561e-02, 1.4048e-02, 5.1153e-01,
         1.4896e+01, 1.4596e+01, 1.7062e+01, 3.6345e+02, 5.4591e-01, 3.3749e+01,
         9.5824e+00, 2.0189e+00, 4.2544e+00, 5.9301e+00, 6.7591e-01, 1.7833e+00,
         4.9540e+00, 1.2948e+00, 1.3866e-01, 1.9560e+00, 7.9693e+00, 4.3351e+00,
         1.1131e-01, 2.1775e+00, 1.4274e+00, 4.4898e-02, 2.7286e-01, 5.7682e+01,
         4.4774e-02, 2.1442e+00, 2.5416e+00, 2.0015e-01, 8.5445e+00, 8.9551e-01,
         2.5528e-02, 5.4830e+00, 1.0603e+01, 8.5242e-01],
        [4.8417e-01, 1.0800e+00, 7.0782e+00, 1.5522e-02, 1.0433e+00, 4.8935e-01,
         2.2969e-04, 1.8604e+01, 6.1710e+00, 1.5621e+00, 8.1414e+01, 1.5243e-02,
         9.0848e-03, 1.6901e-01, 4.4917e-03, 3.9303e+00, 4.7641e+00, 1.9043e-02,
         1.7291e+03, 3.6739e+01, 2.0330e-02, 5.1675e-01, 1.1767e+01, 5.6665e-01,
         2.7816e-02, 8.9851e-01, 4.6224e-01, 1.1610e-02, 5.7499e-04, 4.8838e-01,
         2.9209e-03, 1.6365e-02, 4.7353e-03, 2.3098e-03, 3.9981e+01, 1.5693e-02,
         2.4681e-02, 3.5816e+01, 2.1784e+01, 2.7023e+01, 5.0345e-01, 3.1508e-01,
         1.0807e+01, 3.7147e-02, 2.2485e+02, 1.1630e+00, 1.2188e+03, 1.5828e+00,
         6.5795e-01, 2.5323e+01, 2.2047e-02, 3.4233e-02, 1.2040e+00, 1.3179e+00,
         9.7738e-04, 2.5281e-03, 5.4858e+00, 1.7138e-02, 1.0056e-02, 3.2351e-01,
         5.6250e-01, 2.7088e+01, 5.8464e-02, 1.1055e-03, 2.4491e-02, 1.2109e+00,
         4.2026e+00, 1.0843e+00, 1.1381e+02, 4.2764e+01, 1.9306e+01, 3.5828e+00,
         1.2136e+02, 1.4078e+00, 4.0324e+00, 4.8053e+02, 9.8931e-02, 2.0569e+00,
         1.0619e+00, 1.0630e+01, 1.0730e+00, 3.0914e-02, 5.8888e+00, 8.7752e+00,
         3.7538e-01, 1.7483e+00, 8.1063e-01, 1.1617e-03, 1.6057e-01, 3.0211e+02,
         1.9149e-02, 3.1090e-02, 1.6551e+00, 6.8875e-02, 1.7197e-02, 1.8283e+01,
         2.9407e-02, 4.6353e+01, 3.1082e+00, 1.1390e-02]], device='cuda:6')
2025-08-01 11:39:56,224 - forward - INFO - Original Rs:
 tensor([[451.,   2.,  25.],
        [  0.,  27.,   4.],
        [155.,   3.,  18.]], device='cuda:6')
2025-08-01 11:39:56,225 - forward - INFO - Generative mu:
 tensor([[56.7508, 60.1260, 53.7265],
        [10.3388, 43.6731, 38.2124],
        [56.7508, 60.1260, 53.7265]], device='cuda:6', dtype=torch.float64)
2025-08-01 11:39:56,226 - forward - INFO - X_hat theta:
 tensor([[9.7069e+06, 1.9075e+01, 5.9520e+06],
        [3.5894e+05, 1.5391e+00, 1.1215e+02],
        [9.7069e+06, 1.9075e+01, 5.9520e+06]], device='cuda:6')
2025-08-01 11:39:56,227 - forward - INFO - JS-divergence omega orig 0.14689707773992297 learn 0.4147799807227962 flat8.195639361607795e-08
2025-08-01 11:39:56,229 - forward - INFO - average loss when compared to Xs: tensor([ 95394.3750,   4623.3960,  24335.1719,  35285.9570,  21597.9863,
         12616.3652,  29697.6602, 109984.8594,   2505.4324,   1101.5046,
         45620.7148,  25449.6445,   1872.3591,   9005.2754,  11367.5068,
         39138.3164,   1074.9998,  34769.6836,  17102.9746,   1890.0608,
         51318.6484,  26862.2930,  52021.8359,  37126.3672,   3752.9409,
         23220.0977,  11095.6992,  13128.3574,  10124.4072,   1448.2700,
          4565.5625,   2881.9502,  38704.3438,  12164.8096,  36338.2188,
          2261.2249,   4350.7466,  64082.9609,  10965.2822,  31191.9707,
         31472.2441,   8256.0391,  10730.2285,   1615.7760,  15044.3438,
         23471.3906,  76450.2500,   1332.8325,  31089.0273,  52093.0703,
          9514.7578,   1184.5605,  61615.6836,  19244.3359,  48460.5703,
         42692.6016,  19528.9609,  78379.6484,  12937.6787,   5271.7031,
          4652.9067,   1297.6240,   1610.8302,  35632.8594,  51712.2188,
         21613.4551,  50823.3477,   2116.8413,  28450.5703,   7075.9961,
          5171.3755,   9202.0518,   6315.2734,  58228.3164,   2151.7852,
         28904.9375,  65088.8125,  10585.2080,  53512.4453,   2364.4705,
          1349.4280,  17179.7363,  84794.9922,   1180.2295,  19388.2559,
          2834.3738,  35398.4922,   1968.6748,  30467.4629,  26613.0430,
          1715.9183,   1789.2703,  58716.3828,  64721.7812,   6774.2275,
         19101.7422,  10542.4209,  43080.3359,   9109.1270,  37706.5859],
       device='cuda:6')
2025-08-01 11:39:56,229 - forward - INFO - -------------------------------- END --------------------------------
2025-08-01 11:39:56,230 - forward - INFO - 
2025-08-01 11:39:56,231 - forward - INFO - Loss Function         | Loss Value(s)        
2025-08-01 11:39:56,231 - forward - INFO - ---------------------------------------------
2025-08-01 11:39:56,232 - forward - INFO - Adj KL Lib:           |                   0.0
2025-08-01 11:39:56,232 - forward - INFO - Adj KL Gene:          |    243.14820861816406
2025-08-01 11:39:56,232 - forward - INFO - Adj KL X:             |      1090.36572265625
2025-08-01 11:39:56,232 - forward - INFO - Adj Recon Lib:        | 0.00e+00, 0.00e+00
2025-08-01 11:39:56,233 - forward - INFO - Adj Recon DA:         | 9.27e+05, 1.34e+06
2025-08-01 11:39:56,233 - forward - INFO - Adj Batch Correction: | 0.00e+00
2025-08-01 11:39:56,233 - forward - INFO - Adj Recon Express:    | 2.433e+06            
2025-08-01 11:39:56,234 - forward - INFO - Validation Epoch: 005/1000 | Batch 0001/0002
2025-08-01 11:39:56,235 - forward - INFO - 
Average validation loss per epoch 4691.56591796875
2025-08-01 11:39:56,237 - forward - INFO - 
Time elapsed: 0.02 min
2025-08-01 11:39:56,238 - forward - INFO - 
batch X for epoch: 5 batch: 0
2025-08-01 11:39:56,239 - forward - INFO - Size: torch.Size([100, 100]) dtype: torch.float32 Min: tensor(0., device='cuda:6')Max: tensor(565., device='cuda:6')
2025-08-01 11:39:56,240 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,243 - forward - INFO - tensor([[ 81.,   5.,  47.,  ...,  71.,  41.,  40.],
        [ 42.,   2.,   2.,  ..., 101.,  62., 170.],
        [  0., 395.,  46.,  ...,   0.,  28.,   0.],
        ...,
        [ 21.,  36.,  39.,  ...,  50.,  10.,   0.],
        [ 29.,   5., 104.,  ...,  93.,   7.,  13.],
        [ 57.,  17., 103.,  ...,  61.,  74.,  19.]], device='cuda:6')
2025-08-01 11:39:56,244 - forward - INFO - Size: torch.Size([100, 100]) dtype: torch.float32 Min: tensor(0., device='cuda:6')Max: tensor(602., device='cuda:6')
2025-08-01 11:39:56,245 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,248 - forward - INFO - tensor([[ 14.,   0.,  25.,  ...,  45.,  63.,  33.],
        [110., 127.,  21.,  ..., 113.,   4.,  11.],
        [  3.,  48.,   4.,  ...,   4.,  15.,   9.],
        ...,
        [  1.,  44.,  25.,  ...,  14.,  42.,  33.],
        [  2.,   9.,   7.,  ...,  12.,   5.,  50.],
        [131.,  70.,  32.,  ..., 112.,   4.,   0.]], device='cuda:6')
2025-08-01 11:39:56,248 - forward - INFO - 
batch indices for epoch: 5 batch: 0
2025-08-01 11:39:56,248 - forward - INFO - Size: torch.Size([100]) dtype: torch.int64 Min: tensor(0, device='cuda:6')Max: tensor(99, device='cuda:6')
2025-08-01 11:39:56,250 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,252 - forward - INFO - tensor([10, 71,  5, 83, 73, 33, 25, 37, 81, 51, 11, 15, 82, 48,  2, 14,  7, 27,
        87,  3, 75, 22, 46, 97, 80, 86, 55, 18, 42, 45, 84, 41,  0, 63, 90, 24,
        89, 66, 13, 54, 40, 70, 95, 69, 60,  8, 79, 61, 96, 67, 23, 50, 32, 57,
        77, 88, 56, 36, 85,  1, 21, 17, 94, 62, 19, 74, 99, 65, 38, 98, 31, 52,
        68, 34, 16, 59, 12, 39, 26, 64, 91, 43, 30, 53, 93, 35,  9, 28, 78,  4,
        92, 58, 76, 49,  6, 72, 20, 44, 29, 47], device='cuda:6')
2025-08-01 11:39:56,252 - forward - INFO - Size: torch.Size([100]) dtype: torch.int64 Min: tensor(0, device='cuda:6')Max: tensor(99, device='cuda:6')
2025-08-01 11:39:56,254 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,256 - forward - INFO - tensor([10, 71,  5, 83, 73, 33, 25, 37, 81, 51, 11, 15, 82, 48,  2, 14,  7, 27,
        87,  3, 75, 22, 46, 97, 80, 86, 55, 18, 42, 45, 84, 41,  0, 63, 90, 24,
        89, 66, 13, 54, 40, 70, 95, 69, 60,  8, 79, 61, 96, 67, 23, 50, 32, 57,
        77, 88, 56, 36, 85,  1, 21, 17, 94, 62, 19, 74, 99, 65, 38, 98, 31, 52,
        68, 34, 16, 59, 12, 39, 26, 64, 91, 43, 30, 53, 93, 35,  9, 28, 78,  4,
        92, 58, 76, 49,  6, 72, 20, 44, 29, 47], device='cuda:6')
2025-08-01 11:39:56,288 - forward - INFO - --------------++------ LOSS INFORMATION ------------------++--------
2025-08-01 11:39:56,289 - forward - INFO - X_hat mu:
 tensor([[1.0144e-14, 3.6468e-06, 1.1897e-01],
        [5.4518e-02, 6.4046e-01, 9.8346e+01],
        [1.0071e+00, 6.4106e+00, 1.1949e+01]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,290 - forward - INFO - lib size factors:
 tensor([[5287.],
        [5314.],
        [2985.]], device='cuda:6')
2025-08-01 11:39:56,292 - forward - INFO - omega:
 tensor([[1.9187e-18, 6.8976e-10, 2.2503e-05],
        [1.0259e-05, 1.2052e-04, 1.8507e-02],
        [3.3740e-04, 2.1476e-03, 4.0031e-03]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,298 - forward - INFO - omega * lib_size:
 tensor([[1.0144e-14, 3.6468e-06, 1.1897e-01, 1.5879e-01, 2.9536e+03, 2.6989e-07,
         3.2890e-02, 1.6251e+01, 2.6493e-10, 4.1115e-01, 7.9255e-09, 1.3593e-08,
         4.6543e+01, 4.5525e-02, 1.7459e-06, 5.6235e-05, 1.1062e-07, 9.3402e-11,
         1.1820e-01, 1.9041e-06, 1.1492e-02, 4.9015e-05, 5.8405e-11, 2.0552e-03,
         3.4777e-06, 2.0468e-08, 3.3162e-12, 6.9977e-05, 6.4477e-03, 9.1312e-03,
         5.0051e-04, 8.5863e-06, 1.1609e-04, 1.6680e-05, 1.1616e-11, 3.8751e+00,
         8.6852e-03, 3.3066e+00, 8.1600e-06, 1.3683e-02, 1.2156e-04, 1.4620e-02,
         1.7393e+03, 7.5916e-14, 6.0715e-05, 2.8703e-01, 1.0355e-07, 4.8538e-07,
         4.7702e-07, 6.4861e-10, 1.8421e-05, 3.5709e-03, 2.6375e+00, 1.5324e-02,
         1.9752e-02, 1.2945e-05, 5.4023e-03, 7.1377e-02, 1.4317e-01, 2.0966e+00,
         8.6813e-05, 1.2074e-03, 6.4766e+00, 1.8534e-06, 3.2210e-01, 1.4064e+00,
         3.3940e+01, 1.3410e-01, 3.7878e-07, 2.0628e-10, 2.8209e-04, 4.0483e-01,
         3.4116e-02, 2.3361e-01, 1.9570e-06, 1.1841e-05, 4.7551e+00, 1.6584e-03,
         1.0667e-01, 3.0723e-03, 3.8035e-09, 2.8419e-12, 4.6970e+02, 3.3589e-01,
         3.2580e-07, 5.4390e-04, 1.7322e-08, 2.8224e-02, 6.4279e-06, 4.7532e-03,
         5.8227e-06, 1.2571e-09, 8.8891e-06, 1.7307e-07, 1.1939e-05, 3.2934e-05,
         4.0249e-08, 7.5981e-09, 9.9589e-04, 2.8042e-04],
        [5.4518e-02, 6.4046e-01, 9.8346e+01, 8.7088e+02, 3.4274e+00, 1.0868e-01,
         1.2462e+00, 1.8308e+00, 4.1786e-01, 1.4657e+02, 2.0432e-01, 4.0675e+01,
         8.7989e-01, 7.2457e+00, 1.2094e-02, 2.1451e-02, 3.1648e-02, 5.8122e-01,
         2.3616e+00, 4.5700e-01, 1.0258e-02, 4.1417e+00, 1.0512e+01, 5.6206e-01,
         1.0599e-01, 1.0227e+01, 1.1894e+00, 9.2479e-01, 8.4002e+00, 2.9137e+00,
         7.0535e-01, 6.3332e-02, 4.6186e+01, 4.1457e+02, 1.6323e-01, 1.2719e+02,
         5.9529e+01, 2.6550e+01, 5.8285e-01, 3.4684e+00, 5.4279e-03, 3.1167e+01,
         1.7304e+01, 4.2329e-02, 3.7511e+01, 2.8787e+01, 2.8358e+01, 7.7545e-03,
         3.5771e+01, 4.2257e+01, 3.7765e-01, 7.4312e+00, 4.5776e+00, 6.1324e+00,
         7.7589e+01, 3.1622e-01, 7.9502e-03, 8.3723e+00, 1.3120e+01, 4.4071e+00,
         2.8933e+00, 5.2255e-01, 2.3735e+00, 2.8524e-01, 2.1145e+01, 5.0388e+02,
         2.4996e+00, 2.1531e+00, 1.1266e+03, 3.0422e+00, 3.0353e+02, 3.7115e+00,
         5.6775e-02, 8.0765e+01, 1.4175e+01, 3.5742e+00, 4.9532e-01, 3.2937e+02,
         4.0373e+01, 1.7451e-02, 7.2797e-03, 3.8463e-01, 6.9693e+00, 2.0348e+01,
         8.0628e-02, 3.7489e+02, 1.8386e-02, 9.2736e+00, 1.3539e+00, 1.3743e+01,
         1.6528e+01, 1.9194e+01, 4.4694e+01, 5.5839e-02, 2.4930e-01, 1.7011e+00,
         3.1831e+00, 9.5335e+01, 2.5392e+01, 1.5926e+00],
        [1.0071e+00, 6.4106e+00, 1.1949e+01, 6.9145e+00, 1.0157e-02, 6.8699e+00,
         1.8632e-02, 2.6856e+00, 1.4102e-01, 1.3465e+01, 2.5987e+01, 7.0367e-02,
         3.2143e-01, 2.0543e-01, 9.2001e-02, 2.1076e+01, 1.2469e-01, 1.2100e+01,
         1.5275e+02, 9.8277e-01, 8.2537e-01, 1.9182e+02, 5.0983e+01, 8.1128e-01,
         4.6308e-03, 5.1990e+00, 2.7044e-02, 5.4374e+00, 8.2770e-01, 3.4950e+00,
         7.0835e+00, 4.1649e+00, 5.9372e-01, 4.9878e-01, 3.9828e+00, 1.0907e-01,
         9.8004e-01, 4.1711e+00, 8.9136e+00, 1.4332e-01, 8.1388e+00, 1.1327e-01,
         6.2809e-01, 1.6238e-03, 1.4598e+01, 3.0934e+00, 2.6093e+02, 2.4538e-01,
         2.1697e-02, 1.2581e+01, 3.0875e+00, 6.9878e-01, 5.1600e-01, 6.6583e+02,
         9.3472e-02, 1.1614e-02, 1.0979e+00, 2.8399e-01, 4.3804e+00, 8.3447e-02,
         3.4457e-01, 5.3456e+00, 2.7169e-01, 6.7716e-01, 1.2577e-01, 7.8467e-01,
         2.5706e+01, 2.7243e+01, 4.4101e-01, 7.7563e+00, 8.4314e-01, 2.9221e-01,
         7.3537e-01, 2.3003e-01, 1.5536e+01, 7.2255e+00, 1.2258e-02, 1.2928e+01,
         3.3754e+02, 2.0515e+00, 8.6548e+00, 2.2317e+00, 5.0902e+01, 6.9018e+01,
         8.7515e+01, 1.3046e+02, 2.1414e+00, 1.3141e-01, 1.6282e-01, 6.1135e+02,
         3.1927e+00, 7.5284e-01, 1.2965e+01, 1.1930e-01, 5.9097e+00, 2.7553e+00,
         4.5543e-02, 7.4919e-01, 1.8712e+01, 1.4562e+00]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,298 - forward - INFO - Original Rs:
 tensor([[ 81.,   5.,  47.],
        [ 42.,   2.,   2.],
        [  0., 395.,  46.]], device='cuda:6')
2025-08-01 11:39:56,300 - forward - INFO - Generative mu:
 tensor([[56.7508, 60.1260, 53.7265],
        [56.7508, 60.1260, 53.7265],
        [10.3388, 43.6731, 38.2124]], device='cuda:6', dtype=torch.float64)
2025-08-01 11:39:56,301 - forward - INFO - X_hat theta:
 tensor([[2.0739e+07, 1.8836e-01, 1.1577e+06],
        [2.0739e+07, 1.8836e-01, 1.1577e+06],
        [1.7790e+07, 5.2288e+00, 5.2928e+00]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,302 - forward - INFO - JS-divergence omega orig 0.14330881665825462 learn 0.49308511033397734 flat8.195639361607795e-08
2025-08-01 11:39:56,304 - forward - INFO - average loss when compared to Xs: tensor([ 69342.8203,  16045.3428,   6245.2744,  22144.7578,  29312.2031,
          1498.9580,   9784.0840,  67089.1250,  25705.3477,   5815.2061,
          8944.5723,   8553.0859,  24740.8613,  15733.8252,  34673.5156,
        119672.6875,  14944.9609,   1266.5835,  61676.3906,   5961.7705,
          2423.4817,  36857.0938,  11690.0010,  52002.5078,  30426.9414,
          8528.1084,  13337.1777,  14030.1035,  24633.2461,  92788.0469,
         60576.4766,  69127.6562,   8887.9746,   4840.7891,  19228.2070,
         84186.0000,   3753.8291,  70718.5156,  20995.0410,  68608.9844,
         39825.4375,  61329.2461,  21467.2344,  28335.8008,   6898.8379,
          2487.2344,  17798.3984,  19113.2383,   8418.6182,  75897.0547,
         24374.3203,  40421.4922,   5800.7358,   9087.1611, 108105.8281,
         14597.3906,   1841.1959,  14283.5176, 107763.9219,  81497.5859,
         71484.6562,  25138.8379,  16427.6484, 104873.2422,  17010.7539,
         50661.6875,  18340.5977,   3826.5984,   6126.6411,  17998.2734,
         36672.2031,  15418.0469,  51281.0312,  89892.7188,  92879.8672,
         12066.3711,   3749.5681,   3521.7703,   2437.6526,  76721.5859,
         20186.2012,  37718.2734,  80396.8125,  32740.7383,  46259.6914,
         56473.7969,  84394.9062,   2514.7483,  69485.6719,  26030.2148,
         27039.0273,  22819.5098,   1768.4998,  53968.5859,  66228.1719,
         46737.9062,   2647.4136,  21062.1094, 107433.7969,  57563.7070],
       device='cuda:6', grad_fn=<MulBackward0>)
2025-08-01 11:39:56,304 - forward - INFO - -------------------------------- END --------------------------------
2025-08-01 11:39:56,305 - forward - INFO - 
2025-08-01 11:39:56,305 - forward - INFO - Loss Function         | Loss Value(s)        
2025-08-01 11:39:56,306 - forward - INFO - ---------------------------------------------
2025-08-01 11:39:56,306 - forward - INFO - Adj KL Lib:           |                   0.0
2025-08-01 11:39:56,306 - forward - INFO - Adj KL Gene:          |     817.1464233398438
2025-08-01 11:39:56,307 - forward - INFO - Adj KL X:             |    1440.6556396484375
2025-08-01 11:39:56,307 - forward - INFO - Adj Recon Lib:        | 0.00e+00, 0.00e+00
2025-08-01 11:39:56,307 - forward - INFO - Adj Recon DA:         | 1.65e+06, 3.21e+06
2025-08-01 11:39:56,307 - forward - INFO - Adj Batch Correction: | 0.00e+00
2025-08-01 11:39:56,308 - forward - INFO - Adj Recon Express:    | 3.516e+06            
2025-08-01 11:39:56,331 - forward - INFO - 
Training Epoch: 006/1000 | Batch 0001/0001
2025-08-01 11:39:56,331 - forward - INFO - 
Average training loss per epoch 16720.75
2025-08-01 11:39:56,362 - forward - INFO - --------------++------ LOSS INFORMATION ------------------++--------
2025-08-01 11:39:56,363 - forward - INFO - X_hat mu:
 tensor([[1.2828e-11, 8.9608e-04, 4.5585e+02],
        [9.9209e-02, 3.3545e+00, 8.0197e+00],
        [1.0529e+00, 3.3819e+01, 2.6309e-01]], device='cuda:6')
2025-08-01 11:39:56,364 - forward - INFO - lib size factors:
 tensor([[5781.],
        [2099.],
        [4750.]], device='cuda:6')
2025-08-01 11:39:56,365 - forward - INFO - omega:
 tensor([[2.2189e-15, 1.5500e-07, 7.8853e-02],
        [4.7265e-05, 1.5981e-03, 3.8207e-03],
        [2.2166e-04, 7.1197e-03, 5.5388e-05]], device='cuda:6')
2025-08-01 11:39:56,371 - forward - INFO - omega * lib_size:
 tensor([[1.2828e-11, 8.9608e-04, 4.5585e+02, 3.0867e-07, 3.7983e-03, 1.3360e-06,
         1.7618e-10, 1.0625e-05, 1.3155e-05, 9.7903e-02, 1.2828e-05, 1.5402e-07,
         1.2519e-04, 3.3174e-05, 1.9281e-11, 2.3975e-05, 6.3349e-03, 7.0747e-07,
         1.2422e+01, 3.9926e-05, 6.2585e-08, 9.3475e-09, 6.8356e-05, 3.3188e-01,
         3.8763e-04, 1.5338e-04, 1.1118e-03, 6.8852e-08, 1.4622e-02, 4.6247e-06,
         1.4488e-08, 1.1802e-09, 4.5488e-06, 5.0013e-03, 9.2890e+00, 9.1042e-05,
         2.9908e-03, 8.4330e-04, 8.2739e-06, 4.8761e+03, 7.3459e-06, 1.5163e-05,
         4.5785e+01, 5.4707e-07, 1.6384e-04, 2.2802e-05, 7.2813e-08, 9.6932e-06,
         5.5950e+01, 7.8690e-06, 1.7959e-06, 1.4546e-03, 6.4754e-02, 1.3794e-12,
         6.0300e-03, 1.1599e-06, 5.3499e-06, 1.0377e-04, 4.2298e-06, 1.5312e+01,
         1.2530e-07, 1.0375e+00, 2.2680e+00, 9.9924e-10, 1.6538e+00, 2.8107e+02,
         3.6233e-03, 7.2139e-05, 8.8506e-02, 1.2128e-05, 7.3345e+00, 8.4117e-04,
         4.0774e-04, 2.2309e+00, 8.0918e-07, 4.2499e-02, 2.5200e-05, 8.1750e-01,
         1.7215e-05, 1.1698e-03, 8.0900e-11, 8.8818e-10, 2.4050e-06, 2.0513e-06,
         6.5532e-12, 9.6253e-04, 2.3005e-06, 1.3913e-07, 5.8176e-02, 9.5978e-02,
         5.5647e-04, 1.8408e-07, 3.3500e-07, 1.7298e-08, 2.2770e-08, 9.8196e+00,
         8.7525e-05, 8.6990e-06, 3.2313e+00, 1.9704e-04],
        [9.9209e-02, 3.3545e+00, 8.0197e+00, 1.3098e-01, 1.5899e+01, 3.7847e+00,
         9.6613e-03, 6.4813e+01, 9.6381e+00, 4.3001e-01, 2.1331e+01, 3.8218e-01,
         1.5841e+00, 1.3904e+00, 4.7579e-01, 2.4958e+01, 6.4818e-02, 3.5628e-02,
         1.3295e+02, 1.8407e+00, 1.0731e-01, 3.3054e-01, 8.8844e+00, 2.7836e-01,
         1.5852e-01, 1.3160e-01, 1.9173e-01, 1.0154e+02, 3.4226e-01, 1.2398e+00,
         3.4024e-02, 1.3632e-01, 1.7269e-02, 2.0685e-02, 5.3098e-02, 2.9355e-01,
         4.0112e-01, 4.4433e+01, 7.6853e+01, 2.2991e+01, 5.1174e+00, 1.7541e-01,
         8.6733e+00, 7.6520e-02, 3.4419e+01, 3.1787e+01, 7.7000e+02, 1.1754e+00,
         3.6081e-02, 5.6876e-01, 3.5700e-01, 1.1668e-01, 2.7065e+00, 3.7219e+00,
         1.4598e-02, 8.5747e-01, 5.9061e+01, 2.1917e+00, 6.5539e-01, 6.3070e-01,
         5.4985e+00, 1.0942e+01, 8.4714e-01, 7.9872e-03, 9.6167e-03, 7.1943e-01,
         1.6399e+01, 1.6325e+01, 8.8461e+00, 4.0291e+02, 5.7915e-01, 3.3297e+01,
         6.4133e+00, 1.4520e+00, 2.6069e+00, 2.8791e+00, 7.7842e-01, 1.4723e+00,
         7.5842e+00, 6.3889e-01, 8.4098e-02, 2.2493e+00, 7.5276e+00, 2.1305e+00,
         8.2425e-02, 2.2992e+00, 7.1986e-01, 4.3601e-02, 1.9544e-01, 6.1035e+01,
         4.7655e-02, 2.1610e+00, 1.9370e+00, 1.3225e-01, 1.5372e+01, 3.0846e-01,
         1.1881e-02, 3.4506e+00, 6.2951e+00, 7.3265e-01],
        [1.0529e+00, 3.3819e+01, 2.6309e-01, 2.1123e+00, 1.8626e-02, 4.9065e-01,
         2.1500e-01, 6.6110e-03, 8.5782e-03, 6.2541e+01, 2.7793e-02, 2.8387e-03,
         3.0378e+00, 7.1658e-02, 2.3334e-05, 1.1206e-03, 5.2312e-01, 4.3770e+00,
         7.2142e-01, 3.0616e+00, 4.2548e-01, 2.4424e-03, 2.1784e-03, 7.8671e+00,
         2.1996e+00, 6.9305e-01, 1.3284e-04, 2.5899e-02, 1.4343e+03, 3.2435e+00,
         1.2286e+00, 3.0024e+02, 2.0549e-02, 1.3586e+02, 2.3691e-02, 9.7812e-01,
         3.0728e+00, 3.5272e-01, 7.7908e-03, 5.2096e+00, 5.9764e-03, 1.2069e-01,
         2.6198e-01, 2.4124e-06, 1.7322e-01, 2.5603e+01, 3.2748e-03, 1.5186e-03,
         1.7820e+00, 7.7394e-01, 6.7813e+00, 8.1418e-01, 5.4104e-02, 2.6274e-02,
         2.2448e+02, 1.1751e-02, 2.2162e-02, 6.3416e-01, 6.7190e+00, 5.0039e+00,
         3.3824e+00, 7.3477e-01, 8.0052e-02, 1.7671e-01, 2.0821e+02, 1.0977e+00,
         2.9195e-02, 1.0449e+01, 3.9872e-03, 1.0456e-02, 9.9648e+00, 2.3665e-03,
         2.6830e-03, 9.4421e-01, 5.1812e+00, 1.4585e+00, 1.1756e-01, 1.0135e-01,
         2.1387e-01, 1.2432e+01, 1.5704e-04, 7.5007e-03, 2.0683e-01, 1.8532e-01,
         4.4497e-03, 4.4945e+02, 1.8422e-01, 3.0922e-01, 5.4595e-02, 1.3860e+03,
         1.5628e+02, 9.4741e+01, 6.8507e-02, 8.3475e-04, 5.0015e-04, 6.5338e+00,
         1.6661e+00, 5.4093e-02, 1.1826e+02, 6.6927e-02]], device='cuda:6')
2025-08-01 11:39:56,372 - forward - INFO - Original Rs:
 tensor([[451.,   2.,  25.],
        [  0.,  27.,   4.],
        [155.,   3.,  18.]], device='cuda:6')
2025-08-01 11:39:56,374 - forward - INFO - Generative mu:
 tensor([[56.7508, 60.1260, 53.7265],
        [10.3388, 43.6731, 38.2124],
        [56.7508, 60.1260, 53.7265]], device='cuda:6', dtype=torch.float64)
2025-08-01 11:39:56,375 - forward - INFO - X_hat theta:
 tensor([[1.5592e+07, 5.0105e-01, 1.3937e+06],
        [9.7046e+06, 8.4369e-01, 7.5981e-01],
        [1.5592e+07, 5.0105e-01, 1.3937e+06]], device='cuda:6')
2025-08-01 11:39:56,376 - forward - INFO - JS-divergence omega orig 0.14689707773992297 learn 0.35900689276724684 flat8.195639361607795e-08
2025-08-01 11:39:56,378 - forward - INFO - average loss when compared to Xs: tensor([75571.9141,  4565.0996, 20597.5234, 63402.8438, 14009.5723, 17507.5625,
         3252.8118, 26443.1211,  1974.5168,   894.7557, 25942.3672,  2587.1030,
         1213.2452,  7963.5869,  1624.1436, 43864.0898,  1019.8254, 45268.5742,
          980.4190,  3360.1914, 59331.2578,  6910.3154, 77376.0859, 46376.6875,
         1482.4198, 50533.5430,  4567.5015,  2122.7844,  1986.3123,  1200.0632,
         1198.0264,  1687.4830,  9679.1582,  4260.1802, 38114.6641, 18133.2539,
         2820.3105, 13174.2617,  2212.1719, 59355.1055, 42449.7344,  8921.0586,
         1633.1921,  1417.7639, 47328.8320, 15436.4453, 42598.0352,  1246.0427,
        31976.4844, 95176.0469, 14505.7754,  1207.8411, 28011.4766, 89234.0312,
        55935.3125, 21169.3633,  7340.8027, 18619.5820,  8407.4688,  1816.6650,
         1995.7795,  1058.6670,  1278.9965,  4710.0229, 32157.0234, 15008.0098,
        10015.5557,  1599.8135,  2670.0417, 45304.1719,   827.2672,  1409.2332,
         7916.5078, 70988.7266,  1627.7284, 36077.9844, 17921.4922, 28281.1914,
        39402.9727,  3086.6172,   926.3102, 34934.3516, 29023.9863,   963.6266,
        16072.7676,  4457.6392, 15101.4414,  1616.8246, 47494.1133, 16839.7852,
         1102.5452,  1458.7216, 18708.6484, 53269.5352,  3003.8999, 77332.0000,
        15367.2881, 29563.0938,  9365.2080,  4932.0542], device='cuda:6')
2025-08-01 11:39:56,378 - forward - INFO - -------------------------------- END --------------------------------
2025-08-01 11:39:56,379 - forward - INFO - 
2025-08-01 11:39:56,379 - forward - INFO - Loss Function         | Loss Value(s)        
2025-08-01 11:39:56,379 - forward - INFO - ---------------------------------------------
2025-08-01 11:39:56,379 - forward - INFO - Adj KL Lib:           |                   0.0
2025-08-01 11:39:56,379 - forward - INFO - Adj KL Gene:          |    127.95863342285156
2025-08-01 11:39:56,381 - forward - INFO - Adj KL X:             |     794.3954467773438
2025-08-01 11:39:56,381 - forward - INFO - Adj Recon Lib:        | 0.00e+00, 0.00e+00
2025-08-01 11:39:56,381 - forward - INFO - Adj Recon DA:         | 7.51e+05, 9.14e+05
2025-08-01 11:39:56,381 - forward - INFO - Adj Batch Correction: | 0.00e+00
2025-08-01 11:39:56,382 - forward - INFO - Adj Recon Express:    | 2.004e+06            
2025-08-01 11:39:56,383 - forward - INFO - Validation Epoch: 006/1000 | Batch 0001/0002
2025-08-01 11:39:56,384 - forward - INFO - 
Average validation loss per epoch 3658.60595703125
2025-08-01 11:39:56,386 - forward - INFO - 
Time elapsed: 0.02 min
2025-08-01 11:39:56,388 - forward - INFO - 
batch X for epoch: 6 batch: 0
2025-08-01 11:39:56,389 - forward - INFO - Size: torch.Size([100, 100]) dtype: torch.float32 Min: tensor(0., device='cuda:6')Max: tensor(565., device='cuda:6')
2025-08-01 11:39:56,390 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,393 - forward - INFO - tensor([[ 81.,   5.,  47.,  ...,  71.,  41.,  40.],
        [ 42.,   2.,   2.,  ..., 101.,  62., 170.],
        [  0., 395.,  46.,  ...,   0.,  28.,   0.],
        ...,
        [ 21.,  36.,  39.,  ...,  50.,  10.,   0.],
        [ 29.,   5., 104.,  ...,  93.,   7.,  13.],
        [ 57.,  17., 103.,  ...,  61.,  74.,  19.]], device='cuda:6')
2025-08-01 11:39:56,394 - forward - INFO - Size: torch.Size([100, 100]) dtype: torch.float32 Min: tensor(0., device='cuda:6')Max: tensor(602., device='cuda:6')
2025-08-01 11:39:56,395 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,397 - forward - INFO - tensor([[ 14.,   0.,  25.,  ...,  45.,  63.,  33.],
        [110., 127.,  21.,  ..., 113.,   4.,  11.],
        [  3.,  48.,   4.,  ...,   4.,  15.,   9.],
        ...,
        [  1.,  44.,  25.,  ...,  14.,  42.,  33.],
        [  2.,   9.,   7.,  ...,  12.,   5.,  50.],
        [131.,  70.,  32.,  ..., 112.,   4.,   0.]], device='cuda:6')
2025-08-01 11:39:56,397 - forward - INFO - 
batch indices for epoch: 6 batch: 0
2025-08-01 11:39:56,397 - forward - INFO - Size: torch.Size([100]) dtype: torch.int64 Min: tensor(0, device='cuda:6')Max: tensor(99, device='cuda:6')
2025-08-01 11:39:56,399 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,402 - forward - INFO - tensor([10, 71,  5, 83, 73, 33, 25, 37, 81, 51, 11, 15, 82, 48,  2, 14,  7, 27,
        87,  3, 75, 22, 46, 97, 80, 86, 55, 18, 42, 45, 84, 41,  0, 63, 90, 24,
        89, 66, 13, 54, 40, 70, 95, 69, 60,  8, 79, 61, 96, 67, 23, 50, 32, 57,
        77, 88, 56, 36, 85,  1, 21, 17, 94, 62, 19, 74, 99, 65, 38, 98, 31, 52,
        68, 34, 16, 59, 12, 39, 26, 64, 91, 43, 30, 53, 93, 35,  9, 28, 78,  4,
        92, 58, 76, 49,  6, 72, 20, 44, 29, 47], device='cuda:6')
2025-08-01 11:39:56,402 - forward - INFO - Size: torch.Size([100]) dtype: torch.int64 Min: tensor(0, device='cuda:6')Max: tensor(99, device='cuda:6')
2025-08-01 11:39:56,403 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,406 - forward - INFO - tensor([10, 71,  5, 83, 73, 33, 25, 37, 81, 51, 11, 15, 82, 48,  2, 14,  7, 27,
        87,  3, 75, 22, 46, 97, 80, 86, 55, 18, 42, 45, 84, 41,  0, 63, 90, 24,
        89, 66, 13, 54, 40, 70, 95, 69, 60,  8, 79, 61, 96, 67, 23, 50, 32, 57,
        77, 88, 56, 36, 85,  1, 21, 17, 94, 62, 19, 74, 99, 65, 38, 98, 31, 52,
        68, 34, 16, 59, 12, 39, 26, 64, 91, 43, 30, 53, 93, 35,  9, 28, 78,  4,
        92, 58, 76, 49,  6, 72, 20, 44, 29, 47], device='cuda:6')
2025-08-01 11:39:56,438 - forward - INFO - --------------++------ LOSS INFORMATION ------------------++--------
2025-08-01 11:39:56,440 - forward - INFO - X_hat mu:
 tensor([[6.7889e-03, 1.1817e+01, 1.7148e+01],
        [1.2513e+00, 2.9437e+01, 3.1470e+01],
        [3.4623e-06, 2.2620e-02, 1.3637e-01]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,441 - forward - INFO - lib size factors:
 tensor([[5287.],
        [5314.],
        [2985.]], device='cuda:6')
2025-08-01 11:39:56,443 - forward - INFO - omega:
 tensor([[1.2841e-06, 2.2352e-03, 3.2435e-03],
        [2.3547e-04, 5.5394e-03, 5.9221e-03],
        [1.1599e-09, 7.5779e-06, 4.5686e-05]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,449 - forward - INFO - omega * lib_size:
 tensor([[6.7889e-03, 1.1817e+01, 1.7148e+01, 7.5548e-03, 5.8321e+00, 2.0427e+00,
         3.1868e-04, 5.1662e-02, 2.9743e+01, 9.0358e-01, 8.1727e-02, 6.9411e-03,
         1.5023e-01, 1.2441e-02, 8.1913e-03, 6.2158e-02, 1.8663e-01, 1.2682e-02,
         1.1264e+01, 2.0504e+00, 1.0128e-03, 8.8501e-02, 8.2407e+00, 2.1634e+00,
         4.1018e+01, 1.6506e-01, 4.8672e+02, 1.5496e-01, 5.8509e+00, 4.4496e-01,
         3.8819e-01, 9.0633e-02, 3.0790e-01, 5.5955e-01, 3.7216e+00, 1.3737e+00,
         1.6435e+01, 1.1706e+01, 8.3439e-01, 2.0459e+01, 6.0428e+00, 5.7115e-01,
         3.0194e+03, 8.7745e-04, 8.1611e-02, 9.0495e-01, 9.6198e-01, 7.2588e-02,
         2.3735e+00, 3.2503e-02, 2.6163e+00, 6.8362e+00, 3.0507e+00, 2.5283e-03,
         3.1581e-01, 5.6405e-01, 5.6408e+00, 3.0840e-01, 1.4667e+01, 3.2898e+00,
         3.5453e+01, 9.8518e-01, 9.7157e-01, 9.3809e-04, 5.4296e+00, 1.2537e+00,
         1.5806e-01, 7.0291e+00, 9.7660e-02, 1.8251e+00, 3.7410e+00, 1.7022e+00,
         1.5311e-01, 5.7471e+01, 2.4370e-04, 2.6246e+00, 6.5770e-01, 5.7339e-02,
         1.8850e-01, 5.0114e+01, 3.3710e-03, 4.5870e-03, 3.0169e-01, 7.4072e-02,
         5.8535e-05, 2.9962e-01, 6.6927e-03, 1.8160e-03, 8.7481e+00, 1.1605e+00,
         2.6091e-01, 2.7910e+00, 9.8800e-02, 1.6910e+00, 6.0431e-02, 5.4638e+00,
         5.9282e+01, 7.8042e-03, 1.2618e+03, 2.5131e+01],
        [1.2513e+00, 2.9437e+01, 3.1470e+01, 1.8812e+01, 1.5510e+00, 2.1395e+00,
         1.3770e-02, 8.1902e+00, 1.5581e+00, 2.0046e+01, 1.6091e+01, 1.0333e+00,
         1.7867e+00, 2.2362e+00, 1.1869e+00, 5.4237e+00, 8.2978e-02, 2.4880e+00,
         2.3954e+00, 4.9359e-01, 1.0846e+00, 1.1605e+01, 1.4664e+01, 5.6367e+00,
         2.7692e-02, 2.1112e+01, 4.0806e-01, 1.7288e+01, 5.9009e+00, 5.7531e+00,
         2.3603e+00, 6.1442e+00, 5.6230e+00, 1.4894e+00, 6.5968e+00, 1.6412e+00,
         3.7553e+00, 2.0657e+01, 2.2279e+02, 7.7010e-01, 1.5350e+00, 3.7412e+00,
         1.4904e-01, 1.5011e-02, 2.8544e+01, 8.4589e+00, 1.5404e+02, 5.0189e-02,
         3.1117e-01, 1.9466e+01, 3.8212e-01, 1.1986e+00, 1.5437e+01, 5.9487e+02,
         7.6566e+00, 4.9872e-02, 1.6048e-01, 4.8554e+00, 4.5774e+01, 3.9794e-02,
         4.7393e+00, 2.5522e+00, 4.9756e-01, 8.5069e+01, 5.6532e-01, 7.0748e-01,
         2.1412e+02, 2.4233e+00, 2.0088e+00, 6.5294e+01, 1.0735e+00, 6.6269e+00,
         2.8887e-01, 2.0158e+00, 1.0315e+02, 8.2809e+01, 1.0919e-01, 7.8002e+01,
         9.6457e+02, 8.6791e+00, 8.0535e+01, 8.1573e+00, 3.1471e+02, 1.4456e+03,
         2.1950e+01, 2.3463e+01, 3.3915e-01, 5.6968e-01, 2.7923e-01, 1.1895e+02,
         1.3434e+00, 2.7755e+01, 1.8735e+02, 1.0109e-01, 1.0144e+01, 5.3718e-01,
         1.7726e-01, 1.2575e+01, 1.4923e+01, 2.9543e+01],
        [3.4623e-06, 2.2620e-02, 1.3637e-01, 1.4174e-05, 4.5011e-03, 1.5309e-03,
         2.5551e-08, 9.0129e+02, 5.7046e-03, 4.7349e-02, 3.6753e-02, 1.6310e-05,
         3.4317e-01, 1.2139e-03, 1.3093e-06, 1.5746e+03, 3.3768e-01, 9.7333e-09,
         6.0818e+01, 1.2773e-03, 9.6110e-06, 1.1434e-01, 2.0743e-05, 7.9547e-03,
         2.9953e-04, 3.8709e-05, 4.4625e-05, 2.5920e-02, 1.9537e-06, 9.5916e-04,
         4.2016e-09, 1.6717e-06, 3.3402e-04, 8.2969e-06, 1.0096e-01, 1.3857e-04,
         2.0733e-02, 4.5623e-03, 4.3543e+00, 2.5842e-03, 1.2521e-04, 8.6016e-01,
         8.5463e-02, 4.0379e-09, 7.5330e-02, 1.6988e+01, 2.2748e+01, 1.0382e-03,
         3.3874e-04, 9.3903e-03, 1.1353e-02, 5.1519e-02, 7.6290e-02, 1.6297e+00,
         1.6951e-05, 9.5432e-04, 2.9777e-01, 6.4265e-01, 2.1587e-03, 4.0461e-03,
         5.4794e-05, 7.7646e-04, 3.3722e-04, 2.2813e-08, 1.2230e-05, 1.2585e-06,
         4.1813e-01, 1.1414e+01, 1.0847e+00, 2.3906e+00, 7.2552e-05, 2.3748e-01,
         9.2747e-01, 5.3136e-01, 3.5123e+02, 1.2975e+01, 2.2022e-04, 5.4333e-05,
         4.7601e-03, 3.9313e-02, 1.7512e-05, 4.4326e-04, 1.5837e-01, 8.9659e-01,
         7.8770e-07, 2.8340e-05, 5.3404e-04, 2.0307e-06, 8.8976e-04, 1.5563e+01,
         1.8254e-06, 7.7543e-07, 6.4334e-04, 5.5852e-03, 1.2836e-01, 5.8830e-02,
         1.6900e-05, 9.1307e-01, 2.8834e-01, 2.0137e-03]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,450 - forward - INFO - Original Rs:
 tensor([[ 81.,   5.,  47.],
        [ 42.,   2.,   2.],
        [  0., 395.,  46.]], device='cuda:6')
2025-08-01 11:39:56,451 - forward - INFO - Generative mu:
 tensor([[56.7508, 60.1260, 53.7265],
        [56.7508, 60.1260, 53.7265],
        [10.3388, 43.6731, 38.2124]], device='cuda:6', dtype=torch.float64)
2025-08-01 11:39:56,453 - forward - INFO - X_hat theta:
 tensor([[9.7040e+06, 9.3743e-01, 1.9244e+04],
        [9.7040e+06, 9.3743e-01, 1.9244e+04],
        [9.8043e+06, 1.3378e+00, 1.0287e+00]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,454 - forward - INFO - JS-divergence omega orig 0.14330881665825462 learn 0.4151426802652934 flat8.195639361607795e-08
2025-08-01 11:39:56,456 - forward - INFO - average loss when compared to Xs: tensor([23199.7109, 13179.6484, 23032.8281, 15274.1602, 20339.7852,  1219.1094,
        24001.8555, 76140.2500,  2533.5039, 11613.1387,  2558.7134,  6434.6782,
        26013.4043,  7068.4243, 77474.2656, 94731.6484,  2830.4124,  4955.3481,
        96057.4688, 18839.1973, 13060.5977, 27169.3750, 10771.9102, 59735.9922,
        18605.2500, 11491.9434, 13940.2891,  1935.9764,  7527.7378, 64971.1328,
        99662.9141, 52803.3984, 14087.0820, 28917.3438,  4542.7598, 69402.5859,
         1396.7610, 13912.3076, 16472.9668, 86211.7969,  4323.1313, 30384.6875,
         2547.3804, 14491.2812,  5869.4785, 25882.7891,  5746.9268,  3992.7568,
         8147.8457, 24872.3984,  5819.6099, 72400.6406,  8095.3350,  2660.9424,
        16986.9004, 13955.1406,  1510.2419, 12704.7480, 20905.2500, 15789.0928,
        14844.2168, 20557.3125,  5631.6699, 16691.7656, 10973.8750, 10372.9619,
        40639.8242,  2130.1267,  3934.3865,  4073.4963,  4771.4609, 42146.1367,
        21647.9141, 74146.1328, 32173.5840, 10630.3594, 12582.7471,  5361.8037,
         2257.5791, 32252.2930,  4298.1968,  3211.5825, 37345.0000, 11458.6582,
        88216.5078, 42007.6133, 16255.8301,  1714.8296, 10394.7285,  9165.0566,
        38050.8828, 21061.0488,  1212.3010, 24194.5254, 27411.0273, 23214.2578,
         2866.5083, 13614.2139, 75673.9453,  4413.5713], device='cuda:6',
       grad_fn=<MulBackward0>)
2025-08-01 11:39:56,456 - forward - INFO - -------------------------------- END --------------------------------
2025-08-01 11:39:56,457 - forward - INFO - 
2025-08-01 11:39:56,458 - forward - INFO - Loss Function         | Loss Value(s)        
2025-08-01 11:39:56,458 - forward - INFO - ---------------------------------------------
2025-08-01 11:39:56,459 - forward - INFO - Adj KL Lib:           |                   0.0
2025-08-01 11:39:56,459 - forward - INFO - Adj KL Gene:          |    458.53656005859375
2025-08-01 11:39:56,459 - forward - INFO - Adj KL X:             |     1096.225830078125
2025-08-01 11:39:56,459 - forward - INFO - Adj Recon Lib:        | 0.00e+00, 0.00e+00
2025-08-01 11:39:56,461 - forward - INFO - Adj Recon DA:         | 1.37e+06, 1.75e+06
2025-08-01 11:39:56,461 - forward - INFO - Adj Batch Correction: | 0.00e+00
2025-08-01 11:39:56,461 - forward - INFO - Adj Recon Express:    | 2.285e+06            
2025-08-01 11:39:56,485 - forward - INFO - 
Training Epoch: 007/1000 | Batch 0001/0001
2025-08-01 11:39:56,485 - forward - INFO - 
Average training loss per epoch 10786.08203125
2025-08-01 11:39:56,516 - forward - INFO - --------------++------ LOSS INFORMATION ------------------++--------
2025-08-01 11:39:56,517 - forward - INFO - X_hat mu:
 tensor([[3.7605e-04, 7.7795e-02, 1.8491e+01],
        [6.1054e+00, 1.9749e+01, 3.4222e+01],
        [1.2235e-15, 1.8024e-10, 4.5504e-01]], device='cuda:6')
2025-08-01 11:39:56,518 - forward - INFO - lib size factors:
 tensor([[5781.],
        [2099.],
        [4750.]], device='cuda:6')
2025-08-01 11:39:56,520 - forward - INFO - omega:
 tensor([[6.5050e-08, 1.3457e-05, 3.1986e-03],
        [2.9087e-03, 9.4087e-03, 1.6304e-02],
        [2.5759e-19, 3.7944e-14, 9.5798e-05]], device='cuda:6')
2025-08-01 11:39:56,526 - forward - INFO - omega * lib_size:
 tensor([[3.7605e-04, 7.7795e-02, 1.8491e+01, 5.3209e+00, 1.2034e+01, 1.5183e-01,
         1.1024e-01, 3.4154e+01, 7.7333e-03, 6.4751e+01, 2.0281e-02, 1.0835e-01,
         6.8602e+01, 2.2079e+00, 2.5743e-02, 2.1146e-02, 3.4336e+00, 6.8624e-03,
         8.7134e+00, 7.1502e-02, 1.3064e+01, 2.7881e-04, 7.8836e-04, 4.5546e+00,
         2.0662e+01, 6.7726e+00, 6.1006e-02, 1.6688e-02, 3.9688e+01, 4.0019e+01,
         2.4494e+00, 6.4646e-01, 5.1588e+00, 3.8044e+00, 2.2784e+01, 9.1548e+01,
         1.1087e+02, 6.5402e+01, 4.5778e-02, 1.0063e+03, 1.5171e-03, 6.8344e+00,
         7.4364e+02, 3.7778e-05, 9.8678e-01, 2.7939e+00, 2.3532e-03, 1.4011e-02,
         5.8415e+02, 1.8848e+00, 3.5583e+00, 1.9449e+01, 2.0736e+01, 9.8930e-04,
         1.9097e+02, 1.9616e+00, 2.0013e+00, 1.4471e+02, 4.7357e+01, 1.2461e+03,
         6.5427e+00, 6.3421e+00, 1.6158e+01, 4.6272e-01, 1.2061e+02, 4.3719e+02,
         2.5833e-02, 1.1361e+01, 2.5895e-02, 8.2827e-03, 7.7385e+00, 2.9708e+01,
         1.8031e+01, 5.0715e+00, 1.2352e+01, 5.0821e+00, 6.5079e-02, 1.0117e+02,
         7.3810e-01, 6.8670e+00, 2.0296e-04, 7.8778e-05, 4.1329e-01, 1.2354e+02,
         8.4292e-03, 7.8513e+00, 2.0806e-02, 2.2225e+00, 4.9401e-01, 2.0063e+00,
         1.3736e+02, 5.8517e-02, 2.9775e-01, 2.4459e-02, 1.4467e-01, 7.8894e+00,
         2.8975e-01, 2.5636e+01, 1.1538e+01, 6.3877e+00],
        [6.1054e+00, 1.9749e+01, 3.4222e+01, 4.7502e+01, 6.2447e+00, 2.0166e+01,
         4.8870e+00, 6.6104e+00, 1.8674e+00, 4.5783e+01, 5.8629e+00, 1.0862e+01,
         1.7751e+01, 1.6733e+01, 8.0495e-01, 2.2656e+00, 2.6940e+00, 7.5692e+00,
         1.6944e+01, 1.3422e+01, 1.4556e+01, 3.0048e+00, 2.2282e+00, 1.1492e+01,
         1.2930e+01, 2.0451e+01, 8.7014e-01, 9.6718e+00, 2.0052e+02, 3.7199e+01,
         4.0522e+01, 2.7508e+01, 2.0945e+01, 2.7370e+01, 5.3014e+00, 1.7293e+01,
         1.4067e+01, 1.4996e+01, 2.4125e+00, 7.7849e+00, 1.9260e+00, 8.0602e+00,
         2.4882e+01, 1.7052e-01, 6.6405e+00, 4.1027e+01, 2.1490e+00, 2.5906e+00,
         1.5420e+01, 8.0971e+00, 9.6868e+00, 2.0378e+01, 6.3641e+00, 3.7852e+01,
         2.9391e+01, 7.1503e+00, 1.7553e+01, 1.5398e+01, 4.1000e+01, 3.4573e+01,
         4.9720e+00, 7.0732e+01, 4.5435e+01, 2.5344e+01, 6.1246e+01, 3.4166e+01,
         4.3766e+00, 2.2126e+01, 1.7954e+01, 9.2345e+00, 1.1148e+01, 7.5582e+00,
         9.1891e+00, 2.9137e+01, 6.7097e+00, 6.6860e+00, 1.2873e+00, 6.0313e+01,
         1.6169e+01, 2.9586e+01, 1.6656e+00, 5.1863e+00, 4.0362e+01, 3.8773e+01,
         5.2849e+00, 5.2737e+01, 1.4964e+01, 3.1392e+01, 4.5533e+00, 8.8933e+01,
         4.3226e+01, 4.2330e+01, 1.3102e+01, 1.3316e+00, 4.4479e+00, 4.8536e+01,
         3.9415e+00, 1.7524e+01, 5.0091e+01, 7.7722e+00],
        [1.2235e-15, 1.8024e-10, 4.5504e-01, 1.0353e-08, 5.7365e-05, 1.6128e-13,
         1.2071e-13, 1.0110e-01, 2.1898e-11, 1.0412e-01, 3.4448e-07, 7.7345e-10,
         5.3242e-08, 2.6315e-08, 3.5574e-12, 4.4209e-02, 1.3039e-10, 1.2526e-15,
         2.1398e-04, 2.9323e-09, 3.6121e-06, 3.1732e-05, 8.2663e-08, 4.4821e-06,
         5.1709e-12, 3.7837e-08, 1.5503e-09, 4.1351e-08, 6.9195e-12, 2.7631e-07,
         8.5645e-11, 8.4836e-12, 3.3005e-08, 5.0657e-14, 4.7443e+03, 1.8119e-08,
         6.7876e-05, 1.4083e-05, 1.0968e-04, 7.6332e-03, 3.2807e-09, 6.4762e-03,
         1.3274e-04, 1.0858e-16, 2.5927e-09, 1.8450e-03, 3.4584e-10, 7.0393e-09,
         2.3204e-05, 3.7155e-04, 2.2242e-07, 6.2122e-08, 2.1825e+00, 2.6315e-03,
         1.3705e-07, 4.4716e-08, 1.6063e-07, 1.2175e-09, 6.7306e-08, 2.0008e-13,
         2.8844e-10, 1.6111e-06, 3.2814e-08, 4.2818e-12, 8.5157e-08, 1.8638e-08,
         1.9014e-06, 1.2628e-12, 2.3812e-09, 1.1899e-06, 2.6767e-07, 2.4568e-07,
         1.3209e-02, 1.8440e-04, 1.0231e-05, 2.3572e-03, 1.6476e-11, 1.0127e-06,
         9.9732e-06, 9.3126e-01, 5.2343e-07, 6.6423e-15, 3.4960e-05, 1.8931e+00,
         4.1220e-07, 6.0263e-10, 4.0526e-13, 5.0217e-12, 3.1796e-07, 3.4873e-05,
         2.2152e-08, 4.8451e-16, 6.9698e-04, 2.8193e-06, 3.1952e-09, 2.8751e-05,
         1.3200e-06, 4.9303e-10, 3.6712e-07, 2.3404e-05]], device='cuda:6')
2025-08-01 11:39:56,527 - forward - INFO - Original Rs:
 tensor([[451.,   2.,  25.],
        [  0.,  27.,   4.],
        [155.,   3.,  18.]], device='cuda:6')
2025-08-01 11:39:56,528 - forward - INFO - Generative mu:
 tensor([[56.7508, 60.1260, 53.7265],
        [10.3388, 43.6731, 38.2124],
        [56.7508, 60.1260, 53.7265]], device='cuda:6', dtype=torch.float64)
2025-08-01 11:39:56,529 - forward - INFO - X_hat theta:
 tensor([[4.9614e+06, 7.1365e-01, 2.1340e+02],
        [6.2821e+05, 1.0478e+00, 5.3362e-01],
        [4.9614e+06, 7.1365e-01, 2.1340e+02]], device='cuda:6')
2025-08-01 11:39:56,529 - forward - INFO - JS-divergence omega orig 0.14689707773992297 learn 0.2814279780243191 flat8.195639361607795e-08
2025-08-01 11:39:56,531 - forward - INFO - average loss when compared to Xs: tensor([28009.4258,   913.7276, 80360.1875, 12887.2773,  8205.8584,  1810.1499,
         3983.3035, 32697.8594,  1108.3113,   641.3003, 57756.0078,   991.5915,
         1021.6419,  9835.6855,   937.2021, 26133.5820,   904.7515, 12405.4111,
          921.2737,  1785.9888, 63666.8633, 19018.1719,  9609.9336, 24303.0938,
         1660.1766,  5901.4736,  1943.4648,  1638.6295,  1210.3711,   966.8468,
          973.9750,   968.3242,  9230.2656,   933.6833,  8526.0830,  1531.7048,
         1051.1865,  4322.1641,   934.0516, 44932.9375, 62756.1875,  1580.1158,
         3497.3755,  1178.8171, 19993.2578,  1171.5917, 12192.7666,   854.3290,
         6787.7822, 14568.4932,  9719.6846,  1222.5494, 20731.2422, 27374.4570,
        54419.7734,  9319.1289,  3336.5098, 28460.8477,  1632.0208,  1466.2411,
         1572.5466,  1067.2190,  1059.2365,  1545.4230, 16260.8379, 37060.3828,
        24680.2207,  1136.6245,  1363.0922,  5030.5127,   940.8050,  1149.1011,
         3994.5613, 13837.7139,  1129.9148, 40903.3906, 66551.0938,  6830.3955,
        34317.7461,  1533.5272,   768.3307,  9841.6152, 19122.0586,   775.8883,
        22409.7227, 17866.4219,  6050.9326,   962.0046, 12049.9893, 13793.3887,
          773.0868,  1092.5227, 45824.8906, 15659.2422,  6055.2207,  4602.6846,
         5897.4912,  5859.5332, 14827.4922,  5603.1416], device='cuda:6')
2025-08-01 11:39:56,531 - forward - INFO - -------------------------------- END --------------------------------
2025-08-01 11:39:56,532 - forward - INFO - 
2025-08-01 11:39:56,532 - forward - INFO - Loss Function         | Loss Value(s)        
2025-08-01 11:39:56,533 - forward - INFO - ---------------------------------------------
2025-08-01 11:39:56,533 - forward - INFO - Adj KL Lib:           |                   0.0
2025-08-01 11:39:56,533 - forward - INFO - Adj KL Gene:          |      83.4492416381836
2025-08-01 11:39:56,533 - forward - INFO - Adj KL X:             |     539.7352905273438
2025-08-01 11:39:56,533 - forward - INFO - Adj Recon Lib:        | 0.00e+00, 0.00e+00
2025-08-01 11:39:56,533 - forward - INFO - Adj Recon DA:         | 7.73e+05, 7.10e+05
2025-08-01 11:39:56,534 - forward - INFO - Adj Batch Correction: | 0.00e+00
2025-08-01 11:39:56,534 - forward - INFO - Adj Recon Express:    | 1.241e+06            
2025-08-01 11:39:56,534 - forward - INFO - Validation Epoch: 007/1000 | Batch 0001/0002
2025-08-01 11:39:56,534 - forward - INFO - 
Average validation loss per epoch 2715.516357421875
2025-08-01 11:39:56,537 - forward - INFO - 
Time elapsed: 0.02 min
2025-08-01 11:39:56,538 - forward - INFO - 
batch X for epoch: 7 batch: 0
2025-08-01 11:39:56,539 - forward - INFO - Size: torch.Size([100, 100]) dtype: torch.float32 Min: tensor(0., device='cuda:6')Max: tensor(565., device='cuda:6')
2025-08-01 11:39:56,539 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,542 - forward - INFO - tensor([[ 81.,   5.,  47.,  ...,  71.,  41.,  40.],
        [ 42.,   2.,   2.,  ..., 101.,  62., 170.],
        [  0., 395.,  46.,  ...,   0.,  28.,   0.],
        ...,
        [ 21.,  36.,  39.,  ...,  50.,  10.,   0.],
        [ 29.,   5., 104.,  ...,  93.,   7.,  13.],
        [ 57.,  17., 103.,  ...,  61.,  74.,  19.]], device='cuda:6')
2025-08-01 11:39:56,543 - forward - INFO - Size: torch.Size([100, 100]) dtype: torch.float32 Min: tensor(0., device='cuda:6')Max: tensor(602., device='cuda:6')
2025-08-01 11:39:56,543 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,545 - forward - INFO - tensor([[ 14.,   0.,  25.,  ...,  45.,  63.,  33.],
        [110., 127.,  21.,  ..., 113.,   4.,  11.],
        [  3.,  48.,   4.,  ...,   4.,  15.,   9.],
        ...,
        [  1.,  44.,  25.,  ...,  14.,  42.,  33.],
        [  2.,   9.,   7.,  ...,  12.,   5.,  50.],
        [131.,  70.,  32.,  ..., 112.,   4.,   0.]], device='cuda:6')
2025-08-01 11:39:56,546 - forward - INFO - 
batch indices for epoch: 7 batch: 0
2025-08-01 11:39:56,546 - forward - INFO - Size: torch.Size([100]) dtype: torch.int64 Min: tensor(0, device='cuda:6')Max: tensor(99, device='cuda:6')
2025-08-01 11:39:56,547 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,549 - forward - INFO - tensor([10, 71,  5, 83, 73, 33, 25, 37, 81, 51, 11, 15, 82, 48,  2, 14,  7, 27,
        87,  3, 75, 22, 46, 97, 80, 86, 55, 18, 42, 45, 84, 41,  0, 63, 90, 24,
        89, 66, 13, 54, 40, 70, 95, 69, 60,  8, 79, 61, 96, 67, 23, 50, 32, 57,
        77, 88, 56, 36, 85,  1, 21, 17, 94, 62, 19, 74, 99, 65, 38, 98, 31, 52,
        68, 34, 16, 59, 12, 39, 26, 64, 91, 43, 30, 53, 93, 35,  9, 28, 78,  4,
        92, 58, 76, 49,  6, 72, 20, 44, 29, 47], device='cuda:6')
2025-08-01 11:39:56,549 - forward - INFO - Size: torch.Size([100]) dtype: torch.int64 Min: tensor(0, device='cuda:6')Max: tensor(99, device='cuda:6')
2025-08-01 11:39:56,550 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,551 - forward - INFO - tensor([10, 71,  5, 83, 73, 33, 25, 37, 81, 51, 11, 15, 82, 48,  2, 14,  7, 27,
        87,  3, 75, 22, 46, 97, 80, 86, 55, 18, 42, 45, 84, 41,  0, 63, 90, 24,
        89, 66, 13, 54, 40, 70, 95, 69, 60,  8, 79, 61, 96, 67, 23, 50, 32, 57,
        77, 88, 56, 36, 85,  1, 21, 17, 94, 62, 19, 74, 99, 65, 38, 98, 31, 52,
        68, 34, 16, 59, 12, 39, 26, 64, 91, 43, 30, 53, 93, 35,  9, 28, 78,  4,
        92, 58, 76, 49,  6, 72, 20, 44, 29, 47], device='cuda:6')
2025-08-01 11:39:56,582 - forward - INFO - --------------++------ LOSS INFORMATION ------------------++--------
2025-08-01 11:39:56,583 - forward - INFO - X_hat mu:
 tensor([[9.2610e-19, 2.4470e-03, 5.2572e-02],
        [5.8185e-03, 1.3490e+00, 3.4596e+00],
        [1.8707e+01, 1.1740e+00, 2.7976e+01]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,584 - forward - INFO - lib size factors:
 tensor([[5287.],
        [5314.],
        [2985.]], device='cuda:6')
2025-08-01 11:39:56,585 - forward - INFO - omega:
 tensor([[1.7516e-22, 4.6284e-07, 9.9437e-06],
        [1.0949e-06, 2.5386e-04, 6.5103e-04],
        [6.2670e-03, 3.9328e-04, 9.3722e-03]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,591 - forward - INFO - omega * lib_size:
 tensor([[9.2610e-19, 2.4470e-03, 5.2572e-02, 8.1588e-01, 8.9637e-01, 1.6735e-08,
         3.4381e-01, 6.3848e-01, 3.2798e-09, 1.9788e-01, 5.3242e-08, 4.7226e-08,
         3.7937e+02, 1.8491e-05, 4.0374e-07, 1.0509e-04, 4.2818e-12, 2.6603e-10,
         2.0002e-02, 1.7396e-04, 2.0344e-06, 1.2815e-03, 3.4097e-07, 6.6014e-02,
         3.7418e-08, 1.1633e-07, 1.3288e-11, 1.8572e+00, 8.6613e-01, 1.1099e-02,
         7.9783e-07, 4.0945e-03, 1.8965e-04, 2.5559e-02, 3.3957e-12, 1.0805e-03,
         1.1643e+00, 6.2976e+02, 2.6354e-05, 2.8069e-03, 1.9577e-04, 1.8707e-04,
         9.1245e+02, 3.9711e-13, 3.7112e-05, 2.9636e+00, 7.5011e-08, 4.4104e-07,
         6.4122e-11, 2.5288e-12, 3.8367e-04, 3.4959e-01, 1.1944e-04, 1.0792e+03,
         1.0316e-04, 3.5122e+00, 9.3240e-02, 1.0549e-04, 1.3163e-03, 3.8447e+00,
         2.9269e-04, 2.8937e-03, 2.9324e-04, 2.4037e-07, 7.3161e-02, 4.9627e-02,
         2.2147e+02, 1.2120e+00, 2.4529e-03, 3.9702e-11, 2.1448e-05, 8.4352e-03,
         3.6275e-03, 2.7348e-02, 9.8097e-13, 2.0940e-08, 2.7740e-04, 7.9034e-04,
         1.5445e+01, 1.5763e-04, 1.4542e-06, 1.3291e-11, 2.0112e+03, 1.8512e+01,
         1.0045e-04, 2.4399e-04, 1.2493e-10, 3.1468e-03, 3.7530e-05, 7.9151e-04,
         1.2289e-08, 9.2996e-08, 8.9530e-06, 2.5827e-08, 3.5098e-01, 2.1190e-05,
         5.5619e-10, 2.3982e-11, 1.5449e-01, 5.9233e-04],
        [5.8185e-03, 1.3490e+00, 3.4596e+00, 1.4470e+00, 2.5532e+00, 3.3846e-01,
         1.7409e-02, 4.9299e-02, 5.2067e-01, 2.0137e+02, 1.4305e+01, 6.4055e-02,
         7.9410e+01, 3.9147e-03, 4.9332e+00, 1.4666e-02, 1.2765e-03, 1.0928e-01,
         1.2939e+01, 5.0548e-01, 8.5007e-03, 6.2566e-03, 1.5404e-02, 4.0324e-02,
         1.4631e-03, 2.9791e+00, 1.9724e-01, 8.7384e-01, 1.1223e+02, 9.2635e-03,
         1.3789e-01, 7.0186e-02, 4.4756e-02, 1.4744e-02, 6.2722e-04, 1.1868e-03,
         4.1184e-02, 6.4218e+00, 1.1967e-03, 1.6658e-04, 5.3037e-06, 2.3450e-05,
         1.0792e-01, 3.0420e-04, 4.6719e-01, 2.9536e+00, 2.4469e-01, 2.1238e-02,
         9.3534e-03, 2.6177e-03, 4.5872e-02, 1.0383e-02, 1.4782e-01, 5.7370e-01,
         1.4865e+00, 4.1026e-01, 8.2769e-01, 1.8876e-01, 1.9360e-02, 5.3981e-01,
         1.9167e-01, 1.1054e+00, 1.1778e+00, 7.7034e-07, 2.5407e-03, 5.7956e-03,
         8.1387e-02, 2.5121e-01, 4.9524e-03, 3.6934e+00, 2.7934e-02, 8.1242e-03,
         2.9444e-01, 2.8011e-04, 2.9187e-03, 7.8999e-01, 5.6443e-03, 4.1043e+02,
         9.3376e-01, 3.9970e-01, 2.1590e-04, 3.3923e-01, 1.4572e-02, 4.3614e+03,
         7.1283e-04, 2.4642e-03, 3.1060e+00, 5.5504e-03, 5.2780e-02, 1.0401e-02,
         1.7208e-04, 7.4362e-02, 8.1959e-01, 3.3661e-07, 4.5923e-03, 1.4266e-02,
         1.5538e-04, 2.6123e-03, 7.3730e+01, 4.0907e-01],
        [1.8707e+01, 1.1740e+00, 2.7976e+01, 9.8232e+01, 8.2897e+01, 6.7614e+00,
         6.7041e-01, 1.7529e+02, 1.1955e+01, 2.8743e+01, 7.5334e+00, 4.4123e+00,
         1.1851e+00, 7.8893e+00, 1.1757e+01, 6.7485e+00, 9.2607e+00, 2.5068e-01,
         2.5913e+01, 4.6103e+00, 7.5406e+00, 2.8215e+00, 4.9291e-01, 1.7366e+01,
         1.2338e+01, 1.4240e+02, 9.8049e+00, 1.0319e+00, 5.1966e+00, 2.4987e+01,
         1.5979e+01, 6.0265e+00, 6.5967e+01, 1.2172e+00, 1.4332e+00, 1.8592e+02,
         2.0727e+01, 1.0689e+01, 2.8645e+00, 6.9496e-01, 1.1846e+00, 3.6508e+01,
         1.2588e+02, 3.7015e-01, 3.1586e+01, 3.0955e+01, 2.2613e+01, 1.4338e+00,
         2.7291e+01, 2.3455e+01, 2.9051e+00, 1.5736e+00, 2.4432e+01, 1.8243e+00,
         3.0115e+01, 3.5087e+00, 1.4408e+01, 3.9243e+01, 4.2089e+01, 4.3268e+00,
         1.7538e+01, 3.2533e+01, 2.8902e+00, 2.5068e+00, 3.7014e+00, 3.6331e+01,
         5.4593e+00, 1.7700e+01, 6.3682e+01, 3.1142e+02, 6.8829e+00, 9.3512e+01,
         5.7792e+01, 9.0668e+00, 9.9621e-01, 7.3622e+01, 5.8574e-01, 1.1104e+02,
         3.1834e+01, 4.8964e+00, 2.1032e-01, 7.2335e+00, 6.2734e+00, 1.7647e+02,
         1.7349e+00, 2.6173e+01, 9.7104e+00, 3.9419e+00, 2.1824e+00, 1.3803e+01,
         5.4962e+01, 3.6697e+00, 8.1284e+01, 7.1449e-01, 2.0199e+00, 2.0278e+01,
         1.6633e+01, 3.5762e+01, 1.0063e+02, 4.1507e+00]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,592 - forward - INFO - Original Rs:
 tensor([[ 81.,   5.,  47.],
        [ 42.,   2.,   2.],
        [  0., 395.,  46.]], device='cuda:6')
2025-08-01 11:39:56,593 - forward - INFO - Generative mu:
 tensor([[56.7508, 60.1260, 53.7265],
        [56.7508, 60.1260, 53.7265],
        [10.3388, 43.6731, 38.2124]], device='cuda:6', dtype=torch.float64)
2025-08-01 11:39:56,594 - forward - INFO - X_hat theta:
 tensor([[2.5089e+07, 4.7617e-01, 2.2808e+04],
        [2.5089e+07, 4.7617e-01, 2.2808e+04],
        [2.4877e+03, 1.2330e+00, 1.6109e+00]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,594 - forward - INFO - JS-divergence omega orig 0.14330881665825462 learn 0.38532287735180776 flat8.195639361607795e-08
2025-08-01 11:39:56,596 - forward - INFO - average loss when compared to Xs: tensor([ 64732.7266,  33717.6016,   3333.7908,   4619.6885,  21709.5000,
          1065.4170,   1679.7710,   9401.0859,  11110.7441,  21772.8867,
          2139.7678,  14360.5801,  66047.1953,   1243.0261,  21061.1465,
         30338.9102,  12694.2559,  21714.5625,  16639.6797,   7382.3926,
          1182.6113,  49795.8320,  10775.7109,  41435.1641,   7487.0098,
          1604.7762,  53540.2969,   1438.1144,  10564.8242,   8769.5449,
         23856.9121,  59559.7188,   3997.1499,  10006.1875,   4039.3794,
         29129.2734,   1293.4086,  56883.0547,  18055.8418,  87212.9062,
         14904.0020,  50437.9414,   1571.1509,  12224.1152,   2498.1016,
          1602.3180,   1798.2808,   2610.0264,   1599.4368,  12070.2754,
          1985.8140,  46200.4766,  37252.9570,   2701.0693,  17351.8164,
          4502.2734,   1437.9802,  11217.8789, 102521.7266,  80028.0625,
         27485.9102,  25546.2891,   3120.4602,  72170.1719,   1118.8704,
         36922.2891,   9095.9844,   3148.8691,   2474.5872,   9863.9609,
         18453.0586,  48537.9062,  59336.2031,  28659.4941,  38997.5000,
         37996.4805,  15006.5508,   1479.7018,   1836.4980,  28600.3398,
          6221.1221,   2574.2524,  38649.5000,   7131.0420,  27855.6621,
         60019.7812,  25307.5938,    997.7335,   3246.6306,   7275.7935,
         52015.7891,   2927.8728,   1198.7491,  31695.8262,  22412.3867,
         34591.0195,   1686.1807,   5613.5635,  53270.2891,  31859.6758],
       device='cuda:6', grad_fn=<MulBackward0>)
2025-08-01 11:39:56,596 - forward - INFO - -------------------------------- END --------------------------------
2025-08-01 11:39:56,597 - forward - INFO - 
2025-08-01 11:39:56,597 - forward - INFO - Loss Function         | Loss Value(s)        
2025-08-01 11:39:56,598 - forward - INFO - ---------------------------------------------
2025-08-01 11:39:56,598 - forward - INFO - Adj KL Lib:           |                   0.0
2025-08-01 11:39:56,599 - forward - INFO - Adj KL Gene:          |     275.7802429199219
2025-08-01 11:39:56,599 - forward - INFO - Adj KL X:             |       828.44580078125
2025-08-01 11:39:56,599 - forward - INFO - Adj Recon Lib:        | 0.00e+00, 0.00e+00
2025-08-01 11:39:56,599 - forward - INFO - Adj Recon DA:         | 1.03e+06, 1.33e+06
2025-08-01 11:39:56,600 - forward - INFO - Adj Batch Correction: | 0.00e+00
2025-08-01 11:39:56,600 - forward - INFO - Adj Recon Express:    | 2.140e+06            
2025-08-01 11:39:56,628 - forward - INFO - 
Training Epoch: 008/1000 | Batch 0001/0001
2025-08-01 11:39:56,628 - forward - INFO - 
Average training loss per epoch 8986.908203125
2025-08-01 11:39:56,659 - forward - INFO - --------------++------ LOSS INFORMATION ------------------++--------
2025-08-01 11:39:56,659 - forward - INFO - X_hat mu:
 tensor([[2.7953e-01, 5.5305e+00, 5.3175e+01],
        [3.8509e+00, 2.8658e+01, 4.9046e+01],
        [3.1301e-04, 9.0408e-02, 4.3773e+02]], device='cuda:6')
2025-08-01 11:39:56,660 - forward - INFO - lib size factors:
 tensor([[5781.],
        [2099.],
        [4750.]], device='cuda:6')
2025-08-01 11:39:56,662 - forward - INFO - omega:
 tensor([[4.8353e-05, 9.5667e-04, 9.1982e-03],
        [1.8346e-03, 1.3653e-02, 2.3366e-02],
        [6.5898e-08, 1.9033e-05, 9.2154e-02]], device='cuda:6')
2025-08-01 11:39:56,667 - forward - INFO - omega * lib_size:
 tensor([[2.7953e-01, 5.5305e+00, 5.3175e+01, 1.5280e+01, 2.8437e+01, 1.1111e-01,
         3.4804e+00, 1.8812e+01, 5.1521e+00, 3.2145e+00, 3.1037e+00, 1.6580e+00,
         2.8120e+00, 2.4172e+00, 1.5003e-01, 2.4653e+00, 3.5955e-01, 5.5128e-02,
         1.6880e+01, 4.3943e+00, 5.1368e-01, 5.7744e-01, 9.8009e+00, 1.9282e+01,
         3.2216e+00, 9.3227e+00, 3.2576e-01, 1.6467e+00, 2.3245e+02, 3.6936e+01,
         1.5857e+01, 1.8571e+01, 1.5124e+02, 3.6640e+00, 1.0793e+01, 2.6358e+00,
         6.6881e+00, 3.2932e+01, 1.2134e+01, 2.0897e+00, 1.9009e+00, 2.7805e+00,
         5.7582e+00, 2.7554e-02, 2.8068e-01, 7.6280e-01, 5.2415e-01, 5.1195e-01,
         8.1898e-01, 3.2395e+00, 2.7628e+00, 5.0685e+00, 2.8354e+01, 2.5548e+03,
         6.5104e+00, 1.8648e+00, 1.3961e+01, 4.2256e+00, 1.5935e+01, 1.9438e+00,
         5.4145e+00, 1.8708e+02, 4.4795e-01, 1.5844e+02, 5.3731e+00, 1.2122e+01,
         3.8690e+01, 1.7939e+01, 1.6799e+01, 2.9707e+00, 7.4098e-01, 2.7461e+01,
         1.2485e+01, 4.4141e+02, 1.8010e-01, 4.4814e+00, 1.7716e-02, 1.1543e+02,
         6.8378e+01, 9.8341e+00, 1.2396e+00, 5.1746e+00, 6.1604e+01, 8.9663e+02,
         1.7446e+01, 2.3071e+00, 5.2418e+00, 4.1502e+00, 1.0897e+00, 1.2230e+02,
         2.8210e+00, 2.3408e-01, 4.2485e+01, 3.1432e+00, 7.5016e+00, 5.6116e+01,
         8.2342e-01, 1.4277e+00, 2.6458e+01, 6.6345e+00],
        [3.8509e+00, 2.8658e+01, 4.9046e+01, 3.1192e+01, 3.8141e+01, 1.7817e+01,
         4.1752e+00, 3.9725e+01, 3.9073e+01, 1.3318e+01, 2.3367e+01, 2.9061e+01,
         9.8934e+00, 1.9799e+01, 8.3333e+00, 1.6814e+01, 2.9740e+00, 4.2510e+00,
         3.0183e+01, 1.3382e+01, 1.6128e+01, 1.0559e+01, 1.8101e+01, 9.6760e+00,
         9.6780e+00, 1.3050e+01, 7.9798e+00, 4.7684e+01, 3.2329e+01, 3.0135e+01,
         1.0208e+01, 7.7027e+00, 5.0684e+01, 6.9362e+00, 5.7915e+00, 2.3445e+01,
         8.5475e+00, 4.7102e+01, 1.2052e+01, 1.4216e+01, 1.2796e+01, 8.2952e+00,
         3.0816e+01, 3.5308e+00, 1.9325e+01, 2.6458e+01, 1.5454e+01, 1.0663e+01,
         5.9483e+00, 1.1552e+01, 1.5480e+01, 1.0179e+01, 2.2963e+01, 5.8846e+01,
         8.2849e+00, 9.7795e+00, 4.0954e+01, 1.2710e+01, 2.4431e+01, 1.2518e+01,
         9.5578e+00, 2.7589e+01, 1.8631e+01, 1.0417e+01, 6.8971e+00, 2.4685e+01,
         3.3812e+01, 2.4989e+01, 2.6426e+01, 5.6238e+01, 1.1276e+01, 4.8746e+01,
         2.0125e+01, 2.9053e+01, 5.2282e+00, 1.9884e+01, 6.0576e+00, 4.6425e+01,
         3.5992e+01, 1.3335e+01, 6.8002e+00, 1.7377e+01, 4.7363e+01, 4.5877e+01,
         6.1971e+00, 2.7171e+01, 2.1290e+01, 1.5141e+01, 3.6867e+00, 3.1302e+01,
         8.7781e+00, 1.2014e+01, 3.4648e+01, 5.4903e+00, 4.0458e+01, 1.2784e+01,
         8.8729e+00, 2.7500e+01, 5.9146e+01, 2.5697e+01],
        [3.1301e-04, 9.0408e-02, 4.3773e+02, 3.2782e-01, 9.4085e+00, 4.7308e-03,
         1.3980e-02, 1.5623e+01, 4.1152e-02, 3.6872e+01, 1.5827e-01, 1.6725e-01,
         1.0241e+00, 3.1118e-01, 1.4260e-02, 4.6126e+00, 6.0763e-03, 3.3082e-03,
         8.2631e-01, 3.9417e-02, 8.0884e+00, 3.7320e-01, 1.1567e-01, 2.0164e+00,
         1.0536e-01, 1.3985e+00, 8.8453e-02, 5.8951e-01, 7.2386e-01, 3.1146e+00,
         4.4291e-02, 1.6810e-01, 3.2718e-01, 3.2480e-03, 3.0066e+03, 1.8436e+00,
         5.5565e+00, 8.2612e+00, 1.7625e+00, 1.9510e+02, 4.2352e-02, 2.6255e+01,
         1.1209e+01, 4.8299e-05, 1.1978e-01, 3.4071e+01, 3.4960e-03, 6.8884e-02,
         4.9334e+00, 1.0303e+01, 7.9270e-01, 3.2844e+00, 1.5696e+02, 3.5993e+01,
         5.2662e+00, 6.3517e-01, 1.6983e+00, 8.9370e-02, 5.2185e+00, 6.0296e-02,
         6.2414e-02, 4.6000e+00, 4.1275e-01, 3.1667e-01, 4.3213e+00, 4.9396e-01,
         1.0150e+00, 3.2381e-03, 5.5049e-02, 5.3200e-01, 2.7934e+00, 1.4659e-01,
         1.1730e+01, 5.4353e+00, 5.0432e-01, 1.8301e+00, 1.9294e-03, 2.1641e+01,
         5.3683e+00, 2.7722e+02, 4.6555e-01, 2.7113e-04, 3.0588e-01, 2.9310e+02,
         9.0798e-01, 1.8373e-01, 3.9878e-03, 1.1439e-01, 6.5840e-01, 1.5784e+00,
         9.5755e-01, 1.2478e-03, 5.3958e+01, 1.3307e+00, 2.7019e-01, 3.4815e+00,
         1.9710e+00, 2.3463e-01, 4.1780e+00, 7.2614e+00]], device='cuda:6')
2025-08-01 11:39:56,668 - forward - INFO - Original Rs:
 tensor([[451.,   2.,  25.],
        [  0.,  27.,   4.],
        [155.,   3.,  18.]], device='cuda:6')
2025-08-01 11:39:56,669 - forward - INFO - Generative mu:
 tensor([[56.7508, 60.1260, 53.7265],
        [10.3388, 43.6731, 38.2124],
        [56.7508, 60.1260, 53.7265]], device='cuda:6', dtype=torch.float64)
2025-08-01 11:39:56,670 - forward - INFO - X_hat theta:
 tensor([[57.4646,  0.6513, 38.8163],
        [ 6.5933,  1.2129,  0.4530],
        [57.4646,  0.6513, 38.8163]], device='cuda:6')
2025-08-01 11:39:56,670 - forward - INFO - JS-divergence omega orig 0.14689707773992297 learn 0.20746672238396704 flat8.195639361607795e-08
2025-08-01 11:39:56,672 - forward - INFO - average loss when compared to Xs: tensor([12766.9141,   489.1125, 17683.8398,  3686.5249,  7884.0703,   583.7059,
          847.9495, 30607.8008,   552.9353,   469.5397,  5948.1445,   625.0786,
          575.3804,  6808.7344,   604.1710,  6719.5938,   649.5713,  4834.2285,
          545.4125,   623.0624, 19457.1367,  1165.0513,  3414.5898,  4907.2764,
          641.3451,  3263.0562,   815.2333,   892.3385,   606.7560,   555.0955,
          586.6745,   551.2678, 38505.6055,   606.2114,  3568.3589,   925.9912,
          642.7933,  3288.5459,   653.6731, 22561.9219,  7131.8003,   801.2882,
          632.3414,   734.8892, 66752.6562,   826.0574,  4472.0488,   612.8871,
         2876.0015,  7825.3262,  4968.4629,   674.3408, 14202.1875,  3219.1680,
         4180.4111,  2646.8562,  1302.6807,  4648.2090,   896.2656,   861.2755,
         1220.4348,   673.2812,   669.5254,   840.5168,  5756.3281, 11172.0986,
        14453.3887,   684.5209,  3654.4785,  3208.6631,   528.5115,   733.6985,
         2739.1206,  5282.1133,   549.8367, 52838.9531,  8670.2510,  9493.8154,
         4832.3169,   970.5065,   512.4750,  2657.1094,  4614.9395,   514.3317,
          859.6021,  4587.7979,  4751.7275,   600.9786, 20560.5312,  5942.5244,
          698.3541,   688.7142, 33242.9375,  4587.9224,  6825.3589,  1800.5718,
         4942.0420,  4443.9761,   677.7356,  3761.1589], device='cuda:6')
2025-08-01 11:39:56,672 - forward - INFO - -------------------------------- END --------------------------------
2025-08-01 11:39:56,673 - forward - INFO - 
2025-08-01 11:39:56,673 - forward - INFO - Loss Function         | Loss Value(s)        
2025-08-01 11:39:56,673 - forward - INFO - ---------------------------------------------
2025-08-01 11:39:56,673 - forward - INFO - Adj KL Lib:           |                   0.0
2025-08-01 11:39:56,674 - forward - INFO - Adj KL Gene:          |     63.54033279418945
2025-08-01 11:39:56,674 - forward - INFO - Adj KL X:             |     257.8431701660156
2025-08-01 11:39:56,674 - forward - INFO - Adj Recon Lib:        | 0.00e+00, 0.00e+00
2025-08-01 11:39:56,675 - forward - INFO - Adj Recon DA:         | 7.02e+05, 7.12e+05
2025-08-01 11:39:56,675 - forward - INFO - Adj Batch Correction: | 0.00e+00
2025-08-01 11:39:56,675 - forward - INFO - Adj Recon Express:    | 5.776e+05            
2025-08-01 11:39:56,676 - forward - INFO - Validation Epoch: 008/1000 | Batch 0001/0002
2025-08-01 11:39:56,677 - forward - INFO - 
Average validation loss per epoch 1986.11181640625
2025-08-01 11:39:56,680 - forward - INFO - 
Time elapsed: 0.03 min
2025-08-01 11:39:56,681 - forward - INFO - 
batch X for epoch: 8 batch: 0
2025-08-01 11:39:56,682 - forward - INFO - Size: torch.Size([100, 100]) dtype: torch.float32 Min: tensor(0., device='cuda:6')Max: tensor(565., device='cuda:6')
2025-08-01 11:39:56,683 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,685 - forward - INFO - tensor([[ 81.,   5.,  47.,  ...,  71.,  41.,  40.],
        [ 42.,   2.,   2.,  ..., 101.,  62., 170.],
        [  0., 395.,  46.,  ...,   0.,  28.,   0.],
        ...,
        [ 21.,  36.,  39.,  ...,  50.,  10.,   0.],
        [ 29.,   5., 104.,  ...,  93.,   7.,  13.],
        [ 57.,  17., 103.,  ...,  61.,  74.,  19.]], device='cuda:6')
2025-08-01 11:39:56,686 - forward - INFO - Size: torch.Size([100, 100]) dtype: torch.float32 Min: tensor(0., device='cuda:6')Max: tensor(602., device='cuda:6')
2025-08-01 11:39:56,686 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,689 - forward - INFO - tensor([[ 14.,   0.,  25.,  ...,  45.,  63.,  33.],
        [110., 127.,  21.,  ..., 113.,   4.,  11.],
        [  3.,  48.,   4.,  ...,   4.,  15.,   9.],
        ...,
        [  1.,  44.,  25.,  ...,  14.,  42.,  33.],
        [  2.,   9.,   7.,  ...,  12.,   5.,  50.],
        [131.,  70.,  32.,  ..., 112.,   4.,   0.]], device='cuda:6')
2025-08-01 11:39:56,689 - forward - INFO - 
batch indices for epoch: 8 batch: 0
2025-08-01 11:39:56,689 - forward - INFO - Size: torch.Size([100]) dtype: torch.int64 Min: tensor(0, device='cuda:6')Max: tensor(99, device='cuda:6')
2025-08-01 11:39:56,690 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,693 - forward - INFO - tensor([10, 71,  5, 83, 73, 33, 25, 37, 81, 51, 11, 15, 82, 48,  2, 14,  7, 27,
        87,  3, 75, 22, 46, 97, 80, 86, 55, 18, 42, 45, 84, 41,  0, 63, 90, 24,
        89, 66, 13, 54, 40, 70, 95, 69, 60,  8, 79, 61, 96, 67, 23, 50, 32, 57,
        77, 88, 56, 36, 85,  1, 21, 17, 94, 62, 19, 74, 99, 65, 38, 98, 31, 52,
        68, 34, 16, 59, 12, 39, 26, 64, 91, 43, 30, 53, 93, 35,  9, 28, 78,  4,
        92, 58, 76, 49,  6, 72, 20, 44, 29, 47], device='cuda:6')
2025-08-01 11:39:56,693 - forward - INFO - Size: torch.Size([100]) dtype: torch.int64 Min: tensor(0, device='cuda:6')Max: tensor(99, device='cuda:6')
2025-08-01 11:39:56,694 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,696 - forward - INFO - tensor([10, 71,  5, 83, 73, 33, 25, 37, 81, 51, 11, 15, 82, 48,  2, 14,  7, 27,
        87,  3, 75, 22, 46, 97, 80, 86, 55, 18, 42, 45, 84, 41,  0, 63, 90, 24,
        89, 66, 13, 54, 40, 70, 95, 69, 60,  8, 79, 61, 96, 67, 23, 50, 32, 57,
        77, 88, 56, 36, 85,  1, 21, 17, 94, 62, 19, 74, 99, 65, 38, 98, 31, 52,
        68, 34, 16, 59, 12, 39, 26, 64, 91, 43, 30, 53, 93, 35,  9, 28, 78,  4,
        92, 58, 76, 49,  6, 72, 20, 44, 29, 47], device='cuda:6')
2025-08-01 11:39:56,727 - forward - INFO - --------------++------ LOSS INFORMATION ------------------++--------
2025-08-01 11:39:56,728 - forward - INFO - X_hat mu:
 tensor([[1.7911e-08, 1.5804e-04, 1.4751e-11],
        [1.2324e-11, 2.4541e-03, 9.9132e-06],
        [1.0811e+00, 2.2539e+01, 4.0124e+01]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,729 - forward - INFO - lib size factors:
 tensor([[5287.],
        [5314.],
        [2985.]], device='cuda:6')
2025-08-01 11:39:56,730 - forward - INFO - omega:
 tensor([[3.3877e-12, 2.9893e-08, 2.7900e-15],
        [2.3192e-15, 4.6183e-07, 1.8655e-09],
        [3.6216e-04, 7.5508e-03, 1.3442e-02]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,736 - forward - INFO - omega * lib_size:
 tensor([[1.7911e-08, 1.5804e-04, 1.4751e-11, 1.0267e-06, 4.6672e-13, 1.0060e-14,
         1.5635e-13, 2.4684e-09, 1.3303e-18, 3.4599e+01, 2.5573e-16, 6.0711e-19,
         2.6939e-03, 6.1846e-15, 1.3596e-16, 2.5799e-13, 2.1394e-12, 1.6273e-10,
         4.2143e-15, 8.5460e-15, 2.5974e-01, 2.1635e-10, 2.7946e-32, 2.0416e-05,
         2.8726e-07, 7.9244e-13, 3.4132e-19, 8.1845e-16, 9.4806e-02, 1.8578e-04,
         4.9260e-12, 2.3872e-05, 4.4368e-21, 1.4485e-10, 1.0756e-08, 8.6833e-06,
         1.6753e-09, 2.3112e-04, 2.6168e-14, 1.8860e+02, 8.6351e-21, 3.4840e-11,
         6.6435e-09, 1.0869e-28, 3.3153e-12, 4.1937e+01, 8.9034e-16, 4.0305e-17,
         2.0460e-06, 1.1116e-09, 3.5442e-07, 8.9054e-03, 2.0609e-14, 6.1495e-17,
         5.0215e+03, 1.5346e-08, 4.5287e-14, 4.9189e-10, 2.9258e-03, 1.2810e-06,
         3.5851e-07, 3.0868e-11, 1.1117e-10, 2.1960e-21, 1.9258e-03, 1.3047e-10,
         3.9298e-12, 1.3616e-03, 5.3482e-23, 6.2434e-21, 3.1696e-10, 4.7641e-17,
         6.1136e-14, 1.3350e-06, 1.6399e-06, 4.3953e-10, 2.6070e-15, 2.8843e-17,
         1.2379e-07, 6.7487e-04, 6.9665e-19, 3.5608e-21, 5.2833e-13, 1.1005e-04,
         3.4874e-17, 2.6555e-02, 1.8051e-13, 1.0519e-13, 1.3628e-06, 6.7852e-05,
         3.7030e-07, 6.1994e-08, 1.7052e-13, 6.2867e-24, 2.8736e-15, 2.7116e-10,
         4.2459e-06, 2.5545e-08, 3.1840e-04, 8.1032e-16],
        [1.2324e-11, 2.4541e-03, 9.9132e-06, 3.3570e-05, 7.4019e-09, 3.9321e-12,
         8.4032e-18, 1.3861e-09, 3.9441e-21, 1.5345e-05, 1.7452e-18, 1.3298e-15,
         2.2485e-05, 3.0797e-07, 6.4341e-19, 1.0115e-15, 5.7718e-11, 1.8184e-18,
         1.1350e-12, 5.5749e-10, 2.3306e-08, 1.0921e-08, 6.1685e-22, 7.9372e-08,
         3.3111e-04, 3.4025e-05, 8.3722e-18, 2.0320e-19, 1.1337e-02, 1.0423e-01,
         2.2352e-10, 1.5295e-02, 4.0935e-11, 1.5630e-04, 2.0283e-09, 1.1042e-01,
         9.4321e-05, 6.8033e-07, 9.5175e-20, 2.8400e-06, 1.1442e-22, 3.3381e-09,
         6.2267e-03, 9.4981e-26, 3.6372e-10, 5.3408e-02, 2.0971e-09, 8.0506e-15,
         6.6972e-03, 3.8230e-07, 2.1709e-08, 1.2516e-05, 3.8672e-11, 1.8749e-09,
         2.4158e-02, 9.5135e-07, 8.8793e-20, 1.3071e-05, 4.4542e+00, 7.8727e-11,
         1.4258e-05, 2.7648e-04, 4.1447e-13, 1.6573e-15, 1.4169e-03, 1.3718e-04,
         3.1965e-12, 1.1554e-08, 4.8482e-10, 1.4016e-12, 7.7317e-06, 2.7974e-11,
         9.9993e-11, 8.8795e-03, 1.0266e-14, 2.5730e-07, 6.0786e-17, 3.5000e-14,
         4.8980e-07, 3.3994e-05, 2.0226e-23, 1.1954e-17, 9.0374e-13, 8.5955e-07,
         5.5097e-18, 6.3203e-02, 2.2595e-15, 1.3709e-14, 7.4416e-03, 1.3070e-02,
         4.7678e+03, 3.6092e-14, 2.7006e-06, 9.7839e-15, 2.2453e-13, 3.2931e-03,
         6.0196e-02, 1.2727e-04, 5.4129e+02, 2.4795e-14],
        [1.0811e+00, 2.2539e+01, 4.0124e+01, 9.3462e+00, 2.3529e+02, 1.9928e+01,
         1.1280e+00, 2.1427e+02, 1.0543e+01, 3.1846e+00, 2.1482e+01, 1.2281e+01,
         3.8611e+00, 4.2315e+01, 8.2928e+00, 5.2763e+01, 1.3226e+00, 1.3822e-01,
         4.6943e+01, 4.4158e+00, 3.8313e+00, 1.2286e+00, 9.8925e+00, 1.0459e+01,
         4.8867e+00, 5.7154e+00, 1.1301e+00, 2.7184e+01, 2.2904e+01, 3.0570e+01,
         5.9291e-01, 1.2870e+00, 7.7504e+00, 1.3937e+00, 3.8075e-01, 3.0643e+01,
         5.3630e+00, 9.3564e+01, 4.6214e+01, 7.2871e+00, 1.5192e+01, 5.7908e+00,
         4.6534e+01, 3.9092e-01, 3.3316e+01, 3.5559e+01, 1.6509e+02, 9.3063e+00,
         5.4958e-01, 8.7717e+00, 5.0136e+00, 1.8181e+00, 2.2193e+01, 6.6109e+01,
         1.4758e+00, 5.3664e+00, 6.5506e+01, 1.8212e+01, 2.6754e+01, 8.3305e+00,
         9.1812e+00, 4.0564e+01, 7.9371e+00, 1.2191e+00, 6.2493e-01, 1.3245e+01,
         4.6132e+01, 3.3325e+01, 2.7448e+01, 4.5357e+02, 6.3950e+00, 1.2416e+02,
         3.5132e+01, 1.7010e+01, 2.8284e+00, 1.9042e+01, 4.1535e+00, 2.5341e+01,
         3.1598e+01, 3.1167e+00, 2.3211e+00, 1.4381e+01, 4.0332e+01, 3.0186e+01,
         7.6149e-01, 1.7497e+01, 3.4598e+00, 9.0667e-01, 2.4356e+00, 1.2079e+02,
         5.2359e+00, 1.1398e+00, 1.1764e+01, 1.1096e+00, 1.0557e+02, 8.9419e+00,
         1.3410e+00, 1.4786e+01, 8.3706e+01, 1.1494e+01]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,737 - forward - INFO - Original Rs:
 tensor([[ 81.,   5.,  47.],
        [ 42.,   2.,   2.],
        [  0., 395.,  46.]], device='cuda:6')
2025-08-01 11:39:56,738 - forward - INFO - Generative mu:
 tensor([[56.7508, 60.1260, 53.7265],
        [56.7508, 60.1260, 53.7265],
        [10.3388, 43.6731, 38.2124]], device='cuda:6', dtype=torch.float64)
2025-08-01 11:39:56,739 - forward - INFO - X_hat theta:
 tensor([[9.7267e+06, 2.0083e+00, 1.1054e+06],
        [9.7267e+06, 2.0083e+00, 1.1054e+06],
        [1.0124e+05, 3.7029e+00, 4.7112e-01]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,740 - forward - INFO - JS-divergence omega orig 0.14330881665825462 learn 0.308168136146486 flat8.195639361607795e-08
2025-08-01 11:39:56,742 - forward - INFO - average loss when compared to Xs: tensor([99210.0000, 89479.9922,  1776.8358,  2069.9639,  8359.9707,   910.2029,
         1171.9708, 64614.5391,  1140.9524,  3677.0425,  1219.1166, 16850.2734,
         5328.8394,  1519.0662, 12834.9492, 16912.9414,  1803.4155,  1733.9375,
        95461.0625,  1906.2531,  1238.4299, 24168.9648,  1742.4241, 64228.1133,
         4438.5811,  2490.8057, 17890.1309, 12747.6055,  4399.9204,  2936.9116,
        61915.1719,  7316.7944,  2117.9536,  1357.5146,  2816.3093,  8471.8828,
         1143.8616,  5449.5713,  3641.1121, 77141.9531,  4906.8994,  8855.4375,
         2070.3257,  8199.3154,  1097.6787,  1291.2819,  1418.0336,  2323.4390,
         1846.6198, 27023.0254,  1518.1460, 47827.2656, 32805.6211,  3032.6343,
        30422.1250,  9805.2705,  1127.5934,  3690.6704,  7177.3828,  8486.2363,
         7023.5503,  6263.4233,  2388.9219, 11053.0996,  2751.9495,  6419.3413,
        16432.1055,  1983.2089,  4276.8701,  5790.2900,  4886.6982, 24202.8223,
         5934.4727, 31190.0898, 32408.4746, 11571.4473,  2831.0312,  2327.6494,
         2384.1455,  7399.9277,  2865.1743, 14789.3945,  8711.9297,  2757.8982,
        76006.9062, 10863.9668, 28335.0996,  1847.3223, 26943.1875, 15520.3828,
        26372.6543,  2365.3135,  1083.6257, 80922.7031,  6345.2705, 17692.2852,
          882.8566,  1970.6195,  9497.9219,  7765.8457], device='cuda:6',
       grad_fn=<MulBackward0>)
2025-08-01 11:39:56,742 - forward - INFO - -------------------------------- END --------------------------------
2025-08-01 11:39:56,743 - forward - INFO - 
2025-08-01 11:39:56,743 - forward - INFO - Loss Function         | Loss Value(s)        
2025-08-01 11:39:56,743 - forward - INFO - ---------------------------------------------
2025-08-01 11:39:56,744 - forward - INFO - Adj KL Lib:           |                   0.0
2025-08-01 11:39:56,744 - forward - INFO - Adj KL Gene:          |      265.901123046875
2025-08-01 11:39:56,744 - forward - INFO - Adj KL X:             |     546.3421630859375
2025-08-01 11:39:56,744 - forward - INFO - Adj Recon Lib:        | 0.00e+00, 0.00e+00
2025-08-01 11:39:56,744 - forward - INFO - Adj Recon DA:         | 9.85e+05, 1.04e+06
2025-08-01 11:39:56,745 - forward - INFO - Adj Batch Correction: | 0.00e+00
2025-08-01 11:39:56,745 - forward - INFO - Adj Recon Express:    | 1.474e+06            
2025-08-01 11:39:56,773 - forward - INFO - 
Training Epoch: 009/1000 | Batch 0001/0001
2025-08-01 11:39:56,773 - forward - INFO - 
Average training loss per epoch 6975.18896484375
2025-08-01 11:39:56,804 - forward - INFO - --------------++------ LOSS INFORMATION ------------------++--------
2025-08-01 11:39:56,805 - forward - INFO - X_hat mu:
 tensor([[1.8569e-01, 2.8153e+01, 8.3499e+01],
        [6.3734e+00, 1.2065e+01, 5.0854e+01],
        [2.2536e+00, 3.0367e+01, 2.1493e+02]], device='cuda:6')
2025-08-01 11:39:56,806 - forward - INFO - lib size factors:
 tensor([[5781.],
        [2099.],
        [4750.]], device='cuda:6')
2025-08-01 11:39:56,807 - forward - INFO - omega:
 tensor([[3.2120e-05, 4.8699e-03, 1.4444e-02],
        [3.0364e-03, 5.7477e-03, 2.4228e-02],
        [4.7445e-04, 6.3930e-03, 4.5248e-02]], device='cuda:6')
2025-08-01 11:39:56,813 - forward - INFO - omega * lib_size:
 tensor([[1.8569e-01, 2.8153e+01, 8.3499e+01, 1.9562e+01, 1.3455e+02, 1.4268e-01,
         4.6824e+00, 2.1606e+01, 3.2673e+01, 3.6720e+00, 9.8513e+00, 3.2714e+00,
         5.6829e+00, 4.4751e+00, 4.0851e-01, 5.4327e+00, 4.4754e-01, 6.5175e-02,
         1.9915e+01, 6.3469e+00, 3.3020e-01, 1.3827e+00, 4.0381e+01, 2.0547e+01,
         4.5271e+00, 1.0045e+01, 2.0427e+00, 6.6130e+00, 1.7192e+02, 4.3978e+01,
         1.2123e+01, 1.4549e+01, 2.2539e+02, 7.0375e+00, 1.6456e+01, 2.1820e+00,
         8.0895e+00, 6.4721e+01, 2.8496e+01, 5.4473e+00, 4.1456e+00, 5.1630e+00,
         8.6294e+00, 1.8061e-01, 1.2034e+00, 7.9184e-01, 1.9129e+00, 3.4229e-01,
         6.8571e-01, 4.6403e+00, 4.7283e+00, 6.7148e+00, 4.0728e+01, 1.5367e+03,
         8.3348e+00, 6.8286e+00, 1.9329e+01, 4.7674e+00, 1.2557e+01, 3.6508e+00,
         7.7084e+00, 5.3840e+01, 3.1533e-01, 3.3058e+01, 2.0465e+00, 1.1284e+01,
         1.4646e+02, 6.8656e+01, 4.0972e+01, 1.2292e+01, 1.6767e+00, 4.5122e+01,
         2.2269e+01, 4.9984e+02, 1.7364e-01, 5.7542e+00, 1.1162e-01, 2.4364e+02,
         1.0766e+02, 1.2287e+01, 1.5732e+00, 6.0967e+00, 1.0297e+02, 1.1436e+03,
         2.8825e+01, 2.9905e+00, 8.5469e+00, 2.4915e+00, 2.4359e+00, 1.6514e+02,
         9.9436e-01, 4.5469e-01, 7.7412e+01, 9.7498e+00, 3.2800e+01, 2.5479e+01,
         3.4613e+00, 1.9703e+00, 7.6346e+01, 2.9576e+01],
        [6.3734e+00, 1.2065e+01, 5.0854e+01, 4.6833e+01, 2.0778e+01, 1.7889e+01,
         4.1065e+00, 2.7100e+01, 1.3331e+01, 1.3784e+01, 1.4133e+01, 3.0804e+01,
         4.8216e+00, 2.2661e+01, 4.9603e+00, 1.2159e+01, 2.8460e+00, 8.2244e+00,
         3.6475e+01, 1.9375e+01, 1.4629e+01, 1.3242e+01, 2.0880e+01, 9.1395e+00,
         8.3458e+00, 1.8793e+01, 5.8101e+00, 1.6902e+01, 4.6670e+01, 2.7136e+01,
         2.1954e+01, 9.1175e+00, 7.2468e+01, 1.2851e+01, 8.6684e+00, 2.5988e+01,
         9.3511e+00, 2.8265e+01, 9.4403e+00, 9.2045e+00, 9.0574e+00, 1.1271e+01,
         2.9854e+01, 1.9069e+00, 1.5584e+01, 1.9955e+01, 1.0041e+01, 9.2372e+00,
         7.3086e+00, 1.5716e+01, 1.3856e+01, 2.0169e+01, 1.9731e+01, 5.9020e+01,
         1.0301e+01, 9.4066e+00, 1.9205e+01, 1.5843e+01, 4.6498e+01, 8.4341e+00,
         8.2063e+00, 2.8440e+01, 1.4943e+01, 3.8276e+01, 1.4399e+01, 1.8197e+01,
         1.7885e+01, 2.1330e+01, 2.8644e+01, 2.5865e+01, 1.2249e+01, 2.5898e+01,
         1.3123e+01, 3.8439e+01, 7.8561e+00, 1.4186e+01, 2.2101e+00, 7.7065e+01,
         3.3643e+01, 1.3526e+01, 1.0334e+01, 1.7847e+01, 6.1908e+01, 8.2225e+01,
         1.0123e+01, 2.8948e+01, 1.9983e+01, 3.0456e+01, 4.0117e+00, 4.4259e+01,
         2.3046e+01, 1.2097e+01, 3.0567e+01, 3.6588e+00, 2.0203e+01, 2.4817e+01,
         1.1860e+01, 2.7530e+01, 4.3833e+01, 2.6088e+01],
        [2.2536e+00, 3.0367e+01, 2.1493e+02, 3.3099e+01, 1.8938e+02, 5.6759e+00,
         2.7091e+00, 2.6732e+01, 3.9285e+01, 9.4017e+01, 5.3109e+01, 1.1529e+01,
         1.0725e+01, 2.6094e+01, 4.1747e+00, 4.4347e+01, 3.3883e+00, 1.3541e+00,
         6.2747e+01, 1.2571e+01, 2.3294e+01, 2.5292e+01, 1.4194e+01, 1.8558e+01,
         4.5190e+00, 2.2159e+01, 2.0139e+01, 1.8502e+01, 2.2702e+01, 1.2633e+01,
         6.5735e+00, 3.6191e+00, 8.5990e+01, 2.2951e+00, 2.9652e+02, 1.9273e+01,
         1.7256e+01, 1.4083e+02, 6.9034e+01, 1.9243e+01, 4.1001e+00, 2.2111e+01,
         2.5084e+02, 9.5309e-01, 1.3275e+01, 3.5714e+01, 2.1718e+00, 1.2089e+01,
         9.0052e+01, 2.8902e+01, 1.7494e+01, 9.8839e+00, 1.3758e+02, 6.0397e+01,
         1.7736e+01, 7.2928e+00, 1.8822e+01, 5.4975e+00, 1.4046e+01, 5.2621e+00,
         7.2626e+00, 8.3315e+01, 4.8159e+00, 2.3353e+00, 2.7993e+01, 1.0246e+01,
         4.1391e+01, 6.4952e+00, 6.4916e+01, 1.7288e+02, 2.1877e+01, 3.4947e+01,
         6.1377e+01, 5.5532e+01, 9.4227e+00, 1.8708e+02, 2.3679e+00, 3.1711e+02,
         1.7814e+01, 1.1072e+02, 2.0864e+01, 2.8872e+00, 7.7538e+01, 4.4063e+02,
         7.5540e+00, 6.2187e+00, 1.3974e+01, 1.3911e+01, 2.0376e+01, 2.6822e+01,
         1.8084e+00, 1.8217e+00, 1.5040e+02, 1.5645e+01, 7.4346e+00, 2.0471e+01,
         3.1932e+01, 3.7151e+01, 1.0669e+02, 4.0641e+01]], device='cuda:6')
2025-08-01 11:39:56,814 - forward - INFO - Original Rs:
 tensor([[451.,   2.,  25.],
        [  0.,  27.,   4.],
        [155.,   3.,  18.]], device='cuda:6')
2025-08-01 11:39:56,815 - forward - INFO - Generative mu:
 tensor([[56.7508, 60.1260, 53.7265],
        [10.3388, 43.6731, 38.2124],
        [56.7508, 60.1260, 53.7265]], device='cuda:6', dtype=torch.float64)
2025-08-01 11:39:56,816 - forward - INFO - X_hat theta:
 tensor([[29.8989,  0.5899, 21.3518],
        [ 3.6192,  1.0035,  0.5113],
        [29.8989,  0.5899, 21.3518]], device='cuda:6')
2025-08-01 11:39:56,816 - forward - INFO - JS-divergence omega orig 0.14689707773992297 learn 0.15701734147249233 flat8.195639361607795e-08
2025-08-01 11:39:56,818 - forward - INFO - average loss when compared to Xs: tensor([ 7621.2939,   490.3210,  3108.0667,  1921.1494,  5911.5850,   768.7224,
         1638.3662,  6104.2764,   587.9218,   522.9403,  1875.8135,   620.0952,
          617.4460,  2306.9277,   698.2670,  1944.9069,   530.6113,  2343.3989,
          680.8059,   817.8730,  2933.5479, 13861.9434,  2380.1045,  1932.5161,
          829.9564,  3021.2263,   981.9459,  1168.2675,   705.7332,   669.4402,
          654.8834,   569.1171,  1298.8357,   775.6726,  2669.0361,  1163.1155,
          730.2900,  2081.4976,   823.5430,  1666.5232,  1586.6742,   955.8901,
          869.7902,   965.6700, 24571.3262,  1308.5029,  1965.0182,   758.1783,
         3062.7378,  1845.0442,  3363.6216,   749.2025,  5077.8535, 20641.5078,
         1926.8485,  1471.4639,   753.6074,  2862.2339,  1241.4464,   795.4651,
         2212.7495,   616.4637,   703.4031,   999.5585,  2275.9036,  1548.2550,
         2062.2961,   686.8766,  1034.5917,  1920.8948,   647.0529,   733.1725,
         1378.5386,  1984.3228,   588.7480,  2355.3374,  3620.8364,  2062.0107,
         1635.3203,   883.0089,   640.1898,  1530.4487,  2985.0049,   536.5487,
         1308.4952,  1419.4294, 16423.1758,   665.6465,  2047.7539,  1898.1079,
          692.8947,   710.4461,  2441.7441,  1607.9597,   983.2969,  1823.5472,
          892.0806,  2315.0605,   940.3619,  1306.7736], device='cuda:6')
2025-08-01 11:39:56,818 - forward - INFO - -------------------------------- END --------------------------------
2025-08-01 11:39:56,819 - forward - INFO - 
2025-08-01 11:39:56,820 - forward - INFO - Loss Function         | Loss Value(s)        
2025-08-01 11:39:56,820 - forward - INFO - ---------------------------------------------
2025-08-01 11:39:56,821 - forward - INFO - Adj KL Lib:           |                   0.0
2025-08-01 11:39:56,821 - forward - INFO - Adj KL Gene:          |     54.03108215332031
2025-08-01 11:39:56,821 - forward - INFO - Adj KL X:             |    153.58377075195312
2025-08-01 11:39:56,822 - forward - INFO - Adj Recon Lib:        | 0.00e+00, 0.00e+00
2025-08-01 11:39:56,822 - forward - INFO - Adj Recon DA:         | 6.46e+05, 6.76e+05
2025-08-01 11:39:56,822 - forward - INFO - Adj Batch Correction: | 0.00e+00
2025-08-01 11:39:56,822 - forward - INFO - Adj Recon Express:    | 2.319e+05            
2025-08-01 11:39:56,823 - forward - INFO - Validation Epoch: 009/1000 | Batch 0001/0002
2025-08-01 11:39:56,824 - forward - INFO - 
Average validation loss per epoch 1549.4676513671875
2025-08-01 11:39:56,827 - forward - INFO - 
Time elapsed: 0.03 min
2025-08-01 11:39:56,828 - forward - INFO - 
batch X for epoch: 9 batch: 0
2025-08-01 11:39:56,829 - forward - INFO - Size: torch.Size([100, 100]) dtype: torch.float32 Min: tensor(0., device='cuda:6')Max: tensor(565., device='cuda:6')
2025-08-01 11:39:56,830 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,833 - forward - INFO - tensor([[ 81.,   5.,  47.,  ...,  71.,  41.,  40.],
        [ 42.,   2.,   2.,  ..., 101.,  62., 170.],
        [  0., 395.,  46.,  ...,   0.,  28.,   0.],
        ...,
        [ 21.,  36.,  39.,  ...,  50.,  10.,   0.],
        [ 29.,   5., 104.,  ...,  93.,   7.,  13.],
        [ 57.,  17., 103.,  ...,  61.,  74.,  19.]], device='cuda:6')
2025-08-01 11:39:56,833 - forward - INFO - Size: torch.Size([100, 100]) dtype: torch.float32 Min: tensor(0., device='cuda:6')Max: tensor(602., device='cuda:6')
2025-08-01 11:39:56,834 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,837 - forward - INFO - tensor([[ 14.,   0.,  25.,  ...,  45.,  63.,  33.],
        [110., 127.,  21.,  ..., 113.,   4.,  11.],
        [  3.,  48.,   4.,  ...,   4.,  15.,   9.],
        ...,
        [  1.,  44.,  25.,  ...,  14.,  42.,  33.],
        [  2.,   9.,   7.,  ...,  12.,   5.,  50.],
        [131.,  70.,  32.,  ..., 112.,   4.,   0.]], device='cuda:6')
2025-08-01 11:39:56,837 - forward - INFO - 
batch indices for epoch: 9 batch: 0
2025-08-01 11:39:56,837 - forward - INFO - Size: torch.Size([100]) dtype: torch.int64 Min: tensor(0, device='cuda:6')Max: tensor(99, device='cuda:6')
2025-08-01 11:39:56,838 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,840 - forward - INFO - tensor([10, 71,  5, 83, 73, 33, 25, 37, 81, 51, 11, 15, 82, 48,  2, 14,  7, 27,
        87,  3, 75, 22, 46, 97, 80, 86, 55, 18, 42, 45, 84, 41,  0, 63, 90, 24,
        89, 66, 13, 54, 40, 70, 95, 69, 60,  8, 79, 61, 96, 67, 23, 50, 32, 57,
        77, 88, 56, 36, 85,  1, 21, 17, 94, 62, 19, 74, 99, 65, 38, 98, 31, 52,
        68, 34, 16, 59, 12, 39, 26, 64, 91, 43, 30, 53, 93, 35,  9, 28, 78,  4,
        92, 58, 76, 49,  6, 72, 20, 44, 29, 47], device='cuda:6')
2025-08-01 11:39:56,841 - forward - INFO - Size: torch.Size([100]) dtype: torch.int64 Min: tensor(0, device='cuda:6')Max: tensor(99, device='cuda:6')
2025-08-01 11:39:56,842 - forward - INFO - Exist NaNs? False Exist infinities? False
2025-08-01 11:39:56,844 - forward - INFO - tensor([10, 71,  5, 83, 73, 33, 25, 37, 81, 51, 11, 15, 82, 48,  2, 14,  7, 27,
        87,  3, 75, 22, 46, 97, 80, 86, 55, 18, 42, 45, 84, 41,  0, 63, 90, 24,
        89, 66, 13, 54, 40, 70, 95, 69, 60,  8, 79, 61, 96, 67, 23, 50, 32, 57,
        77, 88, 56, 36, 85,  1, 21, 17, 94, 62, 19, 74, 99, 65, 38, 98, 31, 52,
        68, 34, 16, 59, 12, 39, 26, 64, 91, 43, 30, 53, 93, 35,  9, 28, 78,  4,
        92, 58, 76, 49,  6, 72, 20, 44, 29, 47], device='cuda:6')
2025-08-01 11:39:56,875 - forward - INFO - --------------++------ LOSS INFORMATION ------------------++--------
2025-08-01 11:39:56,876 - forward - INFO - X_hat mu:
 tensor([[1.1176e+01, 1.9342e+00, 4.8729e+01],
        [2.9049e-10, 6.2130e-04, 5.5341e-05],
        [4.3818e+00, 3.1329e+01, 7.4538e+01]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,877 - forward - INFO - lib size factors:
 tensor([[5287.],
        [5314.],
        [2985.]], device='cuda:6')
2025-08-01 11:39:56,878 - forward - INFO - omega:
 tensor([[2.1139e-03, 3.6585e-04, 9.2168e-03],
        [5.4665e-14, 1.1692e-07, 1.0414e-08],
        [1.4680e-03, 1.0495e-02, 2.4971e-02]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,884 - forward - INFO - omega * lib_size:
 tensor([[1.1176e+01, 1.9342e+00, 4.8729e+01, 3.2610e+01, 4.9696e+01, 8.0009e-01,
         6.3204e-02, 9.3333e+00, 2.4527e+01, 8.0690e+01, 8.1968e+00, 5.7773e-01,
         3.3427e-01, 6.9023e+00, 1.3653e+00, 9.4910e-01, 6.7524e+01, 3.4008e+00,
         2.5357e+01, 1.0917e+01, 2.7598e-01, 2.4115e+00, 3.8839e+00, 3.7517e+00,
         4.7575e+00, 3.9181e+01, 1.8898e+01, 4.9772e-02, 3.7232e-02, 1.6364e+02,
         3.0616e+00, 1.0426e+00, 7.5571e+02, 2.6072e-01, 2.6989e+01, 4.9651e+01,
         5.2982e+00, 8.5517e+01, 3.2037e+00, 6.0793e+00, 2.6460e+00, 4.3038e+00,
         6.6849e+02, 7.5731e+00, 1.9179e+01, 6.1987e-01, 4.4360e+01, 1.7852e-01,
         8.2915e+01, 1.3759e+02, 1.5953e+00, 5.5009e+00, 1.8539e+01, 1.2466e+00,
         1.1944e+00, 2.7015e-02, 5.2746e+00, 5.1986e+01, 8.7296e+01, 6.5760e+00,
         1.0603e+02, 1.2320e+01, 1.4850e+00, 6.1168e+00, 7.3542e+01, 5.6168e+00,
         5.1334e+01, 6.3773e+01, 8.8003e+02, 1.0794e+02, 4.6972e+00, 6.4874e+01,
         1.0930e+02, 4.6542e+01, 3.7802e+01, 4.9881e+02, 4.4758e-01, 1.0141e+01,
         8.8250e+01, 2.8455e-01, 1.0610e+00, 5.4941e-01, 5.6586e+00, 9.3228e+01,
         2.1093e-01, 4.8222e+01, 4.9618e+00, 9.3130e-01, 4.4387e+00, 2.4666e+00,
         4.4258e+01, 2.6841e-01, 7.8604e+01, 4.8270e+00, 1.2199e-01, 9.8714e+00,
         3.6105e+01, 5.6109e+01, 1.9355e+01, 5.4941e-01],
        [2.9049e-10, 6.2130e-04, 5.5341e-05, 1.9782e-11, 1.8883e-05, 8.5563e-07,
         6.8287e-08, 7.1786e-05, 6.3583e-11, 1.2421e-03, 3.9822e-08, 2.2151e-06,
         8.3709e-03, 8.6358e-06, 4.5039e-09, 4.7380e-03, 2.8916e+01, 1.8319e-08,
         2.0651e-03, 2.1590e-05, 3.5189e-10, 1.6139e-04, 6.8636e-11, 1.7318e-04,
         7.7624e-06, 5.3766e-03, 6.6673e-11, 8.0391e-08, 1.1783e-07, 5.1920e+03,
         5.3641e-11, 7.3258e-04, 3.5567e-05, 4.6330e-03, 6.2678e-08, 6.8539e-04,
         1.6045e+01, 1.3850e-06, 2.7130e-02, 1.6686e-05, 7.1997e-08, 1.3961e+01,
         3.9515e+00, 1.5393e-20, 4.5940e-06, 1.1985e-07, 1.1569e-06, 3.5280e-09,
         2.5443e-11, 2.9192e-07, 3.3428e-02, 7.5715e-06, 1.8134e+00, 1.8493e-01,
         2.5136e-02, 7.0697e-07, 1.1289e-07, 9.2248e-01, 4.3183e-04, 1.1320e-01,
         5.0216e-06, 1.5777e-05, 1.4262e-07, 7.0327e-07, 2.8183e+00, 1.3752e-11,
         2.8482e-02, 1.0818e-02, 5.9335e-09, 2.3423e-06, 3.2854e-05, 4.6720e-07,
         8.9491e-06, 6.3201e+00, 2.2877e+01, 2.2899e+01, 4.5061e-06, 5.4966e-08,
         1.1532e-04, 8.6655e-06, 6.4312e-10, 2.7304e-10, 3.5838e-05, 7.4453e-08,
         4.4988e-08, 6.2917e-04, 1.8703e-10, 2.6865e-04, 1.9465e-05, 4.2690e-03,
         7.3025e-01, 3.0832e-10, 1.4657e-06, 7.5129e-04, 2.6139e-05, 8.5144e-08,
         1.8842e-01, 3.2638e-08, 5.1899e-02, 2.8584e-08],
        [4.3818e+00, 3.1329e+01, 7.4538e+01, 3.1786e+01, 3.2912e+01, 4.7735e+01,
         2.7938e+00, 4.9982e+01, 3.9708e+01, 1.8680e+01, 5.0591e+01, 1.5814e+01,
         2.7175e+01, 4.4902e+01, 9.1060e+00, 2.6285e+01, 1.3511e+00, 5.0455e+00,
         3.6809e+01, 5.7677e+00, 2.7614e+01, 1.5935e+01, 5.9242e+00, 3.0593e+01,
         5.0054e+00, 9.3440e+00, 6.8161e+00, 1.9246e+02, 3.3132e+01, 1.9891e+01,
         2.5231e+01, 4.8363e+00, 2.0146e+01, 6.0364e+00, 9.8193e+00, 2.1540e+01,
         2.7122e+01, 1.0816e+02, 5.5730e+01, 5.3222e+01, 1.2044e+01, 2.0156e+01,
         7.6270e+01, 2.7651e+00, 4.3083e+01, 9.1028e+01, 3.1812e+01, 1.4784e+01,
         3.1185e+01, 1.0595e+01, 6.4695e+00, 7.3208e+00, 1.9897e+01, 1.7566e+02,
         8.0390e+00, 5.9084e+00, 1.6651e+01, 2.0097e+01, 1.4357e+01, 1.8555e+01,
         1.9085e+01, 4.3224e+01, 7.6516e+01, 5.6227e+00, 9.2364e+00, 2.6419e+01,
         6.9360e+01, 7.3589e+00, 3.7037e+01, 8.9840e+01, 2.9832e+00, 3.1817e+01,
         6.9554e+00, 2.8242e+01, 9.7946e+00, 3.3539e+01, 2.3554e+01, 3.4823e+01,
         1.1431e+01, 3.7718e+01, 2.0503e+01, 2.0199e+01, 5.0573e+01, 6.9955e+01,
         2.0748e+00, 2.3827e+01, 2.2365e+01, 5.9875e+00, 5.1497e+00, 1.0878e+01,
         6.3699e+00, 2.9630e+01, 3.3980e+01, 5.2936e+00, 2.8811e+01, 1.4288e+01,
         5.2053e+00, 3.9728e+01, 8.0822e+01, 4.2879e+01]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,885 - forward - INFO - Original Rs:
 tensor([[ 81.,   5.,  47.],
        [ 42.,   2.,   2.],
        [  0., 395.,  46.]], device='cuda:6')
2025-08-01 11:39:56,886 - forward - INFO - Generative mu:
 tensor([[56.7508, 60.1260, 53.7265],
        [56.7508, 60.1260, 53.7265],
        [10.3388, 43.6731, 38.2124]], device='cuda:6', dtype=torch.float64)
2025-08-01 11:39:56,887 - forward - INFO - X_hat theta:
 tensor([[242.4592,   0.5374,  82.4731],
        [242.4592,   0.5374,  82.4731],
        [ 12.7497,   0.9173,   0.5421]], device='cuda:6',
       grad_fn=<SliceBackward0>)
2025-08-01 11:39:56,887 - forward - INFO - JS-divergence omega orig 0.14330881665825462 learn 0.26836439358482334 flat8.195639361607795e-08
2025-08-01 11:39:56,889 - forward - INFO - average loss when compared to Xs: tensor([10713.1084, 72388.3750,   772.3280,  2178.7539,  5646.0288,   652.3970,
          960.7148, 84777.6016,   908.5908,  1505.3481,  1130.0752,  2198.6016,
         5917.7891,   833.1959, 25384.1934, 13463.4678,  1242.9688,   930.5682,
        22838.2812,  1253.6849,   548.3531,  6206.6055,  1505.8728, 58274.4531,
         6682.5708,   803.5900,  8696.2930,   860.8636,  6449.0508,  6968.6621,
         9066.5410, 49981.4844,   953.7678,   682.1334,   836.6976,  7755.0039,
          725.8923,  5352.5347,  3563.5942, 61885.7539,  5451.5156,  8267.1533,
          937.8613, 14354.0293,   702.3750,  1661.1216,   662.5251,  2515.8694,
         1026.5137,  7726.8770,   992.4280,  6928.0322,  1027.9222,  1121.5651,
        23439.3203,  1785.5786,   858.2200,  2927.4568, 21195.6758, 28563.9531,
        57844.7656, 61183.9531,   953.0261,  5984.6895,   904.2977, 96331.8203,
        25131.9043,  3026.0398,   964.3481,  1368.1089,  3646.9141,  5777.9365,
        18850.1348, 15255.3047, 33635.6250,  4693.4136,  3838.4194,   833.1360,
          846.5570,  6624.4580,   957.0346,  1975.0801, 12055.3652,  2327.1379,
        30898.0723, 13474.0615,  6373.1050,   642.9776, 12050.3965, 42558.9258,
         2755.2290,  6795.7300,   685.8115, 35270.5195,  4813.1230,  2891.9475,
          755.0232,   883.8929, 20219.9551,  4810.8374], device='cuda:6',
       grad_fn=<MulBackward0>)
2025-08-01 11:39:56,889 - forward - INFO - -------------------------------- END --------------------------------
2025-08-01 11:39:56,890 - forward - INFO - 
2025-08-01 11:39:56,891 - forward - INFO - Loss Function         | Loss Value(s)        
2025-08-01 11:39:56,891 - forward - INFO - ---------------------------------------------
2025-08-01 11:39:56,891 - forward - INFO - Adj KL Lib:           |                   0.0
2025-08-01 11:39:56,892 - forward - INFO - Adj KL Gene:          |     180.0206298828125
2025-08-01 11:39:56,892 - forward - INFO - Adj KL X:             |     444.9504699707031
2025-08-01 11:39:56,892 - forward - INFO - Adj Recon Lib:        | 0.00e+00, 0.00e+00
2025-08-01 11:39:56,892 - forward - INFO - Adj Recon DA:         | 8.69e+05, 1.18e+06
2025-08-01 11:39:56,892 - forward - INFO - Adj Batch Correction: | 0.00e+00
2025-08-01 11:39:56,893 - forward - INFO - Adj Recon Express:    | 1.177e+06            
2025-08-01 11:39:56,924 - forward - INFO - 
Training Epoch: 010/1000 | Batch 0001/0001
2025-08-01 11:39:56,924 - forward - INFO - 
Average training loss per epoch 6439.07958984375
2025-08-01 11:39:56,954 - forward - INFO - --------------++------ LOSS INFORMATION ------------------++--------
2025-08-01 11:39:56,955 - forward - INFO - X_hat mu:
 tensor([[3.1446e-01, 1.5431e+01, 6.7163e+02],
        [3.2846e+00, 1.8840e+01, 9.2900e+01],
        [3.4397e+00, 4.7928e+01, 1.9962e+02]], device='cuda:6')
2025-08-01 11:39:56,956 - forward - INFO - lib size factors:
 tensor([[5781.],
        [2099.],
        [4750.]], device='cuda:6')
2025-08-01 11:39:56,958 - forward - INFO - omega:
 tensor([[5.4395e-05, 2.6693e-03, 1.1618e-01],
        [1.5649e-03, 8.9758e-03, 4.4259e-02],
        [7.2414e-04, 1.0090e-02, 4.2026e-02]], device='cuda:6')
2025-08-01 11:39:56,963 - forward - INFO - omega * lib_size:
 tensor([[3.1446e-01, 1.5431e+01, 6.7163e+02, 5.4618e+01, 1.1088e+02, 4.9636e+00,
         2.4838e+00, 2.3161e+01, 3.7892e+01, 4.5855e+01, 1.1127e+01, 4.7102e+00,
         1.9578e+00, 2.0347e+01, 3.2033e+00, 8.6100e+00, 3.4370e+00, 4.9533e+00,
         6.6151e+01, 1.5561e+01, 1.2641e+01, 4.4448e+00, 2.2105e+01, 8.6033e+00,
         9.3415e+00, 3.0808e+01, 7.7060e+00, 1.1379e+00, 3.0708e+01, 1.4632e+01,
         3.6219e+01, 4.4439e+00, 3.3188e+02, 3.7348e+00, 3.0079e+01, 1.6976e+01,
         8.5509e+00, 3.7960e+02, 9.5261e+00, 1.4524e+01, 3.7745e+00, 3.6090e+00,
         3.4403e+02, 9.2898e-01, 4.4346e+01, 9.9577e+00, 6.0264e+00, 5.1056e+00,
         9.9876e+00, 1.3077e+01, 6.9540e+00, 1.6105e+01, 5.5173e+01, 9.6780e+01,
         9.7972e-01, 2.7109e+00, 1.2074e+01, 7.0951e+00, 5.3187e+01, 6.2169e+00,
         8.5220e+00, 1.6560e+02, 8.5423e+00, 2.9061e+01, 8.5779e+00, 3.7763e+00,
         4.1450e+01, 1.2857e+01, 1.6717e+02, 2.5970e+01, 2.2475e+01, 1.1049e+02,
         2.2198e+01, 3.0410e+01, 4.1498e+00, 5.8732e+01, 4.7793e-01, 1.9034e+02,
         4.7301e+01, 8.1484e+00, 6.3774e+00, 1.1849e+00, 9.0212e+01, 1.3907e+03,
         7.2272e+00, 1.1820e+01, 4.9140e+01, 4.1442e+01, 5.2247e+00, 2.0780e+01,
         2.5997e+00, 4.4158e-01, 2.8895e+02, 4.5593e+00, 1.9533e+00, 1.0422e+01,
         1.1215e+01, 3.1170e+01, 4.4147e+01, 1.2124e+01],
        [3.2846e+00, 1.8840e+01, 9.2900e+01, 3.6087e+01, 3.4671e+01, 1.5976e+01,
         3.4082e+00, 3.2788e+01, 2.2576e+01, 1.1960e+01, 2.1416e+01, 2.4178e+01,
         5.2509e+00, 1.9514e+01, 6.5477e+00, 1.5526e+01, 1.9921e+00, 6.3724e+00,
         3.3623e+01, 1.7135e+01, 1.7326e+01, 1.2792e+01, 1.6343e+01, 9.7871e+00,
         8.8343e+00, 1.7285e+01, 7.1205e+00, 1.9202e+01, 3.8956e+01, 2.3482e+01,
         1.9179e+01, 8.0900e+00, 5.7586e+01, 9.4923e+00, 1.0778e+01, 1.9338e+01,
         8.6600e+00, 4.0533e+01, 1.2138e+01, 1.3073e+01, 1.1124e+01, 8.8815e+00,
         3.8833e+01, 2.0320e+00, 2.4173e+01, 2.2441e+01, 1.1737e+01, 8.6692e+00,
         5.1088e+00, 1.1996e+01, 1.3253e+01, 1.5940e+01, 1.9304e+01, 6.5280e+01,
         5.3245e+00, 1.0384e+01, 2.1372e+01, 1.1425e+01, 3.9128e+01, 9.6273e+00,
         6.4655e+00, 3.9099e+01, 1.4508e+01, 1.8944e+01, 9.3640e+00, 1.3568e+01,
         2.4873e+01, 1.8746e+01, 3.2048e+01, 3.6444e+01, 1.4982e+01, 3.1966e+01,
         1.8035e+01, 3.4164e+01, 5.2430e+00, 1.5013e+01, 2.5833e+00, 5.6077e+01,
         3.2032e+01, 1.1533e+01, 1.2839e+01, 1.6454e+01, 5.2216e+01, 9.1614e+01,
         9.3913e+00, 2.5211e+01, 2.0771e+01, 2.0677e+01, 4.6167e+00, 3.6707e+01,
         1.3477e+01, 9.7584e+00, 4.0332e+01, 5.5380e+00, 2.1534e+01, 1.9932e+01,
         1.1795e+01, 2.2266e+01, 5.1200e+01, 2.6912e+01],
        [3.4397e+00, 4.7928e+01, 1.9962e+02, 5.1090e+01, 1.4736e+02, 1.7410e+01,
         4.8382e+00, 2.3731e+01, 6.4638e+01, 9.3075e+01, 6.7165e+01, 1.9409e+01,
         1.3352e+01, 2.6365e+01, 1.1498e+01, 2.8952e+01, 5.4431e+00, 5.3607e+00,
         8.9238e+01, 2.1500e+01, 1.6482e+01, 2.5182e+01, 3.9633e+01, 1.9536e+01,
         5.5450e+00, 4.2147e+01, 2.7935e+01, 4.9449e+01, 3.7908e+01, 1.1115e+01,
         1.2762e+01, 4.4943e+00, 1.3247e+02, 3.0169e+00, 8.3136e+01, 2.8664e+01,
         2.2875e+01, 1.4819e+02, 4.7788e+01, 1.9408e+01, 6.1026e+00, 1.5536e+01,
         2.8517e+02, 4.7816e+00, 3.7688e+01, 3.5240e+01, 9.2371e+00, 2.2780e+01,
         6.8014e+01, 3.0001e+01, 2.3388e+01, 7.0147e+00, 9.7393e+01, 7.8519e+01,
         1.0190e+01, 7.1317e+00, 1.9371e+01, 7.4569e+00, 1.8758e+01, 9.2474e+00,
         1.6370e+01, 5.9479e+01, 1.1242e+01, 3.2507e+00, 3.0419e+01, 1.2690e+01,
         3.5517e+01, 9.2612e+00, 1.4685e+02, 2.1031e+02, 3.5407e+01, 5.1006e+01,
         4.4563e+01, 3.7633e+01, 1.2287e+01, 1.7336e+02, 9.4228e+00, 3.9796e+02,
         2.5738e+01, 9.0647e+01, 1.9512e+01, 5.3362e+00, 8.3725e+01, 2.2141e+02,
         1.0407e+01, 1.2663e+01, 2.7889e+01, 1.2865e+01, 1.8156e+01, 3.3248e+01,
         2.6182e+00, 6.5738e+00, 1.1430e+02, 1.6887e+01, 9.2158e+00, 2.5744e+01,
         2.4339e+01, 3.5164e+01, 1.1161e+02, 3.2742e+01]], device='cuda:6')
2025-08-01 11:39:56,964 - forward - INFO - Original Rs:
 tensor([[451.,   2.,  25.],
        [  0.,  27.,   4.],
        [155.,   3.,  18.]], device='cuda:6')
2025-08-01 11:39:56,965 - forward - INFO - Generative mu:
 tensor([[56.7508, 60.1260, 53.7265],
        [10.3388, 43.6731, 38.2124],
        [56.7508, 60.1260, 53.7265]], device='cuda:6', dtype=torch.float64)
2025-08-01 11:39:56,967 - forward - INFO - X_hat theta:
 tensor([[3.2397, 0.5574, 0.3944],
        [3.5460, 0.8342, 0.4773],
        [3.2397, 0.5574, 0.3944]], device='cuda:6')
2025-08-01 11:39:56,967 - forward - INFO - JS-divergence omega orig 0.14689707773992297 learn 0.14823495026671135 flat8.195639361607795e-08
2025-08-01 11:39:56,969 - forward - INFO - average loss when compared to Xs: tensor([ 3492.2166,   633.0352,  2191.0330,  1700.3904,  1199.9792,   927.0826,
        22546.4258,  9129.9346,   613.7754,   533.4333,  2116.3706,   675.5668,
          657.9210,  1526.3591,   774.1021,  1253.9705,   633.6659,  2028.6489,
          686.3160,   896.6429,  1256.3420,  1656.6157,  1459.9843,  1251.3823,
          858.4517,  1595.1501,  1085.1914,  1137.0748,   721.6088,   679.0458,
          738.8430,   607.9868,  1627.5642,   901.1495,  1685.3879,  1282.5370,
          884.4474,  2284.2810,  1029.7729,  1101.7089,  1343.6162,  1125.9199,
          993.4587,  1124.8488,  1499.6732,  1100.8105,  1533.9373,   828.2913,
        19491.7227,  1089.7812,  1541.3140,   781.3937,  1016.4526, 10152.5078,
         1255.3193,   946.0952,   899.9237,  2341.1416,  1367.0078,   943.4775,
         1378.8085,   623.9903,   798.6884,  1025.5498,  2615.8064,  2160.5547,
         2218.7441,   810.9419,  1047.7317,  1414.3855,   802.8514,   793.8864,
         1114.2655,  1689.1428,   662.4202,  1188.1447,  1118.8896,  4513.1982,
         1134.8806,   989.7032,   754.7205,  1461.5802,  1493.6191,   580.9150,
         1395.1265,  1328.9536,  2208.3745,   755.6964,  2049.1035,  1468.1475,
          724.0011,   774.1495, 14980.6582,  4558.2549,  1007.4661,  1529.4229,
         1040.2327,  1576.7227,  1212.9768,  1222.7791], device='cuda:6')
2025-08-01 11:39:56,969 - forward - INFO - -------------------------------- END --------------------------------
2025-08-01 11:39:56,970 - forward - INFO - 
2025-08-01 11:39:56,971 - forward - INFO - Loss Function         | Loss Value(s)        
2025-08-01 11:39:56,971 - forward - INFO - ---------------------------------------------
2025-08-01 11:39:56,971 - forward - INFO - Adj KL Lib:           |                   0.0
2025-08-01 11:39:56,971 - forward - INFO - Adj KL Gene:          |     53.68903350830078
2025-08-01 11:39:56,971 - forward - INFO - Adj KL X:             |    120.10261535644531
2025-08-01 11:39:56,972 - forward - INFO - Adj Recon Lib:        | 0.00e+00, 0.00e+00
2025-08-01 11:39:56,972 - forward - INFO - Adj Recon DA:         | 6.11e+05, 6.01e+05
2025-08-01 11:39:56,972 - forward - INFO - Adj Batch Correction: | 0.00e+00
2025-08-01 11:39:56,972 - forward - INFO - Adj Recon Express:    | 1.997e+05            
2025-08-01 11:39:56,972 - forward - INFO - Validation Epoch: 010/1000 | Batch 0001/0002
6,972 - forward - INFO - Validation Epoch: 010/1000 | Batch 0001/0002
